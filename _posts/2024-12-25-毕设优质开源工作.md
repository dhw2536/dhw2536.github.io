---
layout:     post
title:      毕设优质开源工作
subtitle:   xxx
date:       2024-12-25
author:     Space
header-img: img/the-first.png
catalog:   true
tags:
    - 本科毕设探索

---





# 毕设优质开源工作-12.25



## **1. MATLAB中的多机器人路径规划算法（整理一下放传统算法部分，1、2页）**

**概述**  该项目实现了多机器人路径规划算法，涵盖启发式搜索和增量启发式搜索方法。支持多机器人路径规划问题（MRPP）和多机器人A* 算法（MAPF），以及基于多机器人A*的路径规划算法（MAStar）。可以作为多机器人协同路径规划的基础框架。

**链接**  [MRPP-MATLAB GitHub](https://github.com/MortezaHagh/MRPP-MATLAB)

**此部分内容展示如下：**

![image-20241211213027375](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211912.png)

经尝试，可以使用MATLAB跑通，大概结果如上图，内部逻辑暂时未深入研究

![image-20241211213607468](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211914.png)

![image-20241211213615838](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211915.png)



## **2.Multi-Agent path planning in Python    多智能体路径规划（Python实现）**

该仓库包含了多种多智能体路径规划算法的Python实现，旨在为多个智能体在环境中规划路径，考虑静态与动态障碍。

**（让机器人1、2跟随机器人0来走，引入编队的概念）**

### **集中式方法**

#### **1. 优先安全区间路径规划（SIPP）**

- **简介**：SIPP是一个局部规划器，考虑动态障碍物（如其他智能体），生成无碰撞路径。

![image-20241225095333056](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211916.png)

#### **2. 冲突基搜索（CBS）**

- **简介**：CBS是一种全局路径规划算法，处理多个智能体间的路径冲突。

![image-20241225095423322](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211917.png)

### **分散式方法**

#### **1. 速度障碍**

- *+、*简介**：每个智能体计算避碰速度，避免与其他动态障碍发生碰撞。

![image-20241225095458937](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211918.png)

#### **2. 非线性模型预测控制（NMPC）**

- **简介**：通过预测控制方法，智能体避开动态障碍，进行自主路径规划。

![image-20241225095519462](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211919.png)

1. **集中式 vs 分散式**：
   - **集中式方法**（如SIPP、CBS）通常需要一个中央控制器来为所有智能体规划路径，适用于全局路径规划，通常能找到更优的解决方案，但计算复杂度较高。
   - **分散式方法**（如速度障碍、NMPC）每个智能体独立做决策，只考虑局部信息，适用于需要实时反应和动态避障的场景，但可能不如集中式方法找到最优解。
2. **路径规划 vs 避碰**：
   - **SIPP和CBS** 是全局路径规划算法，考虑的是多个智能体之间的路径冲突和协调问题。
   - **速度障碍和NMPC** 更侧重于实时避碰，动态调整速度和控制策略来避开障碍物。





## **3.SMMR-Explore 与 RRT 探索**

### **SMMR-Explore 1.0 版本介绍**
**SMMR-Explore** 是一种基于子地图的多机器人探索系统，提出于 ICAR 2021。该系统通过多机器人协作和潜在场探索方法，提升探索效率，减少通信开销。

#### **主要特性**：
- **子地图共享**：仅共享子地图，减少通信负担。
- **地图合并**：基于子地图进行定位（PR）、相对位姿估计（RelPose）和地图合并。
- **多机器人多目标潜在场探索**（MMPF）：提高探索效率，减少行驶成本，相比 RRT 方法更高效。
- **支持平台**：
  - 多车系统，搭载 Nvidia Jetson Nano 和激光雷达传感器。
  - 使用 Ubuntu 18.04 和 ROS Melodic。

#### **依赖项**：
- **Cartographer**：2D/3D 地图构建方法，支持多机器人 SLAM 和探索。
  - 安装 Cartographer-for-SMMR 和修改后的 Cartographer。
- **Pytorch 和 PointNetVLAD**：用于地点识别和特征提取。
- **Gazebo 仿真**：适用于机器人仿真。

#### **安装步骤、实验配置示例**

**（详细内容见链接）**：

- **RRT 方法（基准方法）**：
  - 小环境 - 三个机器人：`roslaunch turtlebot3sim small_env_three_robots.launch`
  
- **MMPF 方法（多机器人多目标探索）**：
  - 小环境 - 三个机器人：`roslaunch turtlebot3sim small_env_three_robots.launch`

---

### **RRT 探索 - ROS2 版本**
**RRT Exploration** 是基于 **RRT**（快速随机树）算法的多机器人协作探索系统。在一个动态的全局地图上，根据多机器人的位置，系统发出导航目标，协作进行环境探索。这是 **SMMR-Explore** 中 RRT 探索算法的 ROS2 移植版，经过性能和内存优化。

---

- **SMMR-Explore** 代码与文档：[SMMR-Explore](https://github.com/efc-robot/SMMR-Explore)
- **RRT-Explore** 代码与文档：[RRT-Explore (ROS2)](https://github.com/acachathuranga/rrt-explore?tab=readme-ov-file)
- **视频演示**：https://www.bilibili.com/video/BV1QT4y1F71u/?vd_source=e03b252a2c1fefc80e6f48a6f52e2a4d)

![image-20241225100525265](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211920.png)





## **4.3D 多机器人探索项目**

**概述**

[luigifreda/3dmr：3D 多机器人探索、巡逻和导航。 --- luigifreda/3dmr: 3D Multi-Robot Exploration, Patrolling and Navigation.](https://github.com/luigifreda/3dmr)

该项目实现了多机器人在三维环境中的协作探索，主要通过**激光雷达**和**RGBD 相机**（深度相机）获取传感器数据，构建三维体积地图，并通过路径规划与信息增益算法优化探索路径。项目使用 **V-REP** 和 **Gazebo** 仿真平台进行开发，提供了多种机器人（如 UGV、Jackal 和 Pioneer 等）在不同仿真环境中的支持。

![image-20241225103050604](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211921.png)

**主要功能**：

1. 多机器人协作探索：
   - 支持多个机器人协同工作，共享地图信息，通过最大化信息增益来选择探索目标。每个机器人独立构建其体积地图，并通过相互协作推进整个探索过程。
2. 三维体积地图构建：
   - 每个机器人构建自己的三维体积地图，并与其他机器人共享。这些地图可以帮助机器人进行障碍物检测、可达性分析以及路径规划。
3. 信息增益探索：
   - 在探索过程中，机器人通过计算信息增益来优化探索策略，选择最有潜力的探索点。信息增益度量的是从某个视点可以获取的新信息，帮助机器人避免无效的“随机漫步”。
4. 路径规划与协同：
   - 项目支持基于体积地图的路径规划，能够处理复杂的障碍物环境。机器人通过协作解决路径规划中的冲突，避免重复探索或相互干扰。

**如何运行探索**：

1. **启动仿真**：
   - 通过命令行或 PyQt 图形界面启动仿真系统，可以选择不同的仿真环境（如 V-REP 或 Gazebo）和机器人类型（如 UGV、Jackal、Pioneer）。
2. **PyQt 图形界面**：
   - 提供了图形界面工具，可以简化操作，用户通过点击按钮启动或停止探索，选择地图环境，查看探索状态。
3. **体积地图构建与存储**：
   - 项目支持构建、保存和加载体积地图，便于后续的数据分析和复现实验。

**主要脚本**：

- `ugv_3dexplorer`：适用于 V-REP 上的履带式 UGV。
- `jackal_3dexplorer`：适用于 Gazebo 上的 Jackal 机器人。
- `pioneer_3dexplorer`：适用于 V-REP 上的 Pioneer 机器人。



## 5.多智能体系统自适应合作研究（南洋理工）

#### 概述

本报告总结了多智能体系统（MAS）在自适应合作方面的研究进展，特别是在动态环境和不同团队组成下的应用。研究涵盖了自适应合作的挑战、方法论、实验设计、以及未来工作方向。

点击链接查看和 Kimi 智能助手的对话 https://kimi.moonshot.cn/share/ctlsfcte09n6k7ltopvg

#### 研究背景与目标

在人工智能领域，MAS作为关键探索领域，尤其在移动代理团队中，涉及到自主代理的互动、信息共享、行动协调和协作解决问题。研究目标是开发策略以实现自适应合作，使代理能够根据团队组成和环境的动态变化调整行为。

#### 主要挑战

1. **适应新代理/团队**：如何与未见过的或新代理进行自适应团队合作。
2. **适应多样化环境条件**：如何在不可预测的环境中调整策略。

#### 方法论

**架构设计**

研究提出了一个包含交互意识、共享记忆、协调机制和多智能体强化学习范式的架构。该架构强调了以下几个关键组件：

- **交互意识**：通过通信和代理建模增强代理间的信息共享。
- **共享记忆**：利用优先级缓冲区和基础知识来整合记忆和信息。
- **协调机制**：通过团队性能评估器和异构性感知适配器来优化团队协作。
- **决策制定**：采用分散式策略，允许代理根据环境和团队动态做出决策。

**算法与模型**

- **IAHR-MAPPO**：在动态环境中自适应合作的算法，通过训练和测试评估其泛化能力。
- **IA-VLM**：结合视觉-语言模型，提供代理更丰富的环境理解，以适应动态变化。

#### 实验设计与结果

实验在SUMO平台和Habitat Simulator上进行，涉及多种场景，如捕食者捕获猎物（PCP）和异构材料运输（HMT）。结果显示，IAHR-MAPPO在多个测试场景中表现出较低的碰撞率和完成时间，与基线算法相比有显著改善。

#### 未来工作

1. **感知模块**：利用视觉-语言模型增强代理对动态环境的理解。
2. **交互意识**：设计信息共享机制，提高对动态变化的响应能力。
3. **共享记忆模块**：保留并利用经验，提高决策制定和适应性。
4. **现实世界部署**：在真实硬件平台上验证策略。

#### 结论

本研究在MAS的自适应合作方面取得了显著进展，特别是在动态环境和团队组成变化的适应性上。通过架构设计、算法开发和实验验证，研究为MAS在现实世界应用中的泛化和适应性提供了有价值的见解和方向。

---

**新加坡南洋理工大学**为本研究提供了学术支持和资源。

### 基于 PyTorch 的 HARL 算法正式实现

[PKU-MARL/HARL：基于 PyTorch 的 HARL 算法的官方实现。 --- PKU-MARL/HARL: Official implementation of HARL algorithms based on PyTorch.](https://github.com/PKU-MARL/HARL)

这个GitHub仓库是北京大学和BIGAI合作的HARL（异构智能体强化学习）算法的官方实现，基于PyTorch。HARL算法适用于不同的异构智能体，有严格的理论支持，通常能取得比MAPPO更好的性能。这个仓库包含了HAPPO、HATRPO、HAA2C、HADDPG、HATD3、HAD3QN和HASAC等算法，可以让研究人员和实践者在七个挑战性的基准测试上复现结果，或将HARL算法应用于他们的应用中。

主要特点包括：
- 使用顺序更新方案，与MAPPO和MADDPG的同步更新方案不同。
- 理论上保证了单调改进和收敛到平衡，确保了智能体间的合作行为。
- HAPPO和HASAC等算法在多个基准测试中表现出色。

安装指南详细说明了如何安装HARL和七个常见环境（包括SMAC、SMACv2、MAMuJoCo、MPE、Google Research Football、Bi-DexterousHands和Light Aircraft Game）的依赖项。还提供了如何在现有环境中训练算法和将算法应用于新环境的说明。

### MARLlib：多智能体强化学习库

[MARLlib: A Multi-agent Reinforcement Learning Library — MARLlib v1.0.0 documentation](https://marllib.readthedocs.io/en/latest/)

MARLlib是一个多智能体强化学习库，它提供了从单个智能体的（深度）强化学习（RL/DRL）到多智能体强化学习（MARL）的过渡指南。文档内容包括安装说明、框架介绍、支持的环境、快速入门指南，以及如何从RL过渡到MARL的详细说明。

文档分为三部分：
1. 介绍单个智能体的RL和DRL。
2. 从RL到MARL的过渡，包括MARL的基本概念如部分可观测马尔可夫决策过程（POMDP）、集中训练与分散执行（CTDE）以及多样性。
3. MARL的全面调查，包括任务、方法论，以及不同算法家族的介绍和文档，如联合Q学习家族、深度确定性策略梯度家族、高级演员评论家家族、信任域策略优化家族和近端策略优化家族。

此外，还提供了一些资源列表，包括优秀论文列表、基准测试等。

相关环境：[环境 — MARLlib v1.0.0 文档 --- Environments — MARLlib v1.0.0 documentation](https://marllib.readthedocs.io/en/latest/handbook/env.html)

![image-20241225175731049](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211922.png)

github链接：[semitable/robotic-warehouse: Multi-Robot Warehouse (RWARE): A multi-agent reinforcement learning environment](https://github.com/semitable/robotic-warehouse)

![image-20241225180301222](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211923.png)

Multi-Robot Warehouse (RWARE) 是一个多智能体强化学习环境，模拟了机器人在仓库中移动和交付请求货物的场景。这个环境允许配置不同的大小、智能体数量、通信能力和奖励设置（合作/个体）。环境的行动空间包括“向左转”、“向右转”、“前进”和“装载/卸载货架”，而观察空间则是部分可观测的，由以智能体为中心的3x3（可配置）网格组成。环境中的动力学特别关注碰撞处理，当两个或多个智能体尝试移动到同一位置时，会有一个优先级规则来解决冲突。智能体成功交付请求的货架会获得奖励，但同时也需要找到空位置来归还之前交付的货架。

RWARE环境参数包括仓库的大小、智能体数量和请求货架的数量。命名方案描述了如何通过Gymnasium注册不同的环境配置。还支持自定义布局，允许用户设计自定义的仓库布局。

安装RWARE环境可以通过PyPI或Git下载并安装。环境与Open AI的Gym框架兼容，可以像创建Gym环境一样创建RWARE环境。提供了一个简单的脚本来允许人类玩家体验环境，有助于调试和理解环境动态。

### 多智能体论文合集

[Awesome Paper List — MARLlib v1.0.0 文档 --- Awesome Paper List — MARLlib v1.0.0 documentation](https://marllib.readthedocs.io/en/latest/resources/awesome.html#lbf-rware)

### Habitat-Lab

我现在在用habitat模拟器 把marl接进去 https://github.com/facebookresearch/habitat-lab

Habitat-Lab 是一个用于实体人工智能（embodied AI）的模块化高级库，旨在训练代理在室内环境中执行多种任务，并与人类互动。它支持灵活的任务定义、多样的实体代理、训练和评估代理以及人类参与的交互。Habitat-Lab 使用 Habitat-Sim 作为核心模拟器。



## **6. MultiRobo学习：多机器人强化学习框架**

- **MultiRoboLearn** 是一个开源框架，专为多机器人强化学习（MADRL）设计，旨在为研究人员提供一个统一的仿真和真实环境的研究平台。它支持多机器人系统的模拟和实际部署，允许用户在不同的强化学习算法下比较性能。

  **主要特点：**

  - **多机器人系统支持**：可用于连续和离散动作空间。
  - **OpenAI Gym兼容**：接口与OpenAI Gym兼容，支持多种算法。
  - **仿真与现实部署**：可在仿真环境中训练并直接部署到现实中的机器人。
  - **算法支持**：已成功训练并测试了多种MADRL算法，解决了不同任务。
    

  **安装要求：**

  - 兼容Ubuntu 16.04和18.04，支持Python 2.7和3.5及更高版本。
  - 需要安装ROS、TensorFlow、PyTorch等依赖。

  **代码结构：**

  - `scenarios/gym_construct`：存储任务场景。
  - `src/task_envs`：设置不同任务的MADRL环境。
  - `src/robot_envs`：扩展支持不同类型的机器人组合。
  - `algorithms/algorithms_example`：存储示例算法，用户可在此开发自定义算法。

  **使用方式：**

  - 可通过虚拟环境或pip安装。
  - 通过命令行启动仿真环境并加载算法进行训练。

  此框架正在不断开发中，适用于多机器人协作任务的强化学习研究。

**链接**  [MultiRoboLearn GitHub](https://github.com/JunfengChen-robotics/MultiRoboLearn)

[MultiRoboLearn](https://www.youtube.com/watch?v=i43FQOe0PMI)

![image-20241225183418471](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211924.png)



## **7. ROS-LLM：基于ROS的多机器人自然语言控制框架**

**概述**  ROS-LLM是一个针对ROS系统的嵌入式智能框架，旨在通过自然语言与机器人进行交互。该框架利用大型语言模型（如GPT-4和ChatGPT）实现机器人的决策和控制。通过简单的接口，研究人员可以快速集成该框架，实现机器人任务的自然语言控制。

**特点**  

- 无缝集成ROS系统。
- 利用GPT-4、ChatGPT等大语言模型增强机器人决策能力。
- 支持多机器人系统的自然语言控制。
- 快速开发和简单接口。

**链接**  [ROS-LLM GitHub](https://github.com/Auromix/ROS-LLM)  、[ROS-LLM教程](https://www.dongaigc.com/a/ros-llm-getting-started-guide)  、[视频演示](https://www.bilibili.com/video/BV1Aa4y1F7MP)



## **8. 多机器人路径规划（Multi-Agent Path Finding, MAPF）**

**概述**  该文章讨论了多机器人路径规划（MAPF）问题，重点介绍了MAPF算法如何解决多机器人系统中的路径冲突和任务协作问题。内容包括MAPF算法的背景、挑战和解决方案，以及如何应用于实际的机器人系统。

**特点**  

- 介绍了MAPF问题的背景和发展历程。
- 讨论了多机器人路径规划中的算法选择和优化。
- 适合用作多机器人系统路径规划的参考。

**链接**  [多机器人路径规划（MAPF） - CSDN博客](https://blog.csdn.net/qq_43353179/article/details/129199895)

![image-20241225184033977](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211925.png)



## **9.强化学习多机器人复杂地形建图（附完成src代码）**

**概述**  该视频介绍了如何利用强化学习进行多机器人协作来完成复杂地形的建图任务。通过采用强化学习的方式，多个机器人协同工作，在不确定或复杂环境中进行自主探索与建图。视频提供了完整的源代码，帮助开发者理解并实现相关技术。

**特点**  

- **多机器人协作建图**：采用强化学习使多个机器人协同完成复杂地形的建图任务。
- **源代码提供**：视频提供了完整的源码，方便开发者直接使用并进行二次开发。
- **复杂地形环境**：系统可以应对动态和复杂的环境，增强了机器人在实际应用中的适应性。
- **强化学习应用**：通过奖励机制让机器人在执行任务时逐渐优化决策，提高建图精度和效率。

**视频链接**  [强化学习多机器人复杂地形建图（附完成src代码）](https://www.bilibili.com/video/BV1etq7YMEvV/?spm_id_from=333.337.search-card.all.click&vd_source=e03b252a2c1fefc80e6f48a6f52e2a4d)  
**付费内容**  该视频需要购买（39.9元）以获取完整内容和源代码。

![image-20241225184143309](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211926.png)

## **10.基于强化学习和速度障碍法的多机器人导航**

**概述**  本视频介绍了一种结合强化学习与速度障碍法（RVO）的多机器人导航方法。通过使用RVO算法解决多机器人间的碰撞问题，并结合强化学习来优化导航策略，该方法能有效地实现多个机器人在复杂环境中的避障与协作导航。视频展示了如何在不同的场景中运用这一技术来提升多机器人系统的协作与效率。

**特点**  

- **强化学习与RVO结合**：结合速度障碍法（RVO）与强化学习，使得机器人在动态环境中既能避障又能有效导航。
- **多机器人协同**：通过强化学习的优化，使多个机器人能够在复杂环境中协作完成任务，避免碰撞并提高效率。
- **应用于动态环境**：在动态和不可预测的环境中，机器人能够根据实时反馈调整其行为。
- **开源代码**：提供了源代码，开发者可以轻松部署并根据需要修改。

解决碰撞避让问题的挑战在于，在充满交互障碍的复杂场景中自适应地选择最佳机器人速度。

**视频链接**  [基于强化学习和速度障碍法的多机器人导航](https://www.bilibili.com/video/BV1Gx4y1U7DZ/?spm_id_from=333.337.search-card.all.click&vd_source=e03b252a2c1fefc80e6f48a6f52e2a4d)  、[基于强化学习和速度障碍法的多机器人导航_第二部分](https://www.bilibili.com/video/BV1oS4y1U71B/?spm_id_from=333.337.search-card.all.click&vd_source=e03b252a2c1fefc80e6f48a6f52e2a4d)

**相关论文**  

- **IEEE论文链接**：[Reinforcement Learned Distributed Multi-Robot Navigation with Reciprocal Velocity Obstacle Shaped Rewards](https://ieeexplore.ieee.org/document/9740403)  
- **arXiv论文链接**：[Reinforcement Learned Distributed Multi-Robot Navigation with Reciprocal Velocity Obstacle Shaped Rewards (PDF)](https://arxiv.org/pdf/2203.10229.pdf)

**开源代码** [hanruihua/rl_rvo_nav: The source code of the [RA-L\] paper "Reinforcement Learned Distributed Multi-Robot Navigation with Reciprocal Velocity Obstacle Shaped Rewards"](https://github.com/hanruihua/rl_rvo_nav)

![image-20241225184351500](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211927.png)

![image-20241225184332282](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211928.png)



## **11.单车-强化学习导航：仿真环境训练及ROS实车部署**（参考）

**概述**  本项目展示了基于深度强化学习（TD3算法）进行导航任务的仿真训练与ROS实车部署。项目起初基于仿真训练，在训练模型后部署到实车上进行测试。此项目适用于无地图的导航任务，并可在陌生环境中应用。尽管是非专业领域的应用，但为机器人研究者提供了很好的实际操作经验。

**特点**  

- **TD3算法**：采用强化学习的TD3算法进行机器人导航训练。
- **仿真与实车测试**：在仿真环境中训练后，成功将训练好的模型部署至实车进行导航。
- **无需地图**：可在没有地图的情况下进行路径规划，适合于未知环境的自主导航。
- **教学文档**：提供详细的说明文档，适合初学者学习。

**GitHub项目链接**  [DRL-robot-navigation](https://github.com/reiniscimurs/DRL-robot-navigation)  
**论文链接**  [IEEE论文 - 深度强化学习导航](https://ieeexplore.ieee.org/document/9645287?source=authoralert)  
**知乎说明文档**  [ROS+Gazebo强化学习导航项目的仿真训练和实车部署：教程及踩过的一些坑 - 知乎](https://zhuanlan.zhihu.com/p/709427766)



## **12. Goal-guided Transformer-enabled Reinforcement Learning for Efficient Autonomous Navigation（重点参考）**

**概述**  该研究探索了基于Transformer的深度强化学习方法，在没有地图的情况下实现高效的自主导航。重点是如何在没有预设地图的情况下，利用目标引导的强化学习进行自主导航。

**资源链接**  

- [GitHub 仓库](https://github.com/OscarHuangWind/DRL-Transformer-SimtoReal-Navigation)





#### 26. **未知环境下多机器人协同探索的混合多策略快速探索随机树算法**

**概述**  本文提出了一种新的混合多策略快速探索随机树（HMS-RRT）算法，用于多机器人协同探索未知环境。该算法通过引入自适应增量距离策略和基于贪婪边界的探测策略，提高了探索效率，避免了局部最优解。

**关键点**  

- **算法**：HMS-RRT，通过Voronoi图进行环境建模并分配任务。
- **策略**：自适应增量距离和贪婪边界策略提高探索效率。
- **应用**：多机器人协作和实时地图合并。

**资源链接**  

- [论文链接](https://mp.weixin.qq.com/s/bnAPcJzJfD4oYUOQ3utIsg)

---

#### 27. **基于深度强化学习和自主课程学习的物流AGV无地图导航方法**

**概述**  本研究采用深度强化学习与自动课程学习相结合的方法，提出了一种适用于无地图导航的物流AGV（自动引导车）控制方法。该方法通过自主学习提高了在复杂物流环境中的导航效率，尤其是在动态变化的环境中。

**资源链接**  

- [详细文章链接](https://mp.weixin.qq.com/s/lPHe62bpXXimsEN-hwSmBg)

---

#### 28. **使用深度强化学习进行高效目标映射的多机器人信息路径规划**

**概述**  这项研究提出了一种新颖的深度强化学习方法，用于多机器人信息路径规划，以在未知的3D环境中映射感兴趣的目标。重点在于如何在有限的资源预算下，例如路径长度或任务时间，同时避免机器人间的碰撞和静态障碍物。研究使用增强图模型来模拟机器人轨迹，从而避免机器人间的碰撞和通信干扰。此外，采用了集中训练和分散执行的策略，使得训练后的模型可以扩展到不同数量的机器人，且无需重新训练。

**关键点**  

- **目标**：高效完成目标映射，避免机器人碰撞。
- **方法**：使用深度强化学习结合增强图模型来规划路径。
- **优势**：在目标发现数量上比其他现有方法高出33.75%。
- **训练策略**：集中训练，分散执行，适用于不同数量的机器人。

**资源链接**  

- [论文链接 (arXiv)](https://arxiv.org/abs/2409.16967)
- [GitHub 仓库](https://github.com/AccGen99/marl_ipp)

---

#### 29. **通过深度强化学习进行多机器人协同探索**

**概述**  该研究展示了一种基于Voronoi图的多机器人自主探索方法，利用深度强化学习在未知环境中进行协同探索。通过这种方法，多个机器人可以在环境中进行有效的资源探索，同时避免冲突并协作完成任务。

**资源链接**  

- [论文链接 (IEEE Xplore)](https://ieeexplore.ieee.org/document/9244647)

---

#### 30. **基于深度强化学习的动态环境中的协作多机器人导航**

**概述**  本研究提出了一种多机器人协作导航方法，特别适用于动态环境。通过深度强化学习，机器人可以在动态变化的环境中进行有效的路径规划和协作导航。该方法的核心是使机器人能够根据环境变化调整策略，确保系统的高效性和稳定性。

**资源链接**  

- [论文链接 (IEEE Xplore)](https://ieeexplore.ieee.org/document/9197209)
- [YouTube 视频](https://www.youtube.com/watch?v=RmT6ZUtyyOA)

