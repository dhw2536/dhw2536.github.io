---
layout:     post
title:      视觉语言导航-仿真验证
subtitle:   介绍了视觉语言导航（VLN）领域的最新研究进展，包括多种仿真平台、数据集、模型方法以及相关论文的总结与分析
date:       2025-01-07
author:     Space
header-img: img/the-first.png
catalog:   true
tags:
    - 视觉语言导航

---



# 视觉语言导航-入门

## **！！！！**

注意看仿真环境、和无人机结合、室内室外都可以，主要是Airsim和英伟达（isaac gym、isaac sim、isaac lab ）有关的那个**

[Isaac Sim - Robotics Simulation and Synthetic Data Generation | NVIDIA Developer](https://developer.nvidia.com/isaac/sim)

## 任务

无人机/无人车等在室内和室外视觉语言导航方向上的仿真环境部分实现都是什么，发展与应用情况如何，适当找几篇论文有所验证

#### 1. **仿真环境**
   - **任务**：调研室内和室外视觉语言导航方向上的仿真环境部分。
   - **内容**：
     - **仿真环境实现**：分析用于视觉语言导航任务的仿真环境，包括模拟室内和室外场景的工具和平台（例如Gazebo、AirSim、V-REP等）。
     - **仿真环境的特点**：评估不同仿真环境的特点，如何模拟现实世界中的复杂场景（例如室内障碍、室外地形、天气变化等），以及这些环境在机器人导航中的实际应用。

#### 2. **发展与应用情况**
   - **任务**：总结仿真环境在视觉语言导航中的发展状况与应用情况。
   - **内容**：
     - **发展状况**：梳理仿真环境的发展历程，了解目前主流仿真工具和平台的特点与局限。
     - **应用情况**：分析这些仿真环境在机器人和无人机视觉语言导航中的应用，例如用于训练和评估机器人在室内外环境中的导航能力。

#### 3. **验证与论文调研**
   - **任务**：调研相关的验证论文。
   - **内容**：寻找并总结几篇论文，探讨仿真环境如何用于验证无人机/无人车的导航策略与技术，包括验证结果的效果与准确性。





## 无人机/无人车视觉语言导航仿真环境

视觉语言导航（VLN）是一个新兴的研究方向，它要求智能体（如无人机或无人车）根据自然语言指令和视觉信息在环境中自主导航。这一技术在室内和室外环境中都有广泛的应用前景，例如在智能家居、农业监察、搜索救援等领域。以下是关于无人机和无人车在室内和室外视觉语言导航方向上的仿真环境部分实现、发展与应用情况的详细分析：

### 仿真环境实现与数据集

1. #### **AeroVerse仿真平台**（AeroSimulator模拟平台）
   
   - **开发者**：中国科学院空天信息创新研究院网络信息系统技术重点实验室。
   
   - **主要贡献**：构建了首个大规模的真实世界图像-文本预训练数据集AerialAgent-Ego10k和虚拟图像-文本-姿态对齐数据集CyberAgent-Ego500k。定义了五个航空航天具身下游任务并构建了相应的指令数据集。
   
   - **仿真器开发**：使用Unreal Engine 4加载城市环境，并选择AirSim来构建无人机模型，开发出AeroSimulator。
   
   - **多动作空间支持**：支持无人机执行多种动作，如改变位置、方向和速度，以及通过加速度调整和力向量应用进行更复杂的机动。
   
   - **环境多样性**：能够模拟不同的光照条件和天气情况，并生成视觉输出，包括RGB图像、深度图和分割数据。
   
   - **链接：**[中科院空天院无人机视觉语言导航新基准！AeroVerse：模拟、预训练、微调和评估空中无人机具身世界模型的测试基准-CSDN博客](https://blog.csdn.net/weixin_37990186/article/details/144945027)
   
   - **原文链接：** AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models (https://arxiv.org/pdf/2408.15511)
   
     ![image-20250107120707917](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211169.png)
   
2. #### **OpenUAV平台**
   
   - **开发者**：北京航空航天大学人工智能研究所、香港中文大学MMLab以及感知与交互智能中心。
   - **主要贡献**：创建了首个专为现实无人机VLN任务设计的大规模轨迹数据集，包含约12k个轨迹，涵盖了多种环境和复杂的飞行动态。
   - **数据集特点**：包含从多个视角捕获的图像、任务描述、助手指令以及通过人工控制收集的连续轨迹。
   - 相关链接：[OpenUAV](https://prince687028.github.io/OpenUAV/)、[OpenUAV|无人机导航数据集|视觉语言处理数据集](https://www.selectdataset.com/dataset/37cd0385982decb34e50a2772d6a909b)、[OpenUAV：首个专为现实无人机视觉语言导航设计的大规模轨迹数据集，由大约 12k 个轨迹组成，涵盖了多种环境和复杂的飞行动态。-CSDN博客](https://blog.csdn.net/u011559552/article/details/142854060)
   
   ![img](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211170.png)
   
3. #### **AdaVLN仿真器**
   
   - **开发者**：使用Unreal Engine 4和Microsoft AirSim插件开发了一个3D模拟器。
   
   - **主要贡献**：提出了自适应视觉语言导航（AdaVLN），在充满动态人类障碍物的复杂3D室内环境中导航。
   
   - **仿真器特点**：基于IsaacSim，支持基于物理的网格和动画人类，以及与Matterport3D环境兼容。
   
   - **链接：**[西工大经典力作！AerialVLN：空中无人机视觉语言导航数据集-CSDN博客](https://blog.csdn.net/weixin_37990186/article/details/144482252)
   
     ![image-20250107120555667](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211172.png)

#### Matterport3D Simulator

https://mp.weixin.qq.com/s/yVI-CLbX4avtQUmKf1HbIA、[Matterport3D搭建与VLNBERT实现 - 知乎](https://zhuanlan.zhihu.com/p/424693141)、[【Matterport3D模拟器安装详细教程】适用于离散视觉语言导航任务的环境部署与安装-CSDN博客](https://blog.csdn.net/weixin_41848012/article/details/134503194)

**Matterport3DSimulator** 是一个专门用于视觉语言导航（Visual Language Navigation, VLN）任务研究的仿真平台，它提供了丰富的三维场景和物理交互功能。然而，由于其安装过程的复杂性以及所需数据集的庞大，这为初学者设置了较高的使用门槛。

**Docker** 是一个开源的容器化平台，它能够简化应用程序及其依赖环境的打包和运行过程，从而在不同系统上实现 Matterport3DSimulator 的快速部署和体验。本文旨在介绍如何利用 Docker 快速部署 Matterport3DSimulator，以便用户能够直观地体验该平台，并帮助初学者在 VLN 领域建立信心。

![image-20250107134711669](https://raw.githubusercontent.com/dhw2536/Picture/main/202501232211173.png)

#### 发展与应用

1. **室内环境应用**
   - **挑战**：室内环境相对封闭，但导航任务复杂性高，需要处理动态障碍物和多变视角。
   - **应用实例**：在智能家居场景下，人类可以通过语音/自然语言对家政机器人下达指令，指示其完成如“帮我去厨房拿杯水”这样的任务。

2. **室外环境应用**
   - **挑战**：室外环境开放，导航路径长且复杂，需要处理更复杂的飞行动态和空中环境。
   - **应用实例**：在农业监察中，无人机可以根据指令自动飞到指定区域，拍摄作物照片并记录生长数据。

3. **技术发展**
   - **数据集构建**：为了训练和测试无人机导航模型，研究人员开发了多个数据集，如AerialAgent-Ego10k、CyberAgent-Ego500k和OpenUAV数据集。
   - **模型训练**：使用了包括LLaMA、MiniGPT4、BLIP2在内的多种视觉-语言模型，并进行了调整以适应任务需求。
   - **评估方法**：开发了基于GPT-4的自动化评估方法SkyAgent-Eval，以及使用BLEU、CIDEr、SPICE等传统指标。

4. **未来展望**
   - **三维空间导航**：未来研究将扩展到三维空间的导航，如无人机在城市环境中的飞行。
   - **模糊导航**：研究智能体在指令中不包含准确行为路径甚至目标位置的情形下，如何进行路径规划和自主探索。
   - **环境交互导航**：探索智能体与环境的交互，如在导航过程中与动态障碍物的交互。



## 视觉语言导航（VLN）入门

[mp.weixin.qq.com/s/9zQhqwGtqPo5ukWjAePYIw](https://mp.weixin.qq.com/s/9zQhqwGtqPo5ukWjAePYIw)

[mp.weixin.qq.com/s/Q6dF9y-zXANdnezmGIlsrQ](https://mp.weixin.qq.com/s/Q6dF9y-zXANdnezmGIlsrQ)

| 模拟器                 | 环境观察 | 对应数据集              | 链接                                                         |
| ---------------------- | -------- | ----------------------- | ------------------------------------------------------------ |
| VizDooma               | 卡通     | -                       | [https://vizdoom.cs.put.edu.pl/](https://vizdoom.cs.put.edu.pl/) |
| House3D                | 三维渲染 | SUNCG                   | [https://github.com/facebookresearch/House3D](https://github.com/facebookresearch/House3D) |
| AI2THOR                | 三维渲染 | -                       | [http://ai2thor.allenai.org](http://ai2thor.allenai.org)     |
| Gibson                 | 真实光景 | 2D-3D-S                 | [http://gibsonenv.stanford.edu/](http://gibsonenv.stanford.edu/) |
| iGibson                | 真实光景 | iGibson                 | [http://gibsonenv.stanford.edu/](http://gibsonenv.stanford.edu/) |
| Matterport3D Simulator | 真实光景 | R2R, R4R, REVERIE, SOON | [https://github.com/peteanderson80/Matterport3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator) |
| Habitat                | 真实光景 | VLN-CE                  | [https://aihabitat.org/](https://aihabitat.org/)             |
| AirSim                 | 三维渲染 | AerialVLN               | [https://github.com/microsoft/AirSim](https://github.com/microsoft/AirSim) |

#### 一、概述
- **定义**：VLN是自然语言处理、计算机视觉和机器人导航等多学科交叉的研究领域，目标是开发能理解自然语言指令并在复杂环境中自主导航的智能体.
- **任务介绍**：依赖于与环境模拟器（如Matterport3D、Habitat等）构建的交互式环境，模拟器根据智能体状态和操作生成动态感知信息，智能体需按指令要求从起始节点出发，在限定步数内到达目标节点.
- **任务类型**：
  - 指令导向：严格遵循给定语言指令导航，如R2R任务.
  - 目标导向：根据给定目标导航，在环境中搜索与目标相匹配的物体，如REVERIE任务.
  - 需求导向：根据用户抽象需求导航，理解需求并找到满足需求的物体或位置，如DDN任务.
  - 单轮指令任务：智能体接收到一个自然语言指令后执行，无需进一步交互.
  - 对话式导航任务：智能体与用户进行多次对话，通过提问获取更多信息或请求澄清.
- **场景类型**：
  - 室内场景：关注家庭或办公环境内的导航，环境复杂，对空间理解能力要求高.
  - 室外场景：涉及街道、公园等开放环境，需处理复杂空间关系和动态性.
  - 空中场景：针对无人机导航任务，需考虑飞行高度和更复杂的空间关系.

#### 二、测试基准
- **模拟器与数据集**：
  - VizDoom：卡通环境观察.
  - House3D：三维渲染环境，对应SUNCG数据集.
  - AI2THOR：三维渲染环境.
  - Gibson：真实光景环境观察，对应2D-3D-S数据集.
  - iGibson：真实光景环境观察，对应iGibson数据集.
  - Matterport3DSimulator：真实光景环境观察，对应R2R、R4R、REVERIE、SOON数据集.
  - Habitat：真实光景环境观察，对应VLN-CE数据集.
  - AirSim：三维渲染环境，对应AerialVLN数据集.
- **数据集简介**：
  - R2R：基于Matterport3D数据集，包含90个房屋的真实照片，智能体需根据语言指令导航至目标位置.
  - R4R：扩展R2R任务，生成更长的指令和轨迹.
  - CVDN：模拟真实家庭环境中人与人之间的对话过程，基于对话历史进行导航并搜索目标.
  - REVERIE：远程对象定位导航任务，智能体需在导航路径终点选择正确对象.
  - SOON：基于视觉的场景定位目标导航方法，智能体被指示寻找详细描述的目标对象.
  - AerialVLN：针对无人机的视觉语言导航任务，包含10个城市的100个飞行场景.
- **评估指标**：
  - 路径长度（PL）：导航轨迹长度.
  - 导航误差：预测路径终点和参考路径终点之间的距离.
  - 导航成功率：预测路径终点和参考路径终点之间的距离不大于3米.
  - Oracle Success Rate（OSR）：衡量导航路径上任意点到目标点的距离是否在预定义阈值范围内.
  - 基于路径加权的成功率（SPL）：同时考虑成功率和路径长度，对过长路径进行惩罚.
  - 长度加权的覆盖分数（CLWCS）：生成路径和参考路径的一致性问题.
  - 基于动态时间规整加权成功率（SDTW）：评估预测路径和参考路径的时空相似性.
  - 远程定位成功率（RGS）和长度加权的远程定位成功率（LRGS）：目标导向任务中评估成功找到目标物体的准确率.

#### 三、典型模型与开源代码
- **传统Seq2seq方法**：使用基于注意力机制的LSTM的序列到序列模型，结合“学生自学”训练方法，学习标注数据的特征信息.
- **基于数据增强方法**：通过数据增强提升模型性能，解决数据稀缺问题.
- **基于辅助目标方法**：辅助推理任务提供额外训练信号，增强模型对环境的理解和语义信息利用，提升导航准确性和效率.
- **基于拓扑图方法**：拓扑图支持全局路径规划，提高探索效率，增强环境记忆，促进自然语言指令与视觉场景的有效对接.
- **基于大模型方法**：大模型处理多模态输入，执行零样本学习，展现高级规划和推理能力，提供强大的决策支持，增强任务泛化性，并能生成高质量的导航指令.

#### 四、理论基础
- **神经网络**：
  - 感知机：简单的线性二分类模型.
  - 全连接网络（FCNs）：由多个全连接层组成.
  - 循环神经网络（RNN）：处理序列数据，捕捉时间依赖性.
  - 长短时记忆网络（LSTM）：解决传统RNN的梯度消失或梯度爆炸问题，通过门控机制学习长距离依赖关系.
  - 门控循环单元（GRU）：LSTM的简化版本，参数更少，计算效率更高.
  - 卷积神经网络（CNN）：提取图像的局部特征.
  - Transformer：基于自注意力机制的模型，处理序列数据，捕捉长距离依赖关系.
- **模型训练**：
  - 模仿学习：模型观察专家导航行为并学习模仿.
  - 强化学习：通过与环境交互学习最优策略.
  - 辅助监督学习：自监督学习利用数据本身的结构生成伪标签，训练模型.
- **工具和框架**：
  - PyTorch：开源机器学习库，提供GPU加速的张量计算能力和动态计算图.
  - Transformers：提供预训练的Transformer模型和构建训练工具.

#### 五、学习社区
- **综述调研**：多所高校和研究机构发布了关于VLN的综述和调研文章，如加州大学、国防科技大学、中科院自动化所等.
- **会议与论坛**：包括CAAl Embodied Al、启智社区、具身智能之心等组织的会议和论坛，以及VALSE、智源社区青源Talk等学术交流活动.
- **知名实验室**：如北京大学具身感知与交互实验室、鹏城实验室多智能体与具身智能研究所、西湖大学机器智能实验室等，在VLN及相关领域开展研究.





### 待看文章

2023年:

1. CVPR-2023 | 视觉语言导航智能体行为分析（×）
   https://mp.weixin.qq.com/s/-aT92jdmaxO05fXpewUYKA

2024年:

1. 视觉语言导航入门必看（√）
   https://mp.weixin.qq.com/s/9zQhqwGtqPo5ukWjAePYIw

2. ICRA-2024 | VLFM：基于视觉-语言边界地图的零样本语义导航（√）
   https://mp.weixin.qq.com/s/csqT-qSd6IxbC7cXnsB9TA

3. arXiv-2024 | 具身智能体要上天！CITYNAV：基于地理信息的无人机视觉语言导航数据集（×）
   https://mp.weixin.qq.com/s/GRT8zU93QqoUYLy_z57g8g

4. AAAI-2024 | NavGPT-2：释放视觉语言大模型的导航推理能力（√）
   https://mp.weixin.qq.com/s/Unn_qj8ly5l1x4cRHYZvdg

5. ECCV-2024 | NavGPT：基于大模型显式推理的视觉语言导航
   https://mp.weixin.qq.com/s/t2eLYHgsH91JkhtVVjLizA

6. arXiv-2024 | BEVInstructor：基于BEV感知和大模型的视觉语言导航指令生成
   https://mp.weixin.qq.com/s/Q_9szEvObzu4kI72hzAWvA

7. CVPR-2024 | SRDF：基于自我精炼数据飞轮的语言引导式导航学习
   https://mp.weixin.qq.com/s/00QDU9NgLTrHUpSloP9Hfw

8. ACL Findings-2024 | NaviLLM：学习迈向具身导航的通用模型
   https://mp.weixin.qq.com/s/f7eDlfKEVmoQApX6edPU0w

9. arXiv-2024 | CorNav：具有自我校正规划的zero-shot视觉语言导航智能体
   https://mp.weixin.qq.com/s/2Fvz3FPMDJh3mdTwEC4FVQ

10. ECCV-2024 | NavAgent: 基于多尺度城市街道视图融合的无人机视觉语言导航
    https://mp.weixin.qq.com/s/aESSQGYklNJ-vNAWaR_49A

11. CoRL-2024 | LeLaN：从无标签视频中学习语言条件下的导航策略
    https://mp.weixin.qq.com/s/lJe9-1rwM-LMesb4VUIOzg

12. arXiv-2024 | Uni-NaVid：基于视频的视觉-语言-动作统一具身导航任务
    https://mp.weixin.qq.com/s/2BnC72k0x8wtBryq1unj5Q

13. AAAI-2024 | WebVLN：面向网站的视觉语言导航
    https://mp.weixin.qq.com/s/iswtq4NTZeZos8OQqSvoqg

14. arXiv-2024 | SAME：基于状态自适应专家混合模型的通用语言引导视觉导航
    https://mp.weixin.qq.com/s/a96f94ybwFHhGlS38LEIQw

15. MM-2024 | ObVLN：突破障碍，受阻环境中的视觉语言导航
    https://mp.weixin.qq.com/s/QusewWFY4BimT_TvuN-IWA

16. CoRL-2024 | InstructNav：未知环境中的零样本通用指令导航智能体
    https://mp.weixin.qq.com/s/2QRz1hmaWImstvGblhroAQ

17. RSS-2024 | VLA-3D：3D语义场景理解和导航数据集
    https://mp.weixin.qq.com/s/3LmgQAc9nWWuwX-movYBUA

18. CVPR-2024 EAI Workshop | PRS：以人为中心的室内物体具身递送数据集
    https://mp.weixin.qq.com/s/07mqF69eEmaqw38Ud8lrLg

19. AeroVerse：模拟、预训练、微调和评估空中无人机具身世界模型的测试基准
    https://mp.weixin.qq.com/s/H-_RdgAQSVPt4LVKWEUUew

20. NaVILA：基于分层导航动作策略的腿式机器人指令导航
    https://mp.weixin.qq.com/s/jxDnpSQRDQjvWvkSU6g45A

21. 导航世界模型
    https://mp.weixin.qq.com/s/pD16UtQ3tzKJ4z-mLq1UNg

22. ICAPS-2024 | GeoText-1652：基于空间关系匹配的测试基准
    https://mp.weixin.qq.com/s/iOpV2693soj0kmwPP7sH7g

23. SayNav：基于大模型动态规划的未知环境导航
    https://mp.weixin.qq.com/s/XON5SF7vMgMLRHhzxBc0OQ

24. ECCV-2024 | VLN-Copilot：用LLM作为粗粒度视觉语言导航智能体的副驾
    https://mp.weixin.qq.com/s/2Fgp-YQnkEH6mb5zjSuQqg

25. RSS-2024 | NaVid：视觉语言导航智能体通过视频学习导航动作规划
    https://mp.weixin.qq.com/s/6WArpKNrFSpZ7VQYYTey8w

26. AerialVLN：空中无人机视觉语言导航数据集
    https://mp.weixin.qq.com/s/aZg5lTPhLHF2saU_zN7-RA

27. arXiv-2024 | LH-VLN：长期发展眼光的视觉语言导航：平台、基准和方法
    https://mp.weixin.qq.com/s/2DyZd72mJ_ijfxzEKSARHA

28. arXiv-2024 | doScenes：基于自然语言指令的人车交互自主导航驾驶数据集
    https://mp.weixin.qq.com/s/Ao2wLk-FruxtgXZX153lZQ

29. ACL-2024 | MapGPT：基于地图引导提示和自适应路径规划机制的视觉语言导航
    https://mp.weixin.qq.com/s/Pcbj_D04AvDQfn2dU_bLjA

30. arXiv-2024 | NL-SLAM：自然语言与SLAM结合的对象引导视觉语言导航
    https://mp.weixin.qq.com/s/sMoHe2_dlPaSatwIWKRU8A

31. arXiv-2024 | STMR：语义拓扑度量表示引导的大模型推理无人机视觉语言导航
    https://mp.weixin.qq.com/s/8HyMHYwid982jXPewrC1qA



### 1. ACL-2024 | MapGPT：基于地图引导提示和自适应路径规划机制的视觉语言导航
- **论文中英文名称**：MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation
- **作者和单位**：
  - Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong
  - 香港大学，中山大学深圳校区，美团
- **原文链接**：https://aclanthology.org/2024.acl-long.529.pdf
- **项目主页链接**：https://chen-judge.github.io/MapGPT/
- **代码链接**：https://github.com/chen-judge/MapGPT/
- **微信推文链接：**[ACL-2024 | MapGPT：基于地图引导提示和自适应路径规划机制的视觉语言导航](https://mp.weixin.qq.com/s/Pcbj_D04AvDQfn2dU_bLjA)
- **文章大致情况介绍**：论文提出了MapGPT，基于地图引导的GPT智能体，用于解决视觉语言导航中的全局探索和路径规划问题。通过在线构建的语言形式的地图和自适应路径规划机制，MapGPT能够动态生成和更新其多步路径规划，适用于GPT-4和GPT-4V，在R2R和REVERIE数据集上均达到了最先进的zero-shot性能.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Matterport3D模拟器
- **实机验证**：未提及
- **数据集**：R2R和REVERIE数据集
- **评估标准**：导航错误（NE）、成功率（SR）、Oracle成功率（OSR）和路径长度加权成功率（SPL）



### 2. arXiv-2024 | 具身导航智能体学会找对象！NL-SLAM：自然语言与SLAM结合的对象引导视觉语言导航
- **论文中英文名称**：NL-SLAM for OC-VLN: Natural Language Grounded SLAM for Object-Centric VLN
- **作者和单位**：
  - Sonia Raychaudhuri, Duy Ta, Katrina Ashton, Angel X. Chang, Jiuguang Wang, Bernadette Bucher
  - 波士顿动力人工智能研究所，西蒙弗雷泽大学，宾夕法尼亚大学，密西根大学
- **原文链接**：https://arxiv.org/pdf/2411.07848
- **项目主页链接**：https://sonia-raychaudhuri.github.io/nlslam/
- **微信推文链接**：[arXiv-2024 | 具身导航智能体学会找对象！NL-SLAM：自然语言与SLAM结合的对象引导视觉语言导航](https://mp.weixin.qq.com/s/sMoHe2_dlPaSatwIWKRU8A)
- **文章大致情况介绍**：论文提出了OC-VLN数据集和NL-SLAM方法，将自然语言导航指令与机器人观测和姿态关联起来，主动执行NL-SLAM以遵循对象中心的自然语言导航指令。利用预训练的语言和视觉基础模型，无需任务特定的训练，提高了方法的通用性和可扩展性，在所有评估指标上均优于基线模型，并在波士顿动力Spot机器人上成功展示了NL-SLAM在真实世界中的导航指令遵循能力.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Habitat模拟器
- **实机验证**：在波士顿动力Spot机器人上进行了验证
- **数据集**：OC-VLN数据集
- **评估标准**：成功率（SR）、按路径长度倒数加权的成功率（SPL）、Oracle成功率（OSR）、归一化的动态时间规整（nDTW）



### 3. arXiv-2024 | STMR：语义拓扑度量表示引导的大模型推理无人机视觉语言导航
- **论文中英文名称**：Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning
- **作者和单位**：
  - Yunpeng Gao, Zhigang Wang, Linglin Jing, Dong Wang, Xuelong Li, Bin Zhao
  - 西北工业大学，上海人工智能实验室，中国电信人工智能研究院
- **原文链接**：https://arxiv.org/pdf/2410.08500
- **项目主页链接**：未提及
- **代码链接**：未提及
- **文章大致情况介绍**：论文提出了一种基于大语言模型（LLM）的端到端框架，用于空中视觉语言导航任务，通过分解自然语言指令为多个子目标，并设计了一种独特的矩阵表示（STMR），包含拓扑、语义和度量信息，显著增强了LLM在户外环境中的空间感知推理能力，在真实和模拟环境中进行了广泛的实验，证明了所提方法的有效性和鲁棒性，在AerialVLN-S数据集上实现了15.9%和12.5%的OSR提升.
- **属于无人机还是无人车**：无人机
- **仿真平台**：Airsim和UE4
- **实机验证**：在搭载Intel RealSense D435i深度相机和NVIDIA Jetson Xavier NX的Q250无人机上进行了验证
- **数据集**：AerialVLN-S数据集
- **评估标准**：导航误差（NE）、成功率（SR）、Oracle成功率（OSR）



### 4. RSS-2024 | 具身智能体也爱看片！NaVid：视觉语言导航智能体通过视频学习导航动作规划
- **论文中英文名称**：NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation
- **作者和单位**：
  - Jiazhao Zhang, Kunyu Wang, Xiaomeng Fang, Rongtao Xu, Qi Wu, Gengze Zhou, Zhizheng Zhang, Yicong Hong, He Wang
  - 北京大学计算机学院前沿计算研究中心，北京人工智能研究院，CASIA，阿德莱德大学，澳大利亚国立大学，Galbot
- **原文链接**：https://arxiv.org/pdf/2402.15852
- **项目主页链接**：https://pku-epic.github.io/NaVid/
- **文章大致情况介绍**：论文提出NaVid，一种基于视频输入的视觉语言模型，通过机器人的单目RGB摄像头和人类指令来导航，无需地图或深度传感器。将机器人的历史轨迹编码为视觉token，提供丰富的上下文信息，支持决策和指令导航。采用混合训练策略，结合非oracle导航轨迹和辅助任务，增强了模型的泛化和鲁棒性.
- **属于无人机还是无人车**：无人车
- **仿真平台**：VLN-CE benchmarks（如R2R和RxR）
- **实机验证**：在四个不同的室内场景（会议室、办公室、实验室和休息室）中进行了实验，使用Turtlebot4机器人和Kinect DK相机进行数据收集
- **数据集**：VLN-CE benchmarks（如R2R和RxR），以及四个不同的室内场景
- **评估标准**：成功率（SR）、路径长度（TL）、导航误差（NE）、路径长度加权成功率（SPL）等



### 5. 西工大经典力作！AerialVLN：空中无人机视觉语言导航数据集
- **论文中英文名称**：AerialVLN: Vision-and-Language Navigation for UAVs
- **作者和单位**：
  - Shubo Liu, Hongsheng Zhang, Yuankai Qi, Peng Wang, Yanning Zhang, Qi Wu
  - 西北工业大学，阿德莱德大学
- **原文链接**：https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.pdf
- **项目主页链接**：未提及
- **代码链接**：https://github.com/AirVLN/AirVLN
- **文章大致情况介绍**：论文提出了AerialVLN任务，允许智能体在四自由度（前向、左转/右转、上升/下降、左移/右移）中移动，更接近真实的无人机飞行行为。开发了包含25个城市级场景的模拟器，支持连续导航和环境扩展配置，能够模拟动态的户外环境，如风吹树叶、车辆运行、光照变化等.收集了8,446条飞行路径和相应的自然语言指令，数据集规模大且多样性强.
- **属于无人机还是无人车**：无人机
- **仿真平台**：使用Unreal Engine 4和Microsoft AirSim插件开发的3D模拟器
- **实机验证**：未提及
- **数据集**：AerialVLN数据集，包含25个城市级场景的模拟器数据
- **评估标准**：未提及具体评估标准，但通常包括成功率、路径长度、导航误差等



### 6. arXiv-2024 | 中山大学重磅出炉！LH-VLN：长期发展眼光的视觉语言导航：平台、基准和方法
- **论文中英文名称**：Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method
- **作者和单位**：
  - Xinshuai Song, Weixing Chen, Yang Liu, Weikai Chen, Guanbin Li, Liang Lin
  - 中山大学，腾讯美国，鹏城实验室
- **原文链接**：https://arxiv.org/pdf/2412.09082
- **项目主页链接**：https://hcplab-sysu.github.io/LH-VLN/
- **代码链接**：未提及
- **文章大致情况介绍**：论文首次提出了多阶段长期视觉语言导航（LH-VLN）任务，旨在评估和增强智能体在复杂、多阶段导航任务中的能力，这些任务需要持续的推理和适应性.开发了自动化数据生成平台NavGen，该平台能够生成具有复杂任务结构的高质量数据集，支持可扩展的任务多样性和提高数据利用率.构建了LHPR-VLN基准，包含3260个任务，每个任务平均有150个步骤，并提出了三个新的评估指标：独立成功率（ISR）、条件成功率（CSR）和基于真实值加权的CSR（CGT）.提出了多粒度动态记忆（MGDM）模块，通过结合短期和长期记忆机制，增强了模型在动态环境中的适应性和记忆处理能力.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Habitat3和Isaac Sim
- **实机验证**：使用Hello Robot的Stretch机器人和Boston Dynamics的Spot机器人进行了验证
- **数据集**：LHPR-VLN基准，包含3260个任务
- **评估标准**：独立成功率（ISR）、条件成功率（CSR）和基于真实值加权的CSR（CGT）



### 7. arXiv-2024 | 当视觉语言导航遇见自动驾驶！doScenes：基于自然语言指令的人车交互自主导航驾驶数据集
- **论文中英文名称**：doScenes: An Autonomous Driving Dataset with Natural Language Instruction for Human Interaction and Vision-Language Navigation
- **作者和单位**：
  - Parthib Roy, Srinivasa Perisetla, Shashank Shriram, Harsha Krishnaswamy, Aryan Keskar, Ross Greer
  - 加州大学默塞德分校Mi实验室
- **原文链接**：https://arxiv.org/pdf/2412.05893
- **项目主页链接**：未提及
- **代码链接**：https://www.github.com/rossgreer/doScenes
- **文章大致情况介绍**：doScenes数据集专门设计用于研究人机指令交互，特别是短期直接影响车辆运动的指令.支持在真实世界场景中的细微和灵活响应，推动了安全有效的人车协作.doScenes集通过自然语言指令和引用标签对多模态传感器数据进行标注，弥合了指令和驾驶响应之间的差距，实现了上下文感知和自适应规划.强调与静态和动态场景对象相关的可执行指令，解决了现有研究中依赖于模拟数据或预定义动作集的局限性.
- **属于无人机还是无人车**：无人车
- **仿真平台**：nuScenes数据集
- **实机验证**：未提及
- **数据集**：doScenes数据集，基于nuScenes数据集进行注释
- **评估标准**：未提及具体评估标准



### 8. 面向自然语言引导的无人机导航！GeoText-1652：基于空间关系匹配的测试基准
- **论文中英文名称**：GeoText-1652: A Benchmark for Spatial Relationship Matching in UAV Navigation Guided by Natural Language
- **作者和单位**：未提及
- **原文链接**：未提及
- **项目主页链接**：未提及
- **代码链接**：未提及
- **文章大致情况介绍**：GeoText-1652是一个测试基准，用于评估无人机在自然语言引导下的空间关系匹配能力。该基准通过模拟复杂的环境和自然语言指令，帮助研究人员开发和测试无人机的导航算法.
- **属于无人机还是无人车**：无人机
- **仿真平台**：未提及
- **实机验证**：未提及
- **数据集**：GeoText-1652数据集
- **评估标准**：未提及具体评估标准



### 9. 能说会“道”的具身导航智能体！SayNav：基于大模型动态规划的未知环境导航
- **论文中英文名称**：SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments
- **作者和单位**：
  - Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, Alvaro Velasquez
  - SRI国际，科罗拉多大学博尔德分校，DARPA
- **原文链接**：https://ojs.aaai.org/index.php/ICAPS/article/download/31506/33666
- **项目主页链接**：https://www.sri.com/ics/computer-vision/saynav
- **代码链接**：未提及
- **文章大致情况介绍**：SayNav是一种基于大语言模型（LLM）的高层规划器，用于在未知环境中进行复杂导航任务。该方法通过动态生成逐步指令和逐步构建3D场景图，实现了在大规模未知环境中的高效导航.
- **属于无人机还是无人车**：无人车
- **仿真平台**：ProcTHOR框架
- **实机验证**：未提及
- **数据集**：ProcTHOR数据集
- **评估标准**：成功率（SR）、按路径长度加权的成功率（SPL）、肯德尔tau距离（Kendall Tau）



### 10. CVPR-2023 | 视觉语言导航智能体行为分析
- **论文中英文名称**：Behavioral Analysis of Vision-and-Language Navigation Agents
- **作者和单位**：
  - Zijiao Yang, Arjun Majumdar, Stefan Lee
  - 俄勒冈州立大学，佐治亚理工学院
- **原文链接**：https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Behavioral_Analysis_of_Vision-and-Language_Navigation_Agents_CVPR_2023_paper.pdf
- **项目主页链接**：未提及
- **代码链接**：https://github.com/Yoark/vln-behave
- **文章大致情况介绍**：论文提出了一种新的方法来评估视觉语言导航（VLN）智能体的细粒度行为，通过生成特定技能的干预并测量智能体预测的变化，提供了对智能体在不同技能上的表现差异的深入理解.
- **属于无人机还是无人车**：无人车
- **仿真平台**：未提及
- **实机验证**：未提及
- **数据集**：RxR和REVERIE数据集
- **评估标准**：成功率、SPL、nDTW等VLN任务指标



### 11. ECCV-2024 | 遇事不决，就问大模型！VLN-Copilot：用LLM作为粗粒度视觉语言导航智能体的副驾
- **论文中英文名称**：LLM as Copilot for Coarse-grained Vision-and-Language Navigation
- **作者和单位**：
  - Yanyuan Qiao, Qianyi Liu, Jiajun Liu, Jing Liu, Qi Wu
  - 阿德莱德大学澳大利亚机器学习研究所，中国科学院自动化研究所，中国科学院人工智能学院，CSIRO Data61，昆士兰大学
- **原文链接**：https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf
- **项目主页链接**：未提及
- **代码链接**：未提及
- **文章大致情况介绍**：VLN-Copilot方法利用大语言模型（LLM）作为副驾，帮助智能体在面对导航困惑时获得实时详细的指导。通过引入混淆分数和LLM环境感知，使智能体能够主动寻求帮助并生成适当的指导信息.
- **属于无人机还是无人车**：无人车
- **仿真平台**：未提及
- **实机验证**：未提及
- **数据集**：REVERIE和CVDN-target数据集
- **评估标准**：成功率（SR）、按路径长度加权的成功率（SPL）、目标进度（GP）、远程锚定成功率（RGS）、按路径长度加权的远程锚定成功率（RGSPL）



### 12. CVPR-2024 EAI Workshop | 极地研究站催生的视觉语言导航！PRS：以人为中心的室内物体具身递送数据集
- **论文中英文名称**：Human-centered In-building Embodied Delivery Benchmark
- **作者和单位**：
  - Zhuoqun Xu, Yang Liu, Xiaoqi Li, Jiyao Zhang, Hao Dong
  - 极地研究中心，三星中国北京研发中心，北京大学
- **原文链接**：https://arxiv.org/pdf/2406.17898
- **项目主页链接**：https://prsorg.github.io/
- **代码链接**：https://github.com/PRS-Organization/prs-delivery
- **文章大致情况介绍**：论文提出了一种以人为中心的室内机器人递送服务的具体商业场景模拟，旨在解决私人空间内的精确高效递送问题.构建了一个全新的虚拟环境系统，该系统以极地研究站为模型，包含多层建筑空间、自主人类角色、具有抓取和移动能力的机器人，以及大量互动物品.创建了一个包含13,000条语言指令的递送数据集，用于指导机器人在虚拟环境中执行服务任务，模拟人类日常生活中的各种需求.
- **属于无人机还是无人车**：无人车
- **仿真平台**：自建虚拟环境系统
- **实机验证**：未提及
- **数据集**：PRS递送数据集，包含13,000条语言指令
- **评估标准**：成功率、解析成功率、操作成功率、目标人搜索成功率、时间等



### 13. 中科院空天院无人机视觉语言导航新基准！AeroVerse：模拟、预训练、微调和评估空中无人机具身世界模型的测试基准
- **论文中英文名称**：AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models
- **作者和单位**：
  - Fanglong Yao, Yuanchang Yue, Youzhi Liu, Xian Sun, Kun Fu
  - 中国科学院空天信息创新研究院网络信息系统技术重点实验室，中国科学院大学电子电气与通信工程学院
- **原文链接**：https://arxiv.org/pdf/2408.15511
- **项目主页链接**：未提及
- **代码链接**：未提及
- **文章大致情况介绍**：论文构建了首个大规模的真实世界图像-文本预训练数据集AerialAgent-Ego10k和虚拟图像-文本-姿态对齐数据集CyberAgent-Ego500k，定义了五个航空航天具身下游任务并构建了相应的指令数据集，开发了基于GPT-4的自动化评估方法SkyAgent-Eval，旨在增强无人航空飞行器（UAV）和其他航天平台的自主感知、认知和行动能力.
- **属于无人机还是无人车**：无人机
- **仿真平台**：AeroSimulator模拟平台
- **实机验证**：未提及
- **数据集**：AerialAgent-Ego10k、CyberAgent-Ego500k、SkyAgent系列指令数据集
- **评估标准**：BLEU、CIDEr、SPICE、LLM-Judge-Scene、LLM-Judge-Reason&Nav、LLM-Judge-Plan等



### 14. 机器狗也学视觉语言导航！NaVILA：基于分层导航动作策略的腿式机器人指令导航
- **论文中英文名称**：NAVILA: LEGGED ROBOT VISION-LANGUAGE ACTION MODEL FOR NAVIGATION
- **作者和单位**：
  - An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang
  - 加州大学圣地亚哥分校，南加利福尼亚大学，NVIDIA
- **原文链接**：https://arxiv.org/pdf/2412.04453
- **项目主页链接**：https://navila-bot.github.io/
- **代码链接**：https://github.com/yang-zj1026/legged-loco
- **文章大致情况介绍**：论文提出了NaVILA框架，结合了视觉-语言-动作模型（VLA）与运动控制的两级系统，以提高腿式机器人的导航能力.NaVILA通过VLM生成高层次动作指令，增强模型泛化性，并在VLN基准测试中显著提升成功率.
- **属于无人机还是无人车**：无人车（腿式机器人）
- **仿真平台**：Isaac Sim模拟器
- **实机验证**：在Go2机器人狗上进行了验证
- **数据集**：VLN-CE-Isaac基准测试数据集
- **评估标准**：成功率（SR）、导航误差（NE）等



### 15. 世界模型助力视觉语言导航！LeCun新作：导航世界模型
- **论文中英文名称**：Navigation World Models
- **作者和单位**：
  - Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun
  - Meta FAIR，纽约大学，布里克利AI研究
- **原文链接**：https://arxiv.org/abs/2412.03572
- **项目主页链接**：https://www.amirbar.net/nwm/
- **代码链接**：未提及
- **文章大致情况介绍**：论文提出了一种新的导航世界模型（NWM），能够根据过去的观测和导航动作预测未来的视觉观测.引入了条件扩散Transformer（CDiT），在多种环境和实体上训练，并展示了在未见环境中改进的视频预测和生成性能.
- **属于无人机还是无人车**：未明确指出，但适用于多种机器人和智能体
- **仿真平台**：未提及
- **实机验证**：未提及
- **数据集**：SCAND、TartanDrive、RECON、HuRoN机器人数据集，Ego4D无标签视频数据集
- **评估标准**：绝对轨迹误差（ATE）、相对位姿误差（RPE）、LPIPS、DreamSim、PSNR、FID、FVD等



### 16. arXiv-2024 | 吴琦教授新作！SAME：基于状态自适应专家混合模型的通用语言引导视觉导航
- **论文中英文名称**：SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts
- **作者和单位**：
  - Gengze Zhou, Yicong Hong, Zun Wang, Chongyang Zhao, Mohit Bansal, Qi Wu
  - 阿德莱德大学，Adobe Research，北卡罗来纳大学教堂山分校，新南威尔士大学悉尼分校
- **原文链接**：https://arxiv.org/pdf/2412.05552
- **项目主页链接**：未提及
- **代码链接**：https://github.com/GengzeZhou/SAME
- **文章大致情况介绍**：论文提出了State-Adaptive Mixture of Experts（SAME）模型，能够在不同粒度的语言和动态观测下推断决策，显著提升了多任务导航的性能.通过SAME模型，训练的智能体能够同时处理七个语言引导导航任务，并且在大多数任务上达到了最先进的性能.
- **属于无人机还是无人车**：无人车
- **仿真平台**：未明确提及，但使用了多个导航基准数据集进行实验
- **实机验证**：未提及
- **数据集**：R2R、RxR-EN、REVERIE、OBJECTNAV-MP3D、CVDN、SOON和R2R-CE等七个主要的导航基准数据集
- **评估标准**：轨迹长度（TL）、导航误差（NE）、成功率（SR）、路径长度加权的成功率（SPL）和基于动态时间规整加权成功率（nDTW）



### 17. MM-2024 | 智能体遇山开路，遇水架桥！ ObVLN：突破障碍，受阻环境中的视觉语言导航
- **论文中英文名称**：Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments
- **作者和单位**：
  - Haodong Hong, Sen Wang, Zi Huang
  - 昆士兰大学
- **原文链接**：https://dl.acm.org/doi/pdf/10.1145/3664647.3681640
- **项目主页链接**：未提及
- **代码链接**：https://github.com/honghd16/ObstructedVLN
- **文章大致情况介绍**：论文提出了R2R-UNO数据集，首次将指令-现实不匹配问题引入VLN任务，并提出了ObVLN方法，包括课程训练策略和虚拟图构建机制，帮助智能体有效适应障碍物环境.在R2R-UNO数据集上，ObVLN方法相比现有方法在障碍物环境中的成功率提高了23%，达到了67%的成功率.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Matterport3D模拟器
- **实机验证**：未提及
- **数据集**：R2R和R2R-UNO数据集
- **评估标准**：轨迹长度（TL）、导航误差（NE）、成功率（SR）、路径长度加权的成功率（SPL）



### 18. CoRL-2024 | InstructNav：未知环境中的零样本通用指令导航智能体
- **论文中英文名称**：InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment
- **作者和单位**：
  - Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan and Hao Dong
  - 北京大学计算机科学系，北京大学-Agibot实验室，北京大学计算机科学系多媒体信息处理国家重点实验室，东南大学自动化学院，牛津大学
- **原文链接**：https://openreview.net/pdf?id=fCDOfpTCzZ
- **项目主页链接**：https://sites.google.com/view/instructnav
- **代码链接**：https://github.com/LYX0501/InstructNav
- **文章大致情况介绍**：论文提出了InstructNav系统，能够在未探索环境中执行多种类型指令的通用指令导航系统，无需任何导航训练或预构建地图，实现了在R2R-CE任务上的首次零样本性能，并在Habitat ObjNav和需求驱动导航DDN上超越了现有的最先进方法.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Habitat模拟器
- **实机验证**：在多种室内场景中进行了实验
- **数据集**：HM3D、R2R-CE、DDN数据集
- **评估标准**：轨迹长度（TL）、导航误差（NE）、成功率（SR）、Oracle成功率（OSR）、受路径长度加权的成功率（SPL）



### 19. RSS-2024: 1ST SemRob | VLA-3D：3D语义场景理解和导航数据集
- **论文中英文名称**：VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation
- **作者和单位**：
  - Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang, Wenshan Wang
  - 卡内基梅隆大学机器人研究所
- **原文链接**：https://semrob.github.io/docs/rss_semrob2024_cr_paper12.pdf
- **项目主页链接**：未提及
- **代码链接**：https://github.com/HaochenZ11/VLA-3D
- **文章大致情况介绍**：VLA-3D数据集融合了五个真实世界数据集与Unity合成场景，涵盖超过11.5万个3D室内房间扫描，为视觉与语言导航智能体训练提供多样化数据.每个对象配备语义标签、边界框及参考性语言描述，以增强机器人对自然语言指令的理解和响应能力.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Unity
- **实机验证**：未提及
- **数据集**：VLA-3D数据集，基于ScanNet、Matterport3D、Habitat-Matterport 3D、3RScan、ARKitScenes等五个真实世界数据集
- **评估标准**：未明确提及



### 20. CoRL-2024 | LeLaN：从无标签视频中学习语言条件下的导航策略
- **论文中英文名称**：LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Videos
- **作者和单位**：
  - Noriaki Hirose, Catherine Glossop, Ajay Sridhar, Dhruv Shah, Oier Mees, Sergey Levine
  - 加州大学伯克利分校，丰田汽车北美分公司
- **原文链接**：https://arxiv.org/pdf/2410.03603
- **项目主页链接**：https://learning-language-navigation.github.io/
- **代码链接**：https://github.com/NHirose/learning-language-navigation
- **文章大致情况介绍**：LeLaN提出了一种从无标签视频中学习语言条件导航策略的方法，利用基础模型为机器人和人类导航数据添加语言和动作标注，生成的标签能够训练出在噪声指令、动态目标对象和障碍物避让方面更具鲁棒性的先进策略.
- **属于无人机还是无人车**：无人车
- **仿真平台**：未提及
- **实机验证**：在真实世界环境中进行了实验
- **数据集**：开源了超过120小时的第一视角视频数据，包括来自3个国家11个城市15小时的人工收集视频
- **评估标准**：成功率、轨迹长度、碰撞率等



### 21. arXiv-2024 | 看视频、学技能！Uni-NaVid：基于视频的视觉-语言-动作统一具身导航任务
- **论文中英文名称**：Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks
- **作者和单位**：
  - Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, He Wang
  - 北京大学，Galbot，北京智源人工智能研究院
- **原文链接**：https://arxiv.org/pdf/2412.06224
- **项目主页链接**：https://pku-epic.github.io/Uni-NaVid/
- **代码链接**：未提及
- **文章大致情况介绍**：Uni-NaVid首次提出统一多种具身导航任务的视频-语言-动作（VLA）模型，通过在四个基本的导航子任务上收集360万个导航数据样本，实现了协同学习，并在不同任务上展示了其优势.
- **属于无人机还是无人车**：无人车
- **仿真平台**：Habitat模拟器
- **实机验证**：在真实世界环境中进行了实验
- **数据集**：多任务导航数据集，包含360万个样本，分布在视觉语言导航、目标导航、具身问答和目标人跟随等任务中
- **评估标准**：成功率（SR）、oracle成功率（OS）、按路径长度加权的成功率（SPL）、轨迹长度（TL）、跟随率（FR）、碰撞率（CR）和导航误差（NE）等



### 22. AAAI-2024 | 智能体网上冲浪！ WebVLN：面向网站的视觉语言导航
- **论文中英文名称**：WebVLN: Vision-and-Language Navigation on Websites
- **作者和单位**：
  - Qi Chen，Dileepa Pitawela，Chongyang Zhao，Gengze Zhou，Hsiang-Ting Chen，Qi Wu
  - 澳大利亚机器学习研究所，阿德莱德大学
- **原文链接**：https://ojs.aaai.org/index.php/AAAI/article/view/27878
- **项目主页链接**：未提及
- **代码链接**：https://github.com/WebVLN/WebVLN
- **文章大致情况介绍**：WebVLN提出了一种面向网站的视觉语言导航任务，模拟用户在网站上的自然浏览行为，并基于输入的问题和辅助描述导航到指定目标网页，然后使用从目标网页中提取的信息回答问题.
- **属于无人机还是无人车**：不属于无人机或无人车，而是面向网站的导航任务
- **仿真平台**：WebVLN模拟器
- **实机验证**：未提及
- **数据集**：WebVLN-v1数据集，包含8,990条记录/路径和14,825个问答对
- **评估标准**：导航指标（成功率SR、最优成功率OSR、路径长度加权成功率SPL、轨迹长度TL）和问答指标（Wu-Palmer相似度WUPS）







### 23. 指令生成VS导航决策！左右互搏！SRDF：基于自我精炼数据飞轮的语言引导式导航学习
- **论文中英文名称**：
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/00QDU9NgLTrHUpSloP9HfwCVPR-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 24. 具身导航模型大一统！NaviLLM：学习迈向具身导航的通用模型
- **论文中英文名称**：NaviLLM: Learning Towards a Unified Embodied Navigation Model
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/f7eDlfKEVmoQApX6edPU0wACL
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：NaviLLM旨在通过学习迈向具身导航的通用模型，整合多种导航任务和环境，以实现更高效的导航能力.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 25. Findings-2024 | 导航智能体知错就改！CorNav：具有自我校正规划的zero-shot视觉语言导航智能体
- **论文中英文名称**：CorNav: Zero-shot Vision-Language Navigation Agent with Self-Correction Planning
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/2Fvz3FPMDJh3mdTwEC4FVQarXiv-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：CorNav提出了一种具有自我校正规划的zero-shot视觉语言导航智能体，能够在导航过程中自动检测并纠正错误，提高导航的准确性和鲁棒性.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 26. NavAgent: 基于多尺度城市街道视图融合的无人机视觉语言导航
- **论文中英文名称**：NavAgent: Drone Vision-Language Navigation Based on Multi-scale Urban Street View Fusion
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/aESSQGYklNJ-vNAWaR_49AECCV-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：NavAgent通过融合多尺度城市街道视图，实现无人机在复杂城市环境中的视觉语言导航，提高了导航的准确性和适应性.
- **属于无人机还是无人车**：无人机
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 27. 多模态大模型助力具身导航！NavGPT-2：释放视觉语言大模型的导航推理能力
- **论文中英文名称**：NavGPT-2: Unleashing the Navigation Reasoning Power of Vision-Language Large Models
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/Unn_qj8ly5l1x4cRHYZvdgAAAI-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：NavGPT-2利用多模态大模型的推理能力，通过整合视觉和语言信息，提升具身导航的准确性和效率，能够更好地理解和执行复杂的导航任务.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 28. 大语言模型赋能导航决策！NavGPT：基于大模型显式推理的视觉语言导航
- **论文中英文名称**：NavGPT: Vision-Language Navigation with Explicit Reasoning of Large Models
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/t2eLYHgsH91JkhtVVjLizAECCV-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：NavGPT通过大语言模型的显式推理，增强视觉语言导航的决策能力，能够更准确地理解和执行导航指令，提高导航的鲁棒性和适应性.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 29. 指令不够用、大模型来生成！BEVInstructor：基于BEV感知和大模型的视觉语言导航指令生成
- **论文中英文名称**：BEVInstructor: Vision-Language Navigation Instruction Generation Based on BEV Perception and Large Models
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/Q_9szEvObzu4kI72hzAWvAarXiv-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：BEVInstructor利用基于BEV感知的大模型生成视觉语言导航指令，能够根据环境变化和任务需求生成更丰富、更准确的导航指令，提高导航的灵活性和有效性.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 30. ICRA-2024 | VLFM：基于视觉-语言边界地图的零样本语义导航
- **论文中英文名称**：VLFM: Zero-shot Semantic Navigation Based on Vision-Language Boundary Maps
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/csqT-qSd6IxbC7cXnsB9TAarXiv-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：VLFM提出了一种基于视觉-语言边界地图的零样本语义导航方法，通过构建边界地图来理解环境的语义结构，实现对未知环境的导航.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：
- **评估标准**：

### 31. 具身智能体要上天！CITYNAV：基于地理信息的无人机视觉语言导航数据集
- **论文中英文名称**：CITYNAV: Drone Vision-Language Navigation Dataset Based on Geographical Information
- **作者和单位**：
- **原文链接**：https://mp.weixin.qq.com/s/GRT8zU93QqoUYLy_z57g8gECCV-2024
- **项目主页链接**：
- **代码链接**：
- **文章大致情况介绍**：CITYNAV构建了一个基于地理信息的无人机视觉语言导航数据集，提供了丰富的地理和视觉信息，支持无人机在城市环境中的导航研究.
- **属于无人机还是无人车**：
- **仿真平台**：
- **实机验证**：
- **数据集**：CITYNAV数据集
- **评估标准**：
