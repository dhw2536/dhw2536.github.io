---
layout:     post
title:      冰达机器人-ROS调包侠修炼计划
subtitle:   涉及了机器人技术和计算机视觉领域的多个知识点，可以找一些感兴趣的试试
date:       2024-11-12
author:     Space
header-img: img/the-first.png
catalog:   true
tags:
    - 冰达机器人


---



# 冰达机器人 -ROS调包侠修炼计划

链接：[ROS调包侠修炼计划](https://bingda.yuque.com/staff-hckvzc/ai5gkn?)

本部分是关于ROS（Robot Operating System）功能包的列表，每个功能包都针对特定的机器人应用。每个项目及其用途：

1. **瑞芯微NPU运行YOLO v5项目介绍**：
   - 这个项目介绍如何在瑞芯微的NPU（神经处理单元）上运行YOLO v5模型进行物体检测。YOLO v5是一个流行的实时物体检测系统，而NPU是专门为机器学习任务设计的硬件加速器。

2. **玩转AI(2)-ncnn_ros功能包**：
   - `ncnn_ros`是一个ROS功能包，它集成了ncnn框架，用于在ROS中运行深度学习模型，如图像分类和目标检测。

3. **玩转AI(1)-腾讯ncnn编译安装**：
   - 这个项目提供了编译和安装腾讯ncnn框架的指南，ncnn是一个为移动端和嵌入式设备优化的深度学习推理框架。

4. **laser_filters激光雷达数据屏蔽功能包**：
   - `laser_filters`是一个ROS功能包，用于过滤激光雷达数据，移除机器人本体或其他不需要的部分，以便更准确地检测障碍物。

5. **AprilTag二维码检测和定位**：
   - AprilTag是一个视觉标记系统，用于机器人定位。这个项目介绍如何使用AprilTag进行二维码检测和机器人定位。

6. **KCF目标跟踪**：
   - KCF（Kernelized Correlation Filters）是一种目标跟踪算法，用于在视频流中跟踪目标物体。

7. **robot-upstart开机自启动**：
   - `robot-upstart`是一个工具，用于将ROS节点设置为在系统启动时自动运行，确保机器人系统开机即可运行。

8. **rrt_exploration自主探索建图**：
   - 这个项目使用RRT（Rapidly-exploring Random Tree）算法，使机器人能够自主探索环境并构建地图。

9. **orb_slam2无伤速通(1)--安装**：
   - 介绍如何在ROS中安装ORB-SLAM2，这是一个基于特征点的SLAM（Simultaneous Localization and Mapping）系统。

10. **orb_slam2无伤速通(2)--单目相机SLAM**：
    - 介绍如何使用单目相机进行SLAM，ORB-SLAM2是一个多功能的SLAM系统，支持单目、双目和RGB-D相机。

11. **orb_slam2无伤速通(3)--单目相机纯定位**：
    - 介绍如何使用ORB-SLAM2进行单目相机的纯定位，即在没有构建地图的情况下进行定位。

12. **rosdep一键修复**：
    - `rosdep`是一个工具，用于解决ROS包的依赖问题。这个项目提供了一键修复脚本，帮助解决常见的依赖问题。

13. **ROS中使用摄像头**：
    - 介绍如何在ROS中使用摄像头，包括安装驱动、获取图像数据等。

14. **网页显示摄像头图像--web_video_server**：
    - `web_video_server`是一个ROS功能包，允许用户通过网络浏览器查看ROS中的摄像头图像。

15. **摄像头标定--camera_calibration**：
    - 介绍如何在ROS中对摄像头进行标定，以确保图像数据的准确性和一致性。

这些功能包和项目涵盖了机器人视觉、导航、定位、物体检测和跟踪等多个方面，是构建复杂机器人系统的基础组件。

![image-20241112185904891](https://raw.githubusercontent.com/dhw2536/Picture/main/202411121900908.png)

### 瑞芯微NPU运行YOLO v5项目介绍

**项目概述**：
这是一个利用瑞芯微NPU（神经处理单元）运行YOLO v5模型的项目。它通过RKNN Toolkit2生成rknn模型，并在RK3588、RK3588S、RK3568和RK3566等SOC上测试通过。

**关键特性**：
- 使用`rknn_ros`包，它基于官方Demo实现，移除了RGA库依赖，改用OpenCV，并增加了ROS的图像输入和输出接口。
- 支持多种RKNN模型，包括RK1808/RV1109/RV1126/RK3399Pro等。
- 提供详细的安装指南和测试说明，确保用户可以在不同硬件平台上运行YOLO v5。

**安装与运行**：

- 将`rknn_ros`源码克隆到ROS工作空间的`src`目录中。
- 编译并安装`rknn_ros`包。
- 根据使用的SOC型号设置`chip_type`参数。
- 启动摄像头并运行YOLO v5模型，终端会输出检测到的物体信息。

**使用指南**：
- 确保ROCK Pi 3A的NPU已开启，其他板卡无需此操作。
- 默认摄像头设备号为`video0`，根据板卡不同可能需要调整。
- 运行YOLO v5时，根据SOC型号选择正确的`chip_type`。
- 检测结果将通过`/objects`话题发布，可以通过`rostopic echo /objects`查看。

**版本更新**：
- 项目维护已转移到[airockchip/rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2/tree/master/rknpu2)。
- 提供了详细的版本更新日志，包括性能优化和新功能添加。

**资源链接**：
- [RKNN Toolkit2](https://github.com/rockchip-linux/rknn-toolkit2)
- [RKNN Toolkit](https://github.com/rockchip-linux/rknn-toolkit)
- [RKNPU](https://github.com/rockchip-linux/rknpu)
- [RK3399Pro_npu](https://github.com/airockchip/RK3399Pro_npu)

通过这个项目的学习和使用，用户可以利用瑞芯微NPU的强大计算能力，在机器人或其他嵌入式设备上实现高效的物体检测。



### 玩转AI(2)-ncnn_ros功能包

**项目简介**：
ncnn_ros是一个基于ncnn框架的ROS功能包，用于在机器人或嵌入式设备上运行深度学习模型。它支持多种算法模型，包括NanoDet、YOLOv3、YOLOv5、YOLOX和YOLOv7，以及YOLOv5用于实例分割的YOLOact算法。

**主要特点**：
1. **算法支持**：适配了多种流行的目标检测和分割算法。
2. **性能优化**：移除了GUI显示部分，改为ROS话题发布，提高了检测速度。
3. **参数配置**：增加了参数调节配置接口，方便用户根据需求调整。
4. **消息类型**：单独定义了消息类型，便于跨设备订阅话题。

**安装指南**：
1. **环境准备**：确保ncnn框架已正常编译。
2. **源码获取**：获取ncnn_ros和object_information_msgs源码。
3. **编译工作空间**：在工作空间编译ncnn_ros包。
4. **模型文件**：将模型文件放在用户主目录中，可以从GitHub下载或从提供的压缩包解压。

**运行示例**：
- 启动摄像头：`roslaunch ncnn_ros camera.launch`
- 启动检测节点，例如NanoDet：`roslaunch ncnn_ros nanodet.launch`

**性能测试**：
- 性能测试是在开发过程中进行的，可能不具备严格的对比价值。

**ncnn框架特性**：
- 专为移动平台优化的高性能神经网络推理框架。
- 无第三方依赖，跨平台，运行速度快。
- 支持多种网络结构和量化技术。
- 支持GPU加速，包括Vulkan API。
- 支持自定义层实现和扩展。

**使用场景**：
ncnn_ros功能包适用于需要在ROS环境下进行深度学习模型部署的场景，如机器人视觉检测、智能监控等。

**注意事项**：
- 安装前需要确保ncnn框架已编译。
- 模型文件较大，下载可能需要较长时间，具体取决于网络状况。

**资源链接**：
- [ncnn GitHub](https://github.com/Tencent/ncnn)
- [ros_ncnn GitHub](https://github.com/nilseuropa/ros_ncnn)
- [YOLOX ncnn模型下载](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s_ncnn.tar.gz)（由于网络原因，可能无法直接访问，建议检查链接的合法性并适当重试）

通过ncnn_ros功能包，用户可以在ROS环境中轻松部署和使用ncnn框架支持的深度学习模型，实现高效的机器学习和计算机视觉任务。



### 玩转AI(1)-腾讯ncnn编译安装

**项目简介**：
ncnn是一个专为移动平台设计的高性能神经网络推理框架，由腾讯开源。它无第三方依赖，跨平台，且在手机CPU上的运行速度远超其他已知开源框架。ncnn使得开发者能够轻松将深度学习算法移植到手机端，开发出智能APP，将AI技术带到用户的指尖。

**主要特点**：
- 专为手机端优化的高性能神经网络前向计算框架。
- 无第三方依赖，跨平台，运行速度快。
- 已被腾讯多款应用采用，如QQ、Qzone、微信、天天P图等。

**编译安装步骤**：
1. **安装编译依赖**：确保所有必要的依赖软件已安装。
2. **获取源码**：可以通过Git Clone或直接下载压缩包的方式获取ncnn源码。
3. **编译源代码**：在Ubuntu环境下，创建并跳转到`build`目录，开启NCNN_VULKAN以编译GPU版本。
4. **编译安装**：编译完成后，进行安装结果测试验证。

**测试与验证**：
- 跳转到`example`目录，执行`squeezenet`测试以验证安装结果。
- 进行`benchmark`网络性能测试（可选，不影响后续例程运行）。

**性能测试**：
- 不同硬件平台上的测试结果可能有所不同，如RK3399、RK3588、全志H6和树莓派4B-4GB等。
- 对于RK3588这类大小核心且大核性能强的CPU，调整`powersave`和`num_threads`参数可能会得到更好的性能表现。

**资源链接**：
- [ncnn GitHub](https://github.com/Tencent/ncnn)

通过上述步骤，用户可以在自己的设备上编译安装ncnn，进而利用其强大的神经网络推理能力，开发智能APP或进行深度学习模型的部署和测试。



### laser_filters激光雷达数据屏蔽功能包

**功能简介**：
`laser_filters`是ROS（Robot Operating System）中的一个功能包，用于处理和过滤激光雷达（LIDAR）的原始数据。它可以帮助屏蔽掉雷达扫描到的机器人本体或其他不需要的数据区域，从而获取更准确的障碍物信息，用于SLAM建图或机器人导航。

**主要特点**：
- 支持多种屏蔽方式，包括角度范围屏蔽和BOX范围屏蔽。
- 可以创建自定义的配置文件和启动文件，方便集成到ROS环境中。

**安装步骤**：
1. **安装laser_filters**：通过ROS的package管理工具安装`laser_filters`包。
2. **创建功能包**：创建一个新的ROS功能包，用于存放launch文件和配置文件，例如`laser_filter_test`。

**使用方法**：
- **角度范围屏蔽**：
  - 在`config`目录中创建`angle.yaml`配置文件，设置需要屏蔽的角度范围。
  - 在`launch`目录中创建`angle_filter.launch`文件，用于启动角度屏蔽节点。
  - 启动雷达角度屏蔽节点，通过RViz可视化过滤效果。

- **BOX范围屏蔽**：
  - 在`config`目录中创建`box.yaml`配置文件，设置需要屏蔽的空间BOX范围。
  - 在`launch`目录中创建`box_filter.launch`文件，用于启动BOX屏蔽节点。
  - 启动雷达BOX屏蔽节点，通过RViz可视化过滤效果。

**效果验证**：
- 通过RViz配置页面，将原始数据和过滤后的数据显示为不同颜色，验证屏蔽效果。
- 对于BOX范围屏蔽，可以在RViz中观察到在设定的BOX范围内，原始数据检测到物体，而过滤后的数据则没有显示。

**资源链接**：
- [laser_filters - ROS Wiki](http://wiki.ros.org/laser_filters)

通过使用`laser_filters`功能包，用户可以有效地处理和过滤激光雷达数据，去除不必要的干扰信息，提高机器人SLAM建图和导航的准确性和可靠性。



### AprilTag二维码检测和定位

**AprilTag简介**：
AprilTag是一种视觉标记系统，类似于二维码QR codes，但它不仅能存储信息，还能计算相对于相机的三维位置。AprilTag可用于相机标定、目标大小估计、单目距离测量等多种用途。与二维码不同，AprilTag检测程序可以识别并计算标签的三维位置和所承载的ID信息。

**功能包安装**：
- 使用ROS的包管理工具安装`apriltag_ros`包，这是ROS中的AprilTag检测功能包。
- 为了方便使用，可以创建一个新的功能包来存放相机启动、标定文件以及`apriltag_ros`的配置文件。

**使用前准备**：
1. **功能包目录结构**：
   - `launch`：包含启动相机和相关AprilTag检测的launch文件。
   - `config`：存放相机标定文件和`apriltag_ros`的配置文件，如`settings.yaml`和`tags.yaml`。
   - `doc`：存放标签族的图像文件，方便直接使用。

2. **相机标定文件**：
   - 替换`config`目录中的相机标定文件`ost.yaml`，确保使用实际的相机标定结果。

3. **AprilTag标签**：
   - 打印`doc`目录中的AprilTag标签，并将其放置在相机前进行检测。

**运行AprilTag检测**：
- 启动相机和AprilTag检测程序。
- 使用`rqt_image_view`订阅`/tag_detections_image`话题，查看检测到的标签。
- 在RViz中设置Fixed Frame为`base_footprint`，观察标签和相机的坐标位置关系。
- 通过订阅`/tag_detections`话题，获取检测到的标签的位置信息。

**应用场景**：
- 利用AprilTag检测到的准确位置关系，可以完成目标跟踪、视觉抓取等高级视觉任务。

**资源链接**：
- [apriltag_ros - ROS Wiki](http://wiki.ros.org/apriltag_ros)

AprilTag提供了一种强大的工具，用于在机器人视觉系统中实现精确的定位和识别，极大地扩展了机器视觉的应用范围。



### KCF目标跟踪

**项目简介**：
KCF（Kernelized Correlation Filters）目标跟踪算法是一种在计算机视觉中用于目标跟踪的算法。它利用图像的深度信息来提高跟踪的准确性。本项目是基于ROS（Robot Operating System）的KCF目标跟踪算法的实现，使用深度相机进行目标追踪。

**主要特点**：
- 使用深度相机获取图像深度信息，提高跟踪精度。
- 通过目标框大小估算深度，适用于普通话彩色相机。
- 提供launch文件，支持remap方式修改话题名，方便集成和使用。

**安装步骤**：
1. **获取源码**：从GitHub克隆`tracker_kcf_ros`项目的源码。
2. **安装依赖软件**：安装运行KCF跟踪算法所需的依赖。
3. **编译**：在工作空间中编译源码，生成可执行文件。

**使用方法**：
1. **启动摄像头**：默认使用`video0`，可通过`device`参数指定其他设备号。
2. **启动KCF追踪节点**：在有屏幕的终端（如PC）中执行，会打开图像显示框。
3. **框选目标**：在图像框中使用鼠标框选需要追踪的目标。
4. **移动摄像头或目标**：追踪框会跟随目标移动。
5. **计算速度**：节点会根据追踪框中心位置相对图像中心的位置关系计算机器人需要的角速度和线速度，并发布到`cmd_vel`话题。

**实体机器人上运行追踪**：
- 确保机器人上已经编译安装了`kcf_track`。
- 确保机器人和PC之间已经配置好分布式通信。
- 启动机器人上的摄像头，使用`roslaunch kcf_track rgb_camera.launch`。
- 启动机器人底盘节点，如`roslaunch base_control base_control.launch`。
- 启动KCF追踪节点，使用`roslaunch kcf_track kcf_track.launch`。

**参数调整**：
- 修改`kcf_track/src/runtracker.cpp`文件中`imageCb`函数，调整中心点、大小比例和速度的换算关系。

**资源链接**：
- [tracker_kcf_ros GitHub](https://github.com/TianyeAlex/tracker_kcf_ros)

通过使用本项目，用户可以在ROS环境中实现高效的目标跟踪，适用于多种机器人视觉应用场景，如导航、监控和交互等。



### robot-upstart开机自启动

**项目简介**：
`robot-upstart`是一个ROS（Robot Operating System）软件包，它提供脚本帮助用户在Ubuntu Linux系统上安装和卸载启动ROS节点的`upstart`作业。这些作业在系统启动时运行，使得机器人相关的ROS节点能够自动启动。

**主要功能**：
- 自动生成背景作业，以在系统启动时运行`roslaunch`文件。
- 提供命令行工具和Python API，方便用户操作。

**安装步骤**：
1. **创建启动用的功能包和launch文件**：创建一个新的ROS功能包，例如`base_upstart`，用于存放开机启动的launch文件。
2. **编写launch文件**：将需要启动的节点或launch文件写入`base_upstart/launch`目录。
3. **编译工作空间**：确保工作空间编译无误。
4. **验证launch文件**：确保编写的launch文件可以正确运行。

**使用方法**：
- 使用`rosrun robot_upstart install`命令安装启动作业，例如：
  ```
  rosrun robot_upstart install base_upstart/launch/base_upstart.launch --job base_upstart --interface wlan0 --logdir ~/base_upstart.log
  ```
  该命令会创建一个名为`base_upstart`的作业，并指定网络接口和日志文件目录。

**服务管理**：
- 启动服务：`sudo systemctl start base_upstart`
- 停止节点工作：`sudo systemctl stop base_upstart`
- 移除自启动任务：`roscore` 后使用`rosrun robot_upstart uninstall base_upstart`

**注意事项**：
- 修改源launch文件不会影响开机启动，因为开机启动所使用的launch文件位于`/etc/ros/noetic/base_upstart.d/`目录下。
- 确保使用的ROS版本与`robot_upstart`包支持的版本一致。目前支持的版本包括ROS 1的Jade和ROS 2的Humble、Iron、Rolling。

**资源链接**：
- [robot_upstart - ROS Wiki](http://docs.ros.org/jade/api/robot_upstart/html/)
- [GitHub - clearpathrobotics/robot_upstart](https://github.com/clearpathrobotics/robot_upstart.git)

通过使用`robot-upstart`，机器人管理员可以轻松设置ROS节点在系统启动时自动运行，简化了机器人软件的部署和管理过程。



### rrt_exploration自主探索建图

**项目简介**：
`rrt_exploration`是一个ROS（Robot Operating System）包，用于实现多机器人地图探索算法。它基于Rapidly-Exploring Random Tree（RRT）算法，使用占用网格作为地图表示。该包包含五个不同的ROS节点，包括全局和局部RRT前沿点检测节点、基于OpenCV的前沿点检测节点、过滤节点和分配节点。

**主要功能**：
- **全局RRT前沿点检测节点**：在地图上找到探索目标点并发布。
- **局部RRT前沿点检测节点**：快速检测机器人附近的前沿点。
- **OpenCV基础前沿点检测节点**：使用OpenCV工具检测前沿点。
- **过滤节点**：接收所有检测器的前沿点，过滤并传递给分配节点。
- **分配节点**：接收过滤后的前沿点，并命令机器人进行探索。

**安装步骤**：
1. **安装依赖软件**：
   - 对于Melodic版本和Noetic版本，需要安装相应的依赖。
2. **获取`rrt_exploration`源码**：
   - 从GitHub克隆源码，并根据需要修改以适应Noetic版本。
3. **获取仿真环境和SLAM及导航源码**：
   - 用于测试和运行`rrt_exploration`的仿真环境和相关源码。
4. **编译工作空间**：
   - 在工作空间中编译`rrt_exploration`包。
5. **配置环境变量**：
   - 设置环境变量以确保仿真环境中的机器人和导航参数一致。

**使用方法**：
1. **仿真环境中运行**：
   - 运行仿真环境和建图导航，使用RViz检查SLAM正常运行并建立部分地图。
2. **运行`rrt_exploration`**：
   - 在RViz中点击“Publish Point”按钮，设置地图边界和目标搜索方向。
3. **参数调整**：
   - 调整`rrt_exploration`中的参数以及导航参数，以适应不同的机器人和环境。

**注意事项**：
- 确保使用的ROS版本与`rrt_exploration`包支持的版本一致。
- 在仿真环境中测试`rrt_exploration`之前，确保SLAM和导航功能正常运行。

**资源链接**：
- [rrt_exploration - ROS Wiki](http://wiki.ros.org/rrt_exploration)
- [Bilibili视频教程](https://www.bilibili.com/video/BV19i4y1w7FQ?p=3&vd_source=084ec502bd049c8b6560f1662f6c5317)

通过使用`rrt_exploration`，用户可以有效地设置机器人进行自主探索和环境建图，适用于多种机器人视觉应用场景，如未知环境的探索和地图构建。



### orb_slam2无伤速通(1)--安装

**项目简介**：
`orb_slam2_ros`是一个将ORB-SLAM2 SLAM（Simultaneous Localization and Mapping，即同时定位与建图）库集成到ROS（Robot Operating System）中的ROS包。ORB-SLAM2是一个适用于单目、双目和RGB-D相机的实时SLAM库，能够计算相机轨迹和稀疏3D重建（在双目和RGB-D情况下带有真实尺度）。该库能够实时检测环路并重新定位相机。此ROS实现移除了Pangolin依赖和原始查看器，所有数据I/O通过ROS话题处理。可视化可通过RViz实现。

**安装步骤**：
1. **获取源码**：
   - 将`orb_slam2_ros`源码克隆到ROS工作空间的`src`目录下。
   ```bash
   cd ~/catkin_ws/src
   git clone https://ghproxy.com/github.com/appliedAI-Initiative/orb_slam_2_ros.git
   ```
2. **使用rosdep解决依赖项**：
   - 使用rosdep安装`orb_slam2_ros`包及其依赖。
   ```bash
   rosdep install --from-paths orb_slam_2_ros --ignore-src -r -y
   ```
3. **安装编译所需的依赖软件**：
   - 安装Eigen3开发库。
   ```bash
   sudo apt install libeigen3-dev
   ```
4. **编译工作空间**：
   - 在工作空间目录下编译，确保无报错。
   ```bash
   cd ~/catkin_ws
   catkin_make
   ```

**注意事项**：
- 确保ROS环境已正确安装，并且`catkin_ws`工作空间已创建。
- 使用`rosdep`安装依赖项时，如果遇到问题可以参考相关文档进行修复。
- 编译工作空间时，确保所有必要的依赖都已正确安装。

**资源链接**：
- [ORB-SLAM2 ROS Wiki](http://wiki.ros.org/orb_slam2_ros)

通过上述步骤，用户可以在ROS环境中快速安装并运行ORB-SLAM2，实现机器人的实时定位和地图构建功能。



### orb_slam2无伤速通(2)--单目相机SLAM

**项目简介**：
这一部分介绍如何使用`orb_slam2_ros`包实现单目相机的SLAM（Simultaneous Localization and Mapping，即同时定位与建图）。ORB-SLAM2是一个适用于单目、双目和RGB-D相机的实时SLAM库，可以计算相机轨迹和稀疏3D重建。在单目相机SLAM中，ORB-SLAM2利用单个摄像头捕获的图像序列来估计相机的运动并构建环境地图。

**实验前准备**：
1. **单目摄像头**：可以使用笔记本自带摄像头或USB摄像头。
2. **摄像头标定**：确保摄像头已经完成标定。标定过程可参考“摄像头标定--camera_calibration”。

**文件准备**：
- 创建一个新的ROS功能包`bingda_orb_slam`，用于存放launch文件和其他相关文件。
- 在功能包中创建`launch`、`config`和`map`三个目录。
- 在`launch`目录中创建`mono_camera.launch`文件，并写入相应的配置。
- 在`config`目录中放置摄像头标定参数文件`ost.yaml`。
- 创建从`base_link`到`usb_cam`的TF变换。

**启动单目相机SLAM**：
1. **启动摄像头**：使用相应的launch文件启动摄像头。
2. **启动SLAM**：使用`mono_camera.launch`文件启动SLAM过程。
3. **RViz可视化**：
   - 打开RViz，订阅`/orb_slam2_mono/debug_image`图像话题。
   - 增加TF显示，订阅`/orb_slam2_mono/map_points`点云数据。

**初始化和操作**：
- 初始状态下，RViz会显示TF警告，图像上会显示“TRYING TO INITIALIZE”。
- 缓慢晃动摄像头以完成ORB-SLAM2的初始化。
- 初始化完成后，RViz将显示TF变换关系，图像中用绿色方框标记特征点。

**注意事项**：
- 单目相机不具备深度信息，因此构建的地图与真实世界尺度不匹配。
- ORB-SLAM2构建的地图是稀疏点云地图，不适合用于需要稠密点云的导航等场合。
- 在相机快速旋转等情况下，可能会导致定位跟踪丢失。

**保存地图**：
- 调用`/orb_slam2_mono/save_map`服务将当前地图保存为`map.bin`。
- 保存的地图位于`~/.ros`目录中，可以将其移动到`bingda_orb_slam/map`目录中以方便使用。

**资源链接**：
- [ORB-SLAM2 ROS Wiki](http://wiki.ros.org/orb_slam2_ros)

通过上述步骤，用户可以在ROS环境中使用单目相机实现SLAM，构建环境的稀疏地图并估计相机的运动轨迹。



### orb_slam2无伤速通(3)--单目相机纯定位

**项目简介**：
这一部分介绍如何使用`orb_slam2_ros`包实现单目相机的纯定位。纯定位模式下，ORB-SLAM2将使用已建立的地图文件来估计相机的当前位置，而不会继续构建新的地图。这对于需要快速定位的场景非常有用，例如在已经探索过的环境中进行导航。

**实验前准备**：
- 确保硬件准备与“单目相机SLAM”相同，包括单目摄像头和摄像头标定文件。
- 准备好已建立的ORB-SLAM2地图文件。

**文件准备**：
- 在`bingda_orb_slam`功能包的`launch`目录中创建`orb2_mono_localize.launch`文件。
- 修改文件，设置`localize_only`参数为`true`，`load_map`参数为`true`，并指定`map_file`参数为地图文件的路径。

**启动单目相机纯定位**：
1. **启动摄像头**：
   - 使用`mono_camera.launch`文件启动摄像头。
   ```bash
   roslaunch bingda_orb_slam mono_camera.launch
   ```
2. **启动SLAM**：
   - 使用`orb2_mono_localize.launch`文件启动纯定位模式。
   ```bash
   roslaunch bingda_orb_slam orb2_mono_localize.launch
   ```
3. **RViz可视化**：
   - 打开RViz，配置与SLAM中相同的显示。
   - 晃动相机完成初始化后，可以观察到相机在不同位姿下相对于地图的坐标变换，与SLAM中一致，实现了定位功能。

**注意事项**：
- 确保地图文件正确加载，且摄像头标定参数准确。
- 在纯定位模式下，ORB-SLAM2不会更新地图，只进行定位。

**资源链接**：
- [ORB-SLAM2 ROS Wiki](http://wiki.ros.org/orb_slam2_ros)

通过上述步骤，用户可以在ROS环境中使用单目相机进行纯定位，利用已有的地图文件快速准确地确定相机的当前位置。这对于需要快速定位的应用场景非常有用。



### rosdep一键修复

**问题描述**：
`rosdep`是ROS（Robot Operating System）中用于安装依赖项的命令行工具。在国内使用时，由于网络限制，经常遇到无法访问`raw.githubusercontent.com`的问题，导致`rosdep init`操作失败。

**解决方案**：
使用`rosdep-fix`工具，该工具通过以下步骤解决`rosdep`访问问题：
1. **查找IP地址**：通过`rosdep-fix`脚本，查找`raw.githubusercontent.com`对应的IP地址，并建立域名解析规则。
2. **代理访问**：使用`ghproxy.com`代理访问无法直接访问的资源。

**使用方法**：
1. **克隆`rosdep-fix`仓库**：
   ```bash
   git clone https://gitee.com/bingda_ai/rosdep-fix.git
   ```
2. **进入`rosdep-fix`目录并执行脚本**：
   ```bash
   cd rosdep-fix/
   sudo ./$ROS_DISTRO-rosdep_fix.sh
   ```
   其中`$ROS_DISTRO`替换为你的ROS发行版名称，例如`noetic`、`melodic`等。

**执行结果**：
- 执行完成后，无需再执行`sudo rosdep init`，直接执行`rosdep update`即可。

**后续操作**：
- 检查依赖项是否安装成功：
  ```bash
  rosdep check --from-paths src --ignore-src -r -y
  ```
- 安装依赖项：
  ```bash
  rosdep install --from-paths src --ignore-src -r -y
  ```

**注意事项**：
- 确保`rosdep-fix`脚本中的ROS发行版名称与你的环境一致。
- 执行脚本时可能需要管理员权限。

**资源链接**：
- [rosdep-fix GitHub 镜像](https://gitee.com/bingda_ai/rosdep-fix)

通过上述步骤，用户可以在国内环境中顺利使用`rosdep`，解决因网络限制导致的依赖项安装问题。



### ROS中使用摄像头

**摄像头使用概述**：
在ROS（Robot Operating System）中使用摄像头，首先需要确保摄像头在Linux系统下是可识别的。大多数UVC协议的USB摄像头都可以直接使用，包括大多数笔记本电脑自带的摄像头。对于具有CSI或MIPI接口的嵌入式板卡，接入对应接口的摄像头后可能也可以使用，但这取决于具体的硬件和驱动支持。

**验证摄像头设备可用性**：
- 使用命令`ls /dev/video*`检查当前设备列表，确认是否有`video*`设备，这表示摄像头设备已被系统正确识别。
- 使用Ubuntu下的拍照软件`cheese`进一步验证摄像头是否正常工作。

**获取摄像头驱动包**：
ROS提供了多个摄像头驱动包，如`usb_cam`和`uvc_camera`。`usb_cam`包可以通过apt直接安装，而`uvc_camera`在Melodic版本中有提供二进制安装包，但在Noetic版本中需要通过源码编译安装。

**安装`usb_cam`**：
```bash
sudo apt-get install ros-noetic-usb_cam
```
安装完成后，可以通过`rosrun`或`roslaunch`方式运行摄像头节点。

**使用`usb_cam`**：
- 启动`roscore`，然后运行`usb_cam_node`。
- 使用`rqt`工具查看图像。

**使用`uvc_camera`**：
- 在Melodic版本中，可以通过apt安装二进制包。
- 在Noetic版本中，需要从源码编译安装。

**多摄像头设置**：
当有多个摄像头或需要使用的摄像头设备不是`/dev/video0`时，可以通过编写`launch`文件来启动，修改`video_device`参数的值来指定不同的摄像头设备。

**带压缩的图像话题**：
如果安装了`compressed-image-transport`功能包，摄像头节点会发布压缩后的图像话题，如`/usb_cam/image_raw/compressed`，这种压缩格式在跨设备或带宽资源有限的网络中传输时会更流畅。

**资源链接**：
- [uvc_camera - ROS Wiki](http://wiki.ros.org/uvc_camera)
- [usb_cam - ROS Wiki](https://wiki.ros.org/usb_cam)

通过上述步骤，用户可以在ROS环境中使用摄像头，进行图像采集和传输，为机器人视觉和导航等应用提供支持。



### 网页显示摄像头图像--web_video_server

**功能介绍**：
`web_video_server`是ROS（Robot Operating System）中的一个功能包，它允许用户通过网络访问ROS中发布的图像和视频话题。这个服务特别有用，因为它允许用户在不安装ROS的情况下，通过网页查看ROS系统中的图像数据。

**安装步骤**：
1. **安装`web_video_server`**：
   - 使用apt包管理器安装，替换命令中的ROS版本字段（如`noetic`）以匹配你的ROS版本。
   ```bash
   sudo apt-get install ros-noetic-web_video_server
   ```

2. **启动摄像头节点**：
   - 确保摄像头已正确连接，并启动相应的ROS摄像头节点。

3. **启动`web_video_server`节点**：
   - 启动`web_video_server`节点，它将订阅摄像头节点的图像话题，并提供HTTP服务以便通过网络访问。

4. **访问图像**：
   - 使用`ifconfig`命令查看设备的IP地址。
   - 在浏览器中输入`http://<IP地址>:8080`，其中`<IP地址>`是设备的IP地址，`8080`是`web_video_server`默认的端口号。
   - 访问`http://192.168.31.132:8080/stream_viewer?topic=/usb_cam/image_raw`来预览当前图像。

**参数调整**：
- 在浏览器地址栏中，可以通过添加参数来调整图像格式、尺寸和质量。
- 例如，添加`quality=10`参数可以调整图像质量，牺牲一定的图像品质以换取更流畅的画面。

**资源链接**：
- [web_video_server - ROS Wiki](http://wiki.ros.org/web_video_server)

通过上述步骤，用户可以在ROS环境中使用`web_video_server`功能包，通过网络浏览器远程查看摄像头捕获的图像，无需在每台设备上安装ROS，极大地提高了图像查看的便利性。



### 摄像头标定--camera_calibration

**摄像头标定简介**：
摄像头标定是确保相机成像和三维空间中位置关系对应准确的重要步骤，尤其在对成像和尺寸测量要求严格的场合（如视觉SLAM、尺寸检测等）。标定过程需要计算相机感光元件和镜头制造产生的误差，以获得像素和物体尺寸之间的换算参数。

**标定准备**：
1. **标定板**：需要准备一块标定板，可以购买成品或自制。
2. **摄像头调焦**：确保相机焦距固定，手动对焦的相机需要调整焦距。

**标定步骤**：
1. 安装`camera_calibration`包：
   ```bash
   sudo apt-get install ros-noetic-camera_calibration
   ```
2. 启动摄像头节点，然后启动标定节点`cameracalibrator.py`：
   ```bash
   rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/usb_cam/image_raw camera:=/usb_cam
   ```
   - `--size` 标定板内角点数量。
   - `--square` 标定板方块的实际大小（米）。
   - `image` 和 `camera` 分别指明图像话题和相机名称。

3. 在标定窗口中，移动标定板，使彩色线条出现在标定板上，直到所有进度条变绿。
4. 点击“CALIBRATE”按钮，等待终端输出标定结果。

**保存和使用标定结果**：
- 标定结果保存在`/tmp/calibrationdata.tar.gz`中，解压后包含标定参数文件（如`ost.yaml`）。
- 将标定结果文件复制到其他目录，如用户主目录。

**使用标定结果**：
1. 创建功能包和配置文件目录，将标定结果文件放入。
2. 在功能包的`launch`目录中创建`launch`文件，如`bingda_camera.launch`，并设置`camera_name`和`camera_info_url`参数。

**矫正图像**：
1. 安装`image_proc`包：
   ```bash
   sudo apt-get install ros-noetic-image-proc
   ```
2. 运行`image_proc`节点，对图像进行畸变矫正：
   ```bash
   ROS_NAMESPACE=/usb_cam rosrun image_proc image_proc
   ```
3. 使用`rqt_image_view`订阅原始图像和矫正后的图像话题，查看畸变矫正效果。

**注意事项**：
- 标定过程中要确保标定板在相机视野内，且移动要缓慢以避免成像拖影。
- 标定结果文件应定期备份，以防系统删除。
- 使用标定参数矫正图像时，确保话题名称与标定结果文件一致。

**资源链接**：
- [camera_calibration - ROS Wiki](http://wiki.ros.org/camera_calibration)

通过上述步骤，用户可以在ROS环境中对摄像头进行标定，获取准确的成像参数，并使用这些参数对图像进行畸变矫正，提高机器人视觉系统的精度和可靠性。



### ROS中调用MediaPipe

**项目简介**：
MediaPipe是一个由Google提供支持的开源机器学习工具库，它提供了一系列跨平台的机器学习解决方案，包括人脸检测、关键点识别、手势识别、图像分割和姿态估计等。MediaPipe本身包含多种预训练的模型，能够处理复杂的视觉任务。

**项目地址**：
- [MediaPipe - Google Developers](https://google.github.io/mediapipe/)

**mediapipe_ros功能包**：
`mediapipe_ros`是一个为ROS（Robot Operating System）定制的功能包，它提供了ROS接口来直接调用MediaPipe的功能。这个包使得在ROS中集成和使用MediaPipe的机器学习模型变得容易。

**安装和使用步骤**：
1. **安装pip3**：
   ```bash
   sudo apt install python3-pip
   ```
2. **安装MediaPipe**：
   ```bash
   pip3 install mediapipe
   ```
3. **克隆`mediapipe_ros`源码**：
   - 将源码克隆到ROS工作空间的`src`目录下。
4. **测试运行**：
   - 启动摄像头，并根据实际使用的摄像头设备号填写`device`值。
   - 打开一个rqt窗口，选择图像话题显示图像。

**功能测试**：
- **手部骨骼检测**：
  - 可以识别并标出手部21个关键点的3D坐标。
- **手势识别**：
  - 根据手部关键点判断当前手势，支持1~6的数字手势、竖大拇指和国际友好手势。
- **人像抠图**：
  - 将人像从背景中分离。
- **人脸检测**：
  - 检测人脸的关键点，如眼睛、鼻子、嘴巴和耳朵。
- **人脸网格**：
  - 提取人脸的3D网格特征。
- **人体骨架检测**：
  - 识别并标出人体33个关键点的3D坐标。
- **综合检测**：
  - 同时实现骨架检测、脸部关键点和手部动作检测。

**注意事项**：
- 使用冰达提供的嵌入式板卡镜像已经预装相关软件，无需执行安装步骤，可直接测试运行。
- 实验完成后，可以通过结束终端来关闭相关节点。

**资源链接**：
- [mediapipe_ros - Gitee](https://gitee.com/bingda-robot/mediapipe_ros)

通过上述步骤，用户可以在ROS环境中利用MediaPipe进行多种视觉任务，如手势识别、人脸检测等，极大地扩展了机器人的视觉处理能力。
