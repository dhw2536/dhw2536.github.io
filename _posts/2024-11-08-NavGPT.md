---
layout:     post
title:      Co-NavGPT论文
subtitle:   万事顺利
date:       2024-11-08
author:     Space
header-img: img/the-first.png
catalog:   true
tags:
    - 敬事如仪
---



# Co-NavGPT
## 基本信息

**英文题目：**Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models

**开源链接：**https://sites.google.com/view/co-navgpt

**中文题目：**Co-NavGPT：使用大型语言模型的多机器人协同视觉语义导航

**作者：**Bangguo Yu, Hamidreza Kasaei, Ming Cao

**单位：**University of Groningen, Faculty of Science and Engineering, 9747 AG Groningen, the Netherlands

**年份：**2023

**场景：**未知环境

**任务：**在未知环境中，利用机器人和大型语言模型（LLMs），完成对视觉目标的导航任务。

**算法：**Co-NavGPT框架，该框架将大型语言模型（LLMs）作为全局规划器，为多机器人协同视觉目标导航任务分配探索前沿。

**与其他算法的比较：**有比较，选择了Co-NavGPT算法，因为它在成功率和效率上超过了现有的模型，且无需任何学习过程。

**实现功能：**构建环境地图，利用语言模型指导各个机器人，实现在未知环境中高效的多机器人探索和搜索。

**创新点：**

1. 提出了Co-NavGPT框架，该框架可以构建环境地图并利用语言模型指导单个机器人，实现在未知环境中高效的多机器人探索和搜索。
2. 将环境信息编码为文本格式，使LLMs能够作为全局规划器，为多个机器人分配未探索的前沿。
3. 在HM3D（Habitat-Matterport 3D）数据集上的实验表明，所提出的多机器人协同框架显著提高了视觉目标导航性能。

Github：[ybgdgh/Co-NavGPT: We proposed to explore and search for the target in unknown environment based on Large Language Model for multi-robot system. (github.com)](https://github.com/ybgdgh/Co-NavGPT)

bilibili：[【论文速递】Co-NavGPT：机器人视觉目标导航+多机器人协作+GPT+未知环境_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Pp421y7Gm/?buvid=XU7480F7D220C0CF74CFC61CF2280813764F2&from_spmid=search.search-result.0.0&is_story_h5=false&mid=1Uu9CgLxfF1jntVaN2gKpg%3D%3D&p=1&plat_id=114&share_from=ugc&share_medium=android&share_plat=android&share_session_id=3c39446a-4dad-440b-a4d0-2f856d74eaca&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1725975816&unique_k=ScxWWoR&up_id=318916737&vd_source=e03b252a2c1fefc80e6f48a6f52e2a4d)

论文：[[2310.07937\] Co-NavGPT：使用大型语言模型的多机器人协作视觉语义导航 (arxiv.org)](https://arxiv.org/abs/2310.07937)





## 附图



![image-20240911161055500](F:\Desktop\北科-多车协同\1.毕业设计\毕设内容记录\4.Co-NavGPT.assets\image-20240911161055500.png)

图1：两个机器人视觉目标导航示例。当环境有多个未探索的边界时，语言模型将根据当前观察和目标物体为每个机器人分配边界。





![image-20240911161746130](F:\Desktop\北科-多车协同\1.毕业设计\毕设内容记录\4.Co-NavGPT.assets\image-20240911161746130.png)

图2：目标导航框架的整体架构。对于每个机器人，RGB-D图像被处理生成语义图，随后根据机器人的位置合并语义图。然后将机器人和地图功能集成到提示中，允许LLM充当全局规划者。这将为每个机器人分配边界，并用本地策略决定它们的最终动作。



![image-20240911163819955](F:\Desktop\北科-多车协同\1.毕业设计\毕设内容记录\4.Co-NavGPT.assets\image-20240911163819955.png)

图3：从局部地图(a)中提取场景物体(b)和墙壁(c)的过程。



![image-20240911163547761](F:\Desktop\北科-多车协同\1.毕业设计\毕设内容记录\4.Co-NavGPT.assets\image-20240911163547761.png)

图4：在Habitat平台中使用两个机器人寻找椅子的视觉目标导航实验过程。灰色通道表示障碍，蓝色点表示我们策略选择的长期目标，红色粗线表示机器人的轨迹，红色细线表示边界，其他颜色表示语义对象。



## 文章精读

### 摘要

在先进的人机交互任务中，视觉目标导航对于自主机器人在未知环境中导航至关重要。尽管过去已经开发了许多方法，但大多数是为单机器人操作设计的，由于环境复杂性，这些方法通常效率低下且鲁棒性差。此外，学习多机器人协作策略需要大量的资源。为了解决这些挑战，我们提出了Co-NavGPT，这是一个创新的框架，它将大型语言模型（LLMs）作为多机器人协同视觉目标导航的全局规划器。Co-NavGPT将探索的环境数据编码成提示，增强了LLMs的场景理解能力。然后为每个机器人分配探索前沿，以高效地搜索目标。在Habitat-Matterport 3D（HM3D）上的实验结果表明，Co-NavGPT在成功率和效率上都超过了现有模型，而且不需要任何学习过程，展示了LLMs在多机器人协作领域的巨大潜力。补充视频、提示和代码可以通过以下链接访问：https://sites.google.com/view/co-navgpt。

这项工作得到了中国国家留学基金委的部分支持。所有作者均隶属于荷兰格罗宁根大学科学与工程学院。

### 简介

1. **人类导航能力**：人类非常擅长创建关于世界结构的知识，这使得他们能够在复杂和动态的环境中高效导航。对于机器人来说，为了有效地探索和导航，它们需要能够推理周围的环境。
2. **视觉目标导航的重要性**：视觉目标导航是智能机器人的一项重要任务，它要求机器人在未知环境中探索以高效地定位目标对象。这种能力对于广泛的实际应用至关重要。
3. **现有研究的局限性**：现有的视觉目标导航研究通常分为两类：端到端的强化学习方法和模块化技术。端到端模型通常采用深度强化学习来开发有效的导航策略，但它们在样本效率和跨数据集泛化方面存在局限。模块化方法则使用显式空间地图作为场景记忆，并设计层次化策略进行决策，这通常可以提高样本效率和泛化能力。
4. **多机器人协作导航的挑战**：大多数现有模型主要在单机器人配置中表现出色。在广泛的环境里，单个机器人可能效率低下，因为它需要导航的未知区域很广。此外，单机器人模型的容错能力有限，一个错误或陷阱就可能使整个任务停止或大幅增加时间消耗。本文旨在通过强调多机器人协作导航来解决这些限制。
5. **大型语言模型（LLMs）的潜力**：LLMs 拥有关于世界的丰富知识，可以将语言作为一种通用工具，其中各种任务指令可以明确表示并指导机器人执行。例如，ChatGPT 的成功展示了 LLMs 在遵循人类指令方面的有效性，这表明 LLMs 能够理解基于文本的场景表示。
6. **本文的贡献**：本文提出了 Co-NavGPT，这是一个新颖的框架，它利用 LLMs 来制定多机器人协作的视觉目标导航的高效探索和搜索策略。在这个框架中，LLMs 充当全局规划器，为每个机器人分配未探索的前沿。
7. **实验结果**：在 Habitat 和 HM3D 数据集上的实验结果表明，Co-NavGPT 在成功率和效率上超过了现有的模型，而且不需要任何学习过程，展示了 LLMs 在多机器人协作领域的潜力。

### 相关工作

A. **视觉语义导航**：

- 这是智能机器人的基础任务之一，它利用人类的语义推理和目标定位能力。
- 传统方法通常构建度量或拓扑地图来捕获环境，并规划到达目的地的路径。
- 近期，基于学习的方法被广泛使用，例如使用预训练的 ResNet 来编码输入观察和目标图像，然后通过强化学习驱动的策略进行导航。
- 尽管这些方法在样本效率和跨场景泛化方面取得了进展，但单一策略从头开始学习所有技能也存在效率低下和泛化能力差的问题。

B. **多机器人协同导航**：

- 许多研究通过检查多机器人在不同领域的合作来解决单机器人系统的局限性，包括主动映射、探索和搜索。
- 传统的基于规划的方法主要关注于协调多个机器人的目标分配，但可能无法捕捉复杂的多智能体互动。
- 在探索任务中，多智能体强化学习（MARL）和基于图的学习被用来实现从单智能体到多智能体配置的规划器。
- 尽管基于规划和基于学习的技术在多机器人任务中取得了成功，但它们通常需要对机器人分配进行现实世界的常识学习。

C. **LLMs 在体现任务规划中的应用**：

- 最近，一些研究探索了 LLMs 的规划能力，使它们能够遵循自然语言指令并完成现实世界任务。
- 尽管 LLMs 在复杂推理或视觉输入解释方面面临挑战，但它们已经显示出帮助代理规划的才能，特别是在体现任务中。
- 一些研究展示了 LLMs 在视觉规划方面的能力，例如 LayoutGPT 利用语言模型生成场景布局。
- 其他研究通过使用文本提示与语言模型交互，处理高级语言命令，强调了 LLMs 在单个步骤图像中的作用。

![image-20240911185335765](F:\Desktop\北科-多车协同\1.毕业设计\毕设内容记录\4.Co-NavGPT.assets\image-20240911185335765.png)

**整个架构的目标是利用 LLMs 来提高多机器人系统在未知环境中导航到特定目标的效率和成功率。通过合并来自各个机器人的地图信息，并使用 LLMs 作为全局规划器，Co-NavGPT 框架能够协调机器人之间的行动，优化探索过程，并提高找到目标的速度和准确性。**

1. **GPS+Compass**：每个机器人都配备了 GPS 和指南针，这些设备可以帮助机器人确定其在环境中的位置和方向。

2. **RGBD Image**：机器人使用 RGBD 相机捕获环境的彩色图像和深度信息。这些图像用于理解周围环境并构建语义地图。

3. **Mapping**：每个机器人根据其捕获的 RGBD 图像和位置信息构建自己的局部语义地图。这些地图包含了环境中的物体、障碍物和其他特征。

4. **Semantic Map**：局部语义地图展示了机器人对环境的理解，包括物体的类别和位置。

5. **Merged Semantic Map**：所有机器人的局部语义地图被合并成一个全局地图，这个全局地图提供了对整个环境的更全面的视图。

6. **Environment**：指代整个环境，包括所有的机器人、物体、障碍物和未探索的区域。

7. **Last Decision**：指机器人根据当前决策做出的最后行动。

8. **Goal Object**：目标对象，即机器人需要导航到的特定物体。

9. **Frontier Extract**：从合并的语义地图中提取未探索的前沿区域，这些区域是机器人接下来可能探索的地方。

10. **Object Extract**：从地图中提取有关物体的信息，如物体的类别和位置。

11. **Robot State**：机器人的当前状态，包括位置、方向和可能的其他传感器数据。

12. **Unexplored Frontier**：未探索的前沿区域，这是机器人接下来可能需要探索的区域。

13. **Prompt**：将机器人的状态、环境信息和目标对象编码成文本提示，这些提示将被输入到大型语言模型（LLMs）中。

14. **Response**：LLMs 根据输入的提示生成响应，指示每个机器人应该探索的前沿区域。

15. **Language Models**：大型语言模型（LLMs）作为全局规划器，根据环境信息和目标对象为每个机器人分配探索任务。

16. **Local Policy**：每个机器人都有自己的局部策略，用于规划从当前位置到长期目标的路径，并执行具体的导航动作。

    

**Last Decision：**

在论文中，"Last Decision" 指的是在多机器人视觉目标导航任务中，每个机器人在最新时间步所做出的决策。这个决策是基于机器人当前的观察、它在环境中的位置、它对环境的理解（例如，通过语义地图），以及它的目标对象。

具体来说，"Last Decision" 包括以下几个方面：

1. **决策内容**：机器人根据当前的语义地图和全局规划器（即大型语言模型，LLMs）的指示，决定下一个要探索的前沿区域。这个决策是通过分析环境的结构布局、周围物体的上下文线索、机器人的当前状态和指定的目标来做出的。
2. **决策过程**：在每个全局步骤中，机器人使用更新后的语义地图构建一个包含有关机器人和地图的详细信息的提示（prompt）。然后，LLMs 被用来为每个机器人分配最合适的前沿作为长期目标。这个长期目标是 "Last Decision" 的一部分。
3. **决策实施**：一旦机器人根据 LLMs 的响应确定了长期目标，它将使用局部策略（例如，快速行进方法 Fast Marching Method）来规划从当前位置到这个长期目标的路径。在每一步中，机器人都会根据新的观察更新其局部地图和局部目标，并执行相应的动作以接近长期目标。
4. **决策的重要性**："Last Decision" 对于机器人的导航效率至关重要，因为它直接影响机器人的探索方向和目标搜索的效率。通过优化 "Last Decision"，可以提高任务的成功率和减少完成任务所需的时间。

在论文的实验部分，作者通过与其他多机器人地图基础方法的比较，展示了 Co-NavGPT 框架在成功率（Success Rate, SR）、路径长度加权成功率（Success weighted by Path Length, SPL）和到达目标的距离（Distance to Goal, DTG）等评价指标上的优势。这表明 Co-NavGPT 框架能够有效地处理多机器人协作视觉目标导航任务，即使在没有学习过程的情况下也能实现高成功率和高效率。



### 提出的方法

这部分内容详细说明了 Co-NavGPT 框架如何通过整合 LLMs 来实现多机器人在未知环境中的高效协作导航。

**A. 任务定义**

- **多机器人视觉目标导航任务**：在这个任务中，所有机器人需要合作，在未知场景中定位一个指定类别的对象。
- **场景和类别**：场景集合由 S={s1,…,sk}*S*={*s*1,…,*sk*} 表示，类别集合由 C={c1,…,cm}*C*={*c*1,…,*cm*} 表示。
- **初始化**：每个场景中的 n*n* 个机器人 R={r1,…,rn}*R*={*r*1,…,*rn*} 在同一随机位置 pi初始化，但具有不同的方向。
- **观察和动作**：每个时间步，每个机器人根据其视图接收观察 oi,t 并同时执行动作 ai,t。观察包括 RGB-D 图像、机器人的位置和方向以及对象类别。动作空间 *A* 包括六个离散动作：前进、左转、右转、向上看、向下看和停止。
- **成功条件**：如果机器人与目标之间的距离小于 0.1 米并且机器人执行了停止动作，则认为该场景成功。

**B. 概述**

- **框架结构**：框架利用大型语言模型（LLMs）进行多机器人设置中的目标选择。每个机器人捕获观察结果以构建其语义地图。所有机器人的轨迹合并后，得到一个合并的语义地图。
- **全局规划**：合并的数据（包括机器人位置、场景结构、场景对象和目标类别）被制定成提示（prompt），随后语言模型作为全局规划器，为每个机器人分配前沿目标。

**C. 地图表示**

1. **语义地图**：每个机器人使用 RGB-D 图像和位置信息构建语义地图，该地图表示为 K×M×M 矩阵，其中 M×M是地图维度，K=Cn+2表示语义地图中的通道数。
2. **地图构建**：使用几何方法将视觉输入转换为 3D 点云，然后根据机器人的位置投影到 2D 俯视图上。障碍物和已探索地图的通道基于深度图像，其他地图通道使用语义分割的输出进行投影。
3. **地图合并**：将来自机器人的 N*N*个语义地图标准化到一个共同的坐标系统中，然后使用每个像素的最大池化操作符进行集成，从而得到合并的全局地图。

**D. 基于 LLMs 的策略**

- **预处理**：为了解决环境复杂性，计算每个对象类别的熵，以帮助处理普遍对象（如门和窗户）的影响。
- **提示构建**：提示的前半部分概述了任务指令，包括机器人数量、感知数据、输入输出格式的示例和任务要求；后半部分是观察输入。
- **决策制定**：使用更新的语义地图构建提示，包括有关机器人和地图的详细信息。然后，利用语言模型为每个机器人分配最合适的前沿作为长期目标。

**E. 本地策略**

- **路径规划**：对于每个机器人，使用快速行进方法（FMM）从当前位置规划到长期目标的路径。选择当前位置有限范围内的局部目标，并执行最终动作以到达局部目标。
- **本地地图更新**：随着每一步的进行，根据新的观察更新本地地图和局部目标。

### 实验

这部分内容展示了 Co-NavGPT 框架在模拟环境中的性能，并与现有方法进行了比较，证明了其在多机器人协作视觉目标导航任务中的有效性和优越性。

**A. 模拟实验**

1. **数据集**：
   
   - 使用了 HM3D v0.2 数据集，该数据集包含高分辨率、逼真的 3D 重建真实世界环境。
   - 选择了标准的 36 个验证场景，包含 1,000 个带有语义注释的情节。
   - 定义了六个目标对象类别：椅子、沙发、植物、床、厕所和电视。
   
2. **实验细节**：
   - 在 Habitat 平台上进行评估，这是一个 3D 室内模拟器。
   - 对于每个机器人，观察空间包含 480 × 640 的 RGB-D 图像、基础里程计传感器和表示为整数的目标对象类别。
   - 使用微调的 RedNet 模型来预测所有类别。
   - 地图大小为 24 × 24 米，分辨率为 0.05 米。
   - 全局规划器使用了广泛采用的 GPT3.5-turbo，通过其公共 API 进行接口。
   - 在每个全局步骤中，生成一个更新的提示，将当前观察结果呈现给 LLMs，然后根据指定的输出格式解析响应。
   - 实现基于公开可用的代码，使用 PyTorch 框架。
   - 使用两个机器人评估框架，它们从相同位置初始化，但方向不同。如果没有在观察到的地图中检测到可用的前沿，则将随机点指定为每个机器人的长期目标。全局策略每 25 个局部步骤更新一次这个长期目标。

3. **评估指标**：
   - 使用成功率 (SR)、路径长度加权成功率 (SPL) 和到达目标的距离 (DTG) 来评估多机器人任务。
   - 定义了 SR 和 SPL 的计算方式，以及 DTG 的含义。

4. **基线比较**：
   - 考虑了几个基线方法，每个基线都使用相同的框架通过目标检测技术构建语义地图，但在用于机器人分配的全局策略模块上有所不同。
   - 包括贪婪方法、成本效用方法、地图上随机采样、Multi-SemExp 等。
   - 提出了两个用于消融研究的基线：Single-NavGPT 和 Co-NavGPT (GT-Seg)。

5. **结果和讨论**：
   - 定量结果报告在表格 I 中。成本效用方法优于贪婪方法，强调了距离和前沿大小在探索任务中的重要性。
   - 随机采样长期目标的方法比成本效用方法表现更好，突出了基于地图的方法的优势，允许机器人快速粗略地探索环境。
   - Co-NavGPT 框架在 SR 和 DTG 上一致优于所有基线。虽然随机采样方法的 SPL 略高于我们的方法，但由于其从远距离目标的优越连续探索，我们的前沿方法保持了更一致的成功率。

6. **消融研究**：
   
   - 为了评估框架内各个模块的相对重要性，使用 HM3D 数据集进行了消融研究。
   
   - 测试了 Single-NavGPT，它只使用单个机器人进行目标探索，观察到成功率和效率显著下降，这表明了多机器人框架的鲁棒性。
   
   - 将 GT Seg 纳入我们全面的模型可以提高所有测试的性能。这突出了语义分割在语义映射中的影响，也是失败的主要原因。
   

## 对比

在论文的实验部分，与其他多机器人地图基础方法进行了比较，以评估他们的框架性能。以下是对比研究的结果：

![image-20240913215610232](F:\Desktop\北科-多车协同\1.毕业设计\毕设内容记录\4.Co-NavGPT.assets\image-20240913215610232.png)

SR（Success Rate，成功率）、SPL（Success-weighted Path Length，成功**加权**路径长度）、DTG（Dynamic Time Warping，动态时间规整）。

1. **Greedy [37]**：这是一种贪婪策略，直接将前沿分配给所有机器人，每个机器人选择其最近的前沿作为目标位置。成功率（SR）为 0.611，成功加权路径长度（SPL）为 0.328，距离目标的距离（DTG）为 2.239。
2. **Cost-Utility [38]**：这种方法在获取前沿后，使用成本效用方法评估每个前沿单元，结合前沿区域的大小和前沿与机器人之间的距离。SR 为 0.625，SPL 为 0.323，DTG 为 2.030。
3. **Random Sample on Map**：这种方法通过在地图上随机采样长期目标，作为一种在未知和复杂环境中导航的策略。SR 为 0.636，SPL 为 0.336，DTG 为 2.048。
4. **Multi-SemExp [3]**：这是将单一机器人方法扩展到多机器人设置的基线，两个机器人协作探索环境。SR 为 0.612，SPL 为 0.327，DTG 为 2.234。
5. **Co-NavGPT (Ours)**：作者提出的 Co-NavGPT 方法，在 SR 和 DTG 上一致优于所有基线，SR 为 0.661，SPL 为 0.331，DTG 为 1.831。
6. **Single-NavGPT**：这是 Co-NavGPT 的一个变体，只使用单个机器人进行目标探索。在成功率和效率上显著下降，SR 为 0.539，SPL 为 0.215，DTG 为 2.633。
7. **Co-NavGPT (GT-Seg)**：这是 Co-NavGPT 方法的一个变体，其中语义分割算法被替换为地面真实数据（Ground-Truth Segmentation）。这种方法在所有测试中性能提升，SR 为 0.757，SPL 为 0.448，DTG 为 1.131。

从这些结果可以看出，Co-NavGPT 框架在多机器人协同导航任务中表现出色，尤其是在成功率和距离目标的距离上。这表明使用 LLMs 作为全局规划器，通过编码环境信息到提示中，可以有效地指导机器人进行探索和搜索目标。此外，通过使用地面真实数据进行语义分割，可以进一步提高性能，这突出了语义分割在语义映射中的重要性。





## 参考文献

1. Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, 和 A. Farhadi，“使用深度强化学习的室内场景目标驱动视觉导航”，在 IEEE 国际机器人与自动化会议论文集，第 3357–3364 页，IEEE，2017年5月。

2. A. Khandelwal, L. Weihs, R. Mottaghi, 和 A. Kembhavi，“简单但有效：CLIP 嵌入用于体现智能”，在 2022 IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，第 14809–14818 页，IEEE，2022年6月。

3. D. S. Chaplot, D. Gandhi, A. Gupta, 和 R. Salakhutdinov，“使用目标导向的语义探索的对象目标导航”，在神经信息处理系统进展，第 2020-Decem 卷，NeurIPS，第 1–12 页，2020年。

4. S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, 和 K. Grauman，“PONI: 用于无交互学习的物体目标导航的势能函数”，在 IEEE 计算机学会计算机视觉与模式识别会议，第 2022-June 卷，第 18868–18878 页，2022年。

5. W. Yang, X. Wang, A. Farhadi, A. Gupta, 和 R. Mottaghi，“使用场景先验的视觉语义导航”，在第7届国际学习表示会议，ICLR 2019，第 1–14 页，2019年。

6. Y. Lyu, Y. Shi, 和 X. Zhang，“通过关注3D空间关系改进目标驱动视觉导航”，在神经处理信函，第 54 卷，第 5 期，第 3979–3998 页，2022年。

7. R. Druon, Y. Yoshiyasu, A. Kanezaki, 和 A. Watt，“通过学习空间上下文进行视觉目标搜索”，在 IEEE 机器人与自动化信函，第 5 卷，第 2 期，第 1279–1286 页，2020年。

8. X. Liu, D. Guo, H. Liu, 和 F. Sun，“具有场景先验知识的多智能体体现视觉语义导航”，在 IEEE 机器人与自动化信函，第 7 卷，第 2 期，第 3154–3161 页，2022年。

9. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, 和 R. Lowe，“训练语言模型遵循人类反馈的指令”，2022年3月。

10. W. Huang, P. Abbeel, D. Pathak, 和 I. Mordatch，“作为零样本规划器的语言模型：为体现智能体提取可操作知识”，在机器学习研究进展，第 162 卷，第 9118–9147 页，2022年。

11. C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, 和 Y. Su，“LLM-Planner: 用于体现智能体的少样本地面规划的大型语言模型”，arXiv，2022年12月。

12. M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, 和 D. Batra，“Habitat: 体现 AI 研究平台”，在 2019 IEEE/CVF 国际计算机视觉会议 (ICCV)，第 2019Octob 卷，第 9338–9346 页，IEEE，2019年10月。

13. S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao, 和 D. Batra，“Habitat-Matterport 3D 数据集 (HM3D): 1000 个用于体现 AI 的大规模 3D 环境”，arXiv，2021年9月。

14. R. Ramrakhya, E. Undersander, D. Batra, 和 A. Das，“Habitat-Web: 从人类演示中学习体现对象搜索策略”，在 IEEE 计算机学会计算机视觉与模式识别会议，第 2022-June 卷，第 5163–5173 页，2022年4月。

15. J. Ye, D. Batra, A. Das, 和 E. Wijmans，“辅助任务和探索实现目标导航”，在 IEEE 国际计算机视觉会议论文集，第 16097–16106 页，2021年。

16. O. Maksymets, V. Cartillier, A. Gokaslan, E. Wijmans, W. Galuba, S. Lee, 和 D. Batra，“THDA: 为语义导航的寻宝狩猎数据增强”，在 IEEE 国际计算机视觉会议论文集，第 15354–15363 页，2021年。

17. D. S. Chaplot, R. Salakhutdinov, A. Gupta, 和 S. Gupta，“用于视觉导航的神经拓扑 SLAM”，在 IEEE 计算机学会计算机视觉与模式识别会议论文集，第 12872–12881 页，2020年。

18. D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, 和 R. Salakhutdinov，“使用主动神经 SLAM 学习探索”，在国际学习表示会议 (ICLR)，2020年4月。

19. Y. Liang, B. Chen, 和 S. Song，“SSCNav: 用于视觉语义导航的置信度感知语义场景补全”，在 IEEE 国际机器人与自动化会议论文集，第 2021-May 卷，第 13194–13200 页，IEEE，2021年5月。

20. A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, 和 D. Batra，“ZSON: 使用多模态目标嵌入的零样本目标导航”，在神经信息处理系统进展，第 35 卷，第 32340–32352 页，2022年6月。

21. Z. Al-Halah, S. K. Ramakrishnan, 和 K. Grauman，“零经验要求：即插即用模块化迁移学习用于语义视觉导航”，在 IEEE 计算机学会计算机视觉与模式识别会议，第 2022-June 卷，第 17010–17020 页，2022年。

22. B. Yu, H. Kasaei, 和 M. Cao，“L3MVN: 利用大型语言模型进行视觉目标导航”，arXiv，2023年。

23. K. Ye, S. Dong, Q. Fan, H. Wang, L. Yi, F. Xia, J. Wang, 和 B. Chen，“通过神经二分图匹配进行多机器人主动映射”，在 IEEE 计算机学会计算机视觉与模式识别会议，第 2022-June 卷，第 14819–14828 页，2022年。

24. C. Yu, X. Yang, J. Gao, H. Yang, Y. Wang, 和 Y. Wu，“学习高效的多智能体协同视觉探索”，在计算机科学讲义 (包括人工智能和生物信息学子系列)，第 13699 LNCS 卷，第 497–515 页，2022年。

25. D. Puig, M. A. Garcia, 和 L. Wu，“用于协调多机器人探索的新型全局优化策略：开发和比较评估”，机器人与自主系统，第 59 卷，第 9 期，第 635–653 页，2011年。

26. C. Yu, X. Yang, J. Gao, J. Chen, Y. Li, J. Liu, Y. Xiang, R. Huang, H. Yang, Y. Wu, 和 Y. Wang，“用于高效实时多机器人协同探索的异步多智能体强化学习”，2023年。

27. W. Feng, W. Zhu, T.-j. Fu, V. Jampani, A. Akula, X. He, SHe, S. Basu, X. E. Wang, 和 W. Y. Wang，“LayoutGPT: 与大型语言模型一起进行组合视觉规划和生成”，arXiv，第 1–25 页，2023年。

28. S. Vemprala, R. Bonatti, A. Bucker, 和 A. Kapoor，“用于机器人的 ChatGPT: 设计原则和模型能力”，2023年。

29. D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, 和 P. Florence，“PaLME: 一个体现多模态语言模型”，2023年3月。

30. H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, 和 C. Gan，“使用大型语言模型模块化构建合作体现智能体”，arXiv，第 1–22 页，2023年。

31. S. Hong, X. Zheng, J. Chen, Y. Cheng, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, 和 C. Wu，“MetaGPT: 用于多智能体协作框架的元编程”，arXiv，2023年。

32. W. Chen, S. Hu, R. Talak, 和 L. Carlone，“利用大型语言模型进行机器人 3D 场景理解”，arXiv，2022年9月。

33. J. A. Sethian，“一种用于单调前进前沿的快速行进水平集方法”，美国国家科学院院刊，第 93 卷，第 1591–1595 页，1996年2月。

34. F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, 和 S. Savarese，“Gibson Env: 体现智能体的现实世界感知”，在 2018 IEEE/CVF 计算机视觉与模式识别会议，第 9068–9079 页，IEEE，2018年6月。

35. J. Jiang, L. Zheng, F. Luo, 和 Z. Zhang，“RedNet: 用于室内 RGB-D 语义分割的残差编码器-解码器网络”，arXiv，2018年6月。

36. P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, 和 A. R. Zamir，“体现导航智能体的评估”，arXiv，2018年7月。

37. A. Visser 和 J. D. Hoog，“在通信受限环境中多机器人探索的讨论”，2013 IEEE 国际机器人与自动化会议，第 1–5 页，2013年。

38. M. Julià, A. Gil, 和 O. Reinoso，“自主探索和未知环境映射的路径规划策略比较”，自主机器人，第 33 卷，第 4 期，第 427–444 页，2012年。



## 待研究问题

**桥梁：**明确本篇论文中的工作有哪些点和毕设的内容有高度重合的部分？

了解仿真环境和数据集



## 代码内容

code

