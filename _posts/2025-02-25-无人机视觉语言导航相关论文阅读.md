---
layout:     post
title:      无人机视觉语言导航相关论文阅读
subtitle:   论文主要集中在无人机视觉语言导航领域，涵盖了数据集构建、模型设计、算法优化、实际应用等多个方面。
date:       2025-02-25
author:     Space
header-img: img/the-first.png
catalog:   true
tags:
    - 视觉语言导航

---





# 无人机视觉语言导航相关论文阅读

### 无人机视觉语言导航相关研究的仿真软件、数据集与模型概览

---

#### **一、仿真软件**  
1. **Unreal Engine 4 + AirSim**  
   - 被广泛用于构建高真实度无人机仿真环境，支持连续导航、动态天气（如光照变化、风吹树叶）和复杂城市场景渲染。  
   - **应用案例**：AerialVLN、AeroVerse、STMR等任务均基于该平台开发模拟器。  

2. **基于浏览器的3D模拟器**  
   - **CityNav**：利用Potree（WebGL点云渲染器）构建在线飞行模拟器，支持真实城市3D扫描环境的交互式导航。  
   - **AVDN**：基于xView卫星数据集构建自上而下的连续俯视场景，支持异步人-机对话导航。  

3. **其他仿真工具**  
   - **Matterport3D Simulator**：用于地面视觉语言导航（如R2R数据集），但部分无人机研究借鉴其真实场景构建思路。  
   - **AirSim + Gazebo/ROS**：用于真实无人机硬件在环测试，支持多传感器数据同步与路径规划验证。  

---

#### **二、数据集**  
1. **核心视觉语言导航数据集**  
   - **AerialVLN**：8,446条飞行路径，25个城市场景，支持三维避障与长路径（平均661.8米）导航，含RGB、深度图和动态环境数据。  
   - **CityNav**：32,637条轨迹，覆盖真实城市3D扫描，结合地理信息（如地标语义地图），任务难度更高。  
   - **AVDN**：3,064条轨迹，含多轮异步对话指令，用于研究语言交互对导航的影响。  
   - **AeroVerse**：包含真实图像（AerialAgent-Ego10k）与虚拟图像（CyberAgent-Ego500k）数据集，支持预训练与多任务微调。  

2. **辅助数据集**  
   - **UAV-VisLoc**：6,742张无人机图像与卫星地图匹配数据，用于视觉定位任务。  
   - **Game4Loc**：基于游戏生成的GTA-UAV数据集，覆盖多高度、多视角的无人机地理定位场景。  
   - **UEVAVD**：主动目标检测数据集，模拟遮挡环境下的多视角观测。  

---

#### **三、模型与算法**  
1. **导航模型架构**  
   - **跨模态注意力模型（CMA）**：结合视觉（RGB+深度）与语言特征，通过双向LSTM实现细粒度对齐，AerialVLN中改进为CMA-LAG。  
   - **基于LLM的框架**：  
     - **STMR**：语义-拓扑-度量表征引导的LLM推理，实现零样本端到端规划，显著提升户外导航成功率。  
     - **NavAgent**：融合多尺度街景信息（全局拓扑图+局部地标），结合大型视觉语言模型（如LLaVA）生成决策。  
   - **强化学习模型**：如AeroAgent的“大脑-小脑”架构，分离高层决策（LLM）与底层控制（PID控制器）。  

2. **关键技术**  
   - **前瞻指导策略（LAG）**：生成未来路径片段，缓解指令与动作的不一致性。  
   - **人类注意力预测**：通过热图引导模型关注关键区域，提升长轨迹导航成功率（如HAA-Transformer）。  
   - **动态子目标分解**：将复杂指令拆解为可执行的子任务链（如STMR中的LLM推理）。  

3. **评估方法创新**  
   - **基于GPT-4的自动化评估**：AeroVerse提出LLM-Judge系列指标，针对场景感知、推理与规划任务生成定制化评分。  
   - **多模态对齐指标**：如SDTW（动态时间规整加权成功率），衡量路径相似性与任务完成度的综合性能。  

---

#### **四、工具链与扩展应用**  
1. **开源平台**  
   - **OpenUAV**：北京航空航天大学发布的无人机VLN开放平台，集成仿真、数据集与算法库，支持6DoF运动建模。  
   - **LEVIOSA**：基于自然语言的轨迹生成框架，结合MiniSpec编程语言与流式解释执行，降低LLM响应延迟。  

2. **多模态模型支持**  
   - **CLIP、SAM、Grounding DINO**：用于视觉-语言特征提取与目标检测（如CloudTrack的开放词汇检测）。  
   - **LLaVA、GPT-4V**：处理复杂指令生成与场景描述，增强无人机环境理解能力。  

---

#### **五、挑战与趋势**  
1. **现存问题**  
   - **仿真-现实差距**：动态环境（如移动车辆）与传感器噪声的模拟仍不完善。  
   - **计算效率**：LLM推理延迟高，难以满足实时导航需求（如TypeFly通过流式执行优化）。

2. **未来方向**  
   - **轻量化模型**：压缩LLM规模（如LLaMA-3），适配边缘计算设备。  
   - **多智能体协作**：探索异构无人机群的联合视觉语言导航（如NEUSIS框架）。  







## 视觉语言导航文献

### Aerial Vision-and-Dialog Navigation  

**中文题目**：  无人机视觉与对话导航  

**论文作者**：  Yue Fan, Winson Chen, Tongzhou Jiang, Chun Zhou, Yi Zhang, Xin Eric Wang  

**科研单位**：  美国加州大学圣克鲁兹分校（University of California, Santa Cruz）  

**是否与无人机视觉语言导航有关？为什么？**：  

是。该研究提出了一种结合自然语言对话和视觉感知的无人机导航方法，旨在通过多轮对话交互解决复杂环境中的导航任务，并构建了首个面向无人机视觉与对话导航的数据集（AVDN）和模拟器。  

**开源链接**：  https://sites.google.com/view/aerial-vision-and-dialog/home  

**研究问题（论文想解决什么？）**：  如何在连续、真实的空中环境中，通过自然语言对话实现无人机的精准导航，包括多轮对话指令理解、视觉场景推理，以及动态调整飞行路径的挑战。  

**文章创新点**：  

1. **首创数据集与模拟器**：提出首个面向无人机视觉与对话导航的AVDN数据集（含3K+轨迹与异步人-人对话），并构建了基于卫星图像的连续环境模拟器。  
2. **新任务定义**：引入ANDH（基于对话历史的子轨迹导航）和ANDH-Full（基于完整对话历史的全局导航）任务，评估模型对复杂语言指令的理解能力。  
3. **HAA-Transformer模型**：联合预测导航路径与人类注意力，通过多任务学习提升模型性能，实验表明注意力预测显著提升长轨迹导航成功率。  

***

**数据集**：  

- **AVDN数据集**：  
  - 包含3,064条无人机导航轨迹，每条轨迹包含多轮自然语言对话。  
  - 数据包括：视觉观察（卫星图像裁剪的俯视图）、人类注意力热图、导航路径（中心点坐标、方向、高度）。  
  - 特点：异步人-人对话收集（指挥官与跟随者分离）、连续空间环境、覆盖城市与乡村场景。  
  - 分割方式：训练集、可见验证集、未见验证集、未见测试集，确保场景区域不重叠。  

***

**实验如何开展**：  

1. **基线模型对比**：包括多模态Episodic Transformer（E.T.）、仅视觉/仅语言模型、LSTM模型，以及随机动作模型。  
2. **消融实验**：验证人类注意力预测模块的有效性（如HAA-Transformer vs. E.T.）。  
3. **任务验证**：在ANDH和ANDH-Full任务中评估模型，重点关注长轨迹和复杂对话场景下的性能。  
4. **指标分析**：使用成功率（SR）、路径加权成功率（SPL）、目标进展（GP）等指标，结合注意力预测的NSS分数。  

***

**评价指标**：  

1. **成功率（SR, Success Rate）**：导航最终视角与目标区域的IoU>0.4的比例。  
2. **路径加权成功率（SPL）**：考虑路径长度的加权成功率，平衡效率与准确性。  
3. **目标进展（GP, Goal Progress）**：衡量导航过程中向目标靠近的欧氏距离进展。  
4. **注意力预测指标**：归一化扫描路径分数（NSS），量化预测注意力与真实注意力的匹配度。  

***

**核心模型**：  

**HAA-Transformer**：  

- **输入**：多模态信息（语言对话、视觉观察、飞行方向）。  
- **结构**：  
  - **多模态编码**：BERT处理语言，DarkNet-53提取视觉特征，全连接网络编码方向。  
  - **导航解码**：预测航点（x, y, h）和导航进度（何时停止）。  
  - **注意力解码**：生成人类注意力热图，辅助模型聚焦关键区域。  
- **训练策略**：多任务学习（导航损失 + 注意力损失），交替使用教师强制与学生强制训练。  

***

**关键技术**：  

1. **多模态对齐**：语言指令与视觉场景的联合编码（Transformer融合）。  
2. **人类注意力预测**：通过NSS损失优化，提升模型对关键区域的感知能力。  
3. **航点控制**：将连续导航空间离散化为航点预测，降低搜索复杂度。  
4. **异步对话建模**：支持指挥官与跟随者非实时交互，贴近实际应用场景。  

---

**补充说明**  

1. **数据收集的异步性**：指挥官与跟随者分时工作，降低数据采集成本，同时模拟真实场景中用户无需实时监控无人机的特性。  
2. **模拟器的技术细节**：基于xView卫星数据集生成连续俯视图，支持高度、方向、位置的自由控制，并通过边界约束防止越界。  
3. **注意力预测的实际价值**：不仅提升导航性能，还增强模型可解释性，帮助分析无人机决策依据（如聚焦建筑物或道路）。  
4. **长轨迹挑战**：实验表明，HAA-Transformer在长轨迹（>125米）上的成功率比基线模型高约30%，验证了注意力机制的有效性。





### Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning

**中文题目：**  基于语义-拓扑-度量表征引导LLM推理的无人机视觉语言导航

**论文作者：**  Yunpeng Gao（高云鹏）, Zhigang Wang（王志刚）, Linglin Jing（荆玲琳）, Dong Wang（王栋）, Xuelong Li（李学龙）, Bin Zhao（赵斌）  （*Equal Contribution, †Corresponding Author）

**科研单位：**  

1. **西北工业大学**（Northwestern Polytechnical University）  
2. **上海人工智能实验室**（Shanghai AI Laboratory）  
3. **中国电信人工智能研究院**（Institute of Artificial Intelligence, China Telecom Corp Ltd）  

***

**是否与无人机视觉语言导航有关？为什么？**  

**相关**。  论文聚焦于无人机（UAV）在户外复杂环境中的自主导航任务，通过结合自然语言指令（如“起飞后直行穿越水域至道路”）与视觉感知（RGB图像、深度图），提出了一种基于大语言模型（LLM）的零样本端到端框架。其核心挑战在于解决户外场景中语义丰富、空间关系复杂、规模庞大等问题，而传统方法难以适应空中视角的自由运动需求。通过引入语义-拓扑-度量表征（STMR），论文显著提升了LLM在三维空间中的推理能力，与无人机视觉语言导航的核心目标高度契合。

---

**开源链接：**  论文未明确提供代码或数据集链接。

---

**研究问题（论文想解决什么？）**  

1. **复杂空间关系建模**：户外空中场景的语义多样性、拓扑复杂性及大规模特性导致传统视觉语言导航（VLN）方法难以有效推理。  
2. **指令与视觉对齐**：自然语言指令中模糊的空间描述（如“左侧10米”）易引发LLM的“幻觉”问题（hallucination）。  
3. **端到端零样本适配**：现有方法依赖预定义环境图或低层动作规划器，无法直接应用于无人机自由运动场景。

---

**文章创新点：**  

1. **语义-拓扑-度量表征（STMR）**：  
   - 通过视觉感知模型（Grounding DINO、Tokenize Anything）提取指令相关地标语义掩码，结合深度与位姿信息投影至俯视地图。  
   - 将地图转换为20×20网格矩阵，每个网格编码语义类别（如建筑、道路）及距离信息（相邻网格间距5米），作为LLM的文本化空间提示。  
2. **零样本端到端框架**：  
   - 无需训练或额外动作规划器，直接利用LLM（如GPT-4o）单步推理生成动作（如“左转5度，前进10米”）。  
   - 支持动态子目标分解与状态更新（TODO/In Process/Completed），提升长指令执行鲁棒性。  
3. **真实环境验证**：  
   - 在真实无人机（Q250机架+RealSense D435i）上验证，成功率达40%（对比基线MapGPT的20%）。

---

**数据集：**  

**AerialVLN-S**：  

- 包含8,446条飞行路径，由持证飞行员在仿真与真实环境（城市、工厂、公园、村庄）中录制。  
- 覆盖870+类物体，验证集分为12个“已见”场景与5个“未见”场景，测试集未公开。  
- 提供RGB图像、深度图、位姿及自然语言指令（如“直行至道路后左转”）。

---

**实验如何开展：**  

1. **仿真环境**：  
   - 平台：AirSim + UE4，硬件为i9-12代CPU + RTX 4090 GPU。  
   - 对比方法：随机动作、LingUNet、Seq2Seq、CMA、LAG等。  
2. **真实环境**：  
   - 无人机：Q250机架 + RealSense D435i深度相机 + Jetson Xavier NX。  
   - LLM：云端GPT-4o API，输入STMR矩阵与历史动作生成下一步动作。  
3. **评估指标**：  
   - **导航误差（NE）**：停止点与目标点的欧氏距离（米）。  
   - **成功率（SR）**：停止点距目标≤20米的比例。  
   - **Oracle成功率（OSR）**：轨迹中任意点距目标≤20米的比例。

---

**评价指标：**  

| 指标          | 定义                        | 意义                         |
| ------------- | --------------------------- | ---------------------------- |
| **NE (m) ↓**  | 导航终点与目标点的欧氏距离  | 衡量导航精度，越低越好       |
| **SR (%) ↑**  | 终点误差≤20米的成功率       | 任务完成能力                 |
| **OSR (%) ↑** | 轨迹中任意点误差≤20米的比例 | 反映路径规划合理性（容错性） |

---

**核心模型：**  

1. **STMR生成模块**：  
   - **视觉感知**：Grounding DINO提取地标框，Tokenize Anything生成语义掩码与描述。  
   - **地图投影**：利用深度与位姿将掩码投影至俯视地图，记录轨迹与周围地标位置。  
   - **矩阵压缩**：将局部地图划分为20×20网格，按语义频率编码为数字矩阵（如0:未探索，1:道路）。  
2. **LLM规划器**：  
   - 输入：STMR矩阵、子目标指令、历史动作。  
   - 输出：思维链（Chain-of-Thought）推理后的动作（格式：方向+角度+距离）。  

---

**关键技术：**  

1. **子目标动态分解**：  
   - 利用LLM将长指令拆解为子目标（如“起飞→穿越水域→转向道路”），并跟踪执行状态。  
2. **多模态提示工程**：  
   - 设计结构化提示模板（含任务描述、输入格式、输出格式），强制LLM按“观察→思考→规划→动作”流程推理。  
3. **感知-规划解耦**：  
   - 视觉感知（掩码提取）与空间推理（LLM）分离，避免端到端训练中的耦合误差。  

---

**补充亮点：**  

- **抗幻觉设计**：STMR通过矩阵显式约束空间关系，减少LLM对未观测地标的虚构引用。  
- **跨场景泛化**：在“未见”场景（Validation Unseen）中，OSR达23.0%，显著优于LAG的10.5%。  
- **实时性**：框架依赖云端LLM，未来可优化为轻量化本地模型（如LLaMA-3）以降低延迟。



### AerialVLN: Vision-and-Language Navigation for UAVs  

**中文题目**：AerialVLN：无人机视觉与语言导航  

**论文作者**：Shubo Liu, Hongsheng Zhang, Yuankai Qi, Peng Wang, Yanning Zhang, Qi Wu  

**科研单位**：西北工业大学（Northwestern Polytechnical University）、阿德莱德大学（University of Adelaide）  

**是否与无人机视觉语言导航有关？为什么**：是。论文提出首个针对无人机的视觉语言导航任务（AerialVLN），解决了三维空间中的复杂导航问题，包括飞行高度控制、多自由度动作（如升降、平移）、动态环境（天气、光照变化）以及长路径推理等挑战，与传统地面导航任务显著不同。  

**开源链接**：https://github.com/AirVLN/AirVLN  

**研究问题**：解决无人机在开放城市场景中通过自然语言指令进行连续导航的难题，强调三维空间中的动作规划（如避障、高度调整）、复杂语言-视觉对齐（多目标参照、长指令理解）以及动态环境适应性。  

***

**文章创新点**：  

1. **任务创新**：提出首个面向无人机的三维视觉语言导航任务（AerialVLN），填补空中导航研究的空白。  
2. **模拟器与数据集**：基于Unreal Engine 4和AirSim构建高真实度动态环境模拟器，包含25个城市场景、8,446条飞行路径及25,338条自然语言指令，支持连续动作与动态环境（天气、光照变化）。  
3. **前瞻指导策略（LAG）**：改进基线模型，通过“前瞻路径”生成动作标签，缓解传统最短路径与指令描述不一致的问题。  
4. **多模态分析**：验证RGB与深度信息的互补性，表明二者对导航任务均不可或缺。 

***

**数据集**：  

- **AerialVLN**：25个城市场景，8,446条飞行路径，每条路径对应3条指令（共25,338条），平均指令长度83词，词汇量4,470词，路径平均长度661.8米。  
- **AerialVLN-S**：简化版数据集，路径平均长度321.3米，用于短路径场景研究。  

***

**实验如何开展**：  

1. **基线模型对比**：包括随机动作、动作采样、LingUNet、Seq2Seq、CMA及改进的CMA-LAG模型。  
2. **动态环境测试**：模拟天气、光照变化，验证模型鲁棒性。  
3. **长路径与短路径分组实验**：分析路径长度对成功率的影响（长路径成功率仅1.8%，短路径7.4%）。  
4. **模态消融实验**：移除RGB、深度或语言输入，验证多模态必要性。  

***

**评价指标**：  

- **SR（Success Rate）**：终点与目标距离≤20米视为成功。  
- **OSR（Oracle Success Rate）**：轨迹任意点与目标距离≤20米视为“潜在成功”。  
- **NE（Navigation Error）**：终点到目标的欧氏距离。  
- **SDTW（Success weighted by Dynamic Time Warping）**：结合路径相似性与成功率。  

***

**核心模型**：  

- **改进的CMA（Cross-Modal Attention）模型**：结合双向LSTM与跨模态注意力机制，融合视觉（RGB+深度）与语言特征。  
- **LAG（Look-ahead Guidance）**：动态生成“前瞻路径”指导动作预测，提升指令对齐能力。  

***

**关键技术**：  

1. **跨模态对齐**：语言指令与视觉感知的细粒度匹配（如通过注意力机制关联文本中的地标与图像中的物体）。  
2. **连续动作空间建模**：支持8种低自由度动作（前进、转向、升降、平移等），模拟真实无人机控制。  
3. **动态环境渲染**：支持天气、光照、移动物体等动态变化，缩小仿真与现实的差距。  
4. **长路径稀疏奖励学习**：针对平均660米的长路径，设计动作序列优化策略，解决稀疏奖励下的探索难题。  

***

**特别补充**：  

- **语言复杂性**：AerialVLN指令包含更多空间关系描述（平均9.7个参照物，是R2R的2.6倍），涉及复杂时序（如“先飞过桥，再左转”）。  
- **避障与3D推理**：无人机需估计障碍物三维形状及距离，比地面导航更复杂。  
- **人类性能对比**：人类成功率约80%，而最佳模型仅4.5%，凸显任务挑战性。



### Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied LMM-based Agent on Drones  

**中文题目：**  “代理为大脑，控制器为小脑”：基于大型多模态模型的无人机具身智能体实现  

**论文作者：**  Haoran Zhao, Fengxing Pan, Huqiuyue Ping, Yaoming Zhou  

**科研单位：**  

1. 北京航空航天大学（Beihang University）  
2. 浙江大学（Zhejiang University）  
3. 青鸟AI（qingniaoAI）  

**是否与无人机视觉语言导航有关？为什么？**  

**相关**。论文提出了一种基于大型多模态模型（LMM）的无人机代理系统AeroAgent，其核心能力包括多模态感知（视觉、位置、音频等）、任务规划与动态决策。虽然未明确使用“视觉语言导航”术语，但其通过视觉输入（如摄像头图像）和自然语言任务描述（如“检测危险”）生成导航路径和动作指令，本质上属于视觉语言导航的扩展应用。此外，系统通过ROSchain框架实现感知-决策-执行的闭环，支持复杂环境下的自主导航任务（如搜索救援、安全避障）。  

**开源链接：**  论文未提及开源代码或数据集。

***

**研究问题（论文想解决什么？）**  

1. **端到端机器人学习的局限性**：依赖大规模标注数据、泛化能力不足、仿真与现实的差异。  
2. **现有大型模型在无人机任务中的不足**：响应速度慢、冗余性低、具身智能（Embodied Intelligence）效率低。  
3. **复杂工业场景下的自主决策需求**：如火灾救援、基础设施巡检等需动态规划与多任务协调的场景。  

***

**文章创新点：**  

1. **“代理为大脑，控制器为小脑”架构**：将高层决策（LMM代理）与底层控制（传统控制器）解耦，兼顾智能与实时性。  
2. **LMM-based Agent设计**：  
   - 多模态记忆数据库（融合视觉、操作序列记忆）支持Few-shot学习。  
   - 自动规划生成器（Automatic Plan Generator）动态生成任务策略。  
3. **ROSchain框架**：基于ROS的通信链路，实现LMM与无人机传感器、执行器的无缝集成。  
4. **工业无人机任务验证**：在仿真（Airgen）与真实场景（个体搜救）中验证多任务性能，优于传统DRL方法。  

***

**数据集：**  

- **仿真环境**：Airgen平台生成的高保真无人机场景（包括火灾救援、视觉着陆、基础设施检测、安全导航任务）。  
- **真实场景**：个体搜救案例（建筑工地开放区域模拟危险环境）。  

***

**实验如何开展：**  

1. **仿真实验**（Airgen平台）：  

   - **任务设计**：火灾搜救、视觉着陆、风电场故障检测、废弃工厂安全导航。  
   - **对比基线**：DRL-based Agent、单次调用LMM（无记忆与规划）、AeroAgent（带/不带ROSchain）。  
   - **评估方式**：任务完成效率、路径规划精度、异常检测准确率。  

2. **真实案例**：  

   - 建筑工地个体搜救任务，验证自主决策与动态规划能力。  

   ***

**评价指标：**  

1. **任务完成度**（如救援人数、故障检测准确率）。  
2. **动作效率**：步骤数、响应延迟（如ROSchain对比迭代提示机制的耗时）。  
3. **鲁棒性**：突发场景（如火灾烟雾干扰）下的决策稳定性。  
4. **仿真-现实一致性**：通过Airgen与真实案例验证系统迁移能力。  

***

**核心模型：**  

**AeroAgent**：  

- **组件**：  
  1. **自动规划生成器**（LMM驱动，输入多模态感知与记忆）。  
  2. **多模态记忆数据库**（标签化存储视觉、操作序列）。  
  3. **具身动作库**（预定义无人机动作函数，如`moveToPosition()`）。  
- **关键技术依赖**：GPT-4V等大型多模态模型。  

***

**关键技术：**  

1. **多模态感知融合**：视觉（RGB/红外）、位置、音频数据的联合处理。  
2. **动态规划与Few-shot学习**：通过记忆标签实现环境变化快速适应（如对比历史图像检测风机故障）。  
3. **分层动作执行**：高层规划生成目标坐标，底层控制器解析为具体控制指令（如PID调节）。  
4. **ROSchain通信框架**：ROS节点间消息封装与API调用，确保实时性与系统稳定性。  

---

**补充亮点（针对无人机视觉语言导航方向）：**  

- **视觉-语言任务对齐**：通过自然语言描述任务目标（如“检测危险”），代理生成视觉关注的导航路径（如烟雾区域）。  
- **主动感知增强**：代理可主动调用传感器（如红外摄像头）补充环境信息，提升导航决策可靠性。  
- **潜在扩展方向**：结合开放词汇目标检测（如CLIP）实现更细粒度的视觉-语言 grounding，进一步提升复杂场景下的导航泛化能力。



### AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models

**中文题目：**AeroVerse：用于模拟、预训练、微调和评估航空航天具身世界模型的无人机智能体基准套件  

**论文作者：**Fanglong Yao, Yuanchang Yue, Youzhi Liu, Xian Sun, Kun Fu  

**科研单位：**中国科学院空天信息创新研究院  

**是否与无人机视觉语言导航有关？为什么？**

是。本文提出了一个针对无人机智能体的航空航天具身世界模型基准套件AeroVerse，旨在提升无人机在复杂环境中的自主感知、认知和行动能力，与无人机视觉语言导航密切相关。  

**开源链接：**暂未提供具体开源链接，但文中提到将发布AeroVerse基准套件以促进航空航天具身智能的发展。  

**研究问题（论文想解决什么？）**现有无人机具身智能研究主要集中在室内场景或地面智能体，针对无人机的具身智能研究较少，尤其是缺乏大规模无人机具身数据集。本文旨在填补这一空白，通过构建模拟平台、预训练数据集、下游任务指令数据集和评估指标，推动无人机具身智能的发展。  

***

**文章创新点：**  

1. 构建了首个大规模真实世界图像-文本预训练数据集AerialAgent-Ego10k和虚拟图像-文本-姿态对齐数据集CyberAgent-Ego500k，用于无人机具身世界模型的预训练。  
2. 首次明确定义了五个无人机具身下游任务，包括场景感知、空间推理、导航探索、任务规划和运动决策，并构建了相应的指令数据集。  
3. 开发了基于GPT-4的下游任务评估方法SkyAgent-Eval，提供了全面、灵活和客观的评估指标。  
4. 集成了多个2D/3D视觉-语言模型、预训练数据集、下游任务指令数据集和评估指标，形成了AeroVerse基准套件。  

***

**数据集：**  

1. **AerialAgent-Ego10k**：包含10,000张真实城市的第一人称视角图像及其细粒度多属性文本描述。  
2. **CyberAgent-Ego500k**：包含500,000张虚拟城市场景的第一人称视角图像、文本描述和无人机姿态数据。  
3. **SkyAgent-Scene3k**：包含3,000个无人机场景描述指令数据。  
4. **SkyAgent-Reason3k**：包含3,000个无人机空间推理指令数据。  
5. **SkyAgent-Nav3k**：包含3,000个无人机导航探索指令数据。  
6. **SkyAgent-Plan3k**：包含3,000个无人机任务规划指令数据。  
7. **SkyAgent-Act3k**：包含3,000个无人机运动决策指令数据。 

***

**实验如何开展：**  

1. 选择多个主流的2D和3D视觉-语言模型作为基线，包括LLaVA、MiniGPT4、BLIP2等。  
2. 对这些模型进行修改，以适应无人机具身任务的输入和输出格式。  
3. 在五个下游任务数据集上进行评估，使用传统指标（如BLEU、SPICE）和基于GPT-4的指标（如LLM-Judge-Scene、LLM-Judge-Reason&Nav、LLM-Judge-Plan）进行量化分析。  
4. 通过定性和定量分析，评估模型在不同任务和场景下的表现，揭示其潜力和局限性。  

***

**评价指标：**  

1. **传统指标**：BLEU-1、BLEU-2、BLEU-3、BLEU-4、CIDEr、SPICE等，用于评估文本生成任务的质量。  
2. **基于GPT-4的指标**：LLM-Judge-Scene、LLM-Judge-Reason&Nav、LLM-Judge-Plan，用于评估无人机具身任务的定制化需求，提供更全面和客观的评估结果。  

***

**核心模型：**  

1. **AeroSimulator**：一个包含四个真实城市场景的无人机飞行模拟平台，用于数据收集和模型训练。  
2. **AerialAgent-Ego10k和CyberAgent-Ego500k**：用于无人机具身世界模型预训练的图像-文本数据集。  
3. **SkyAgent-Scene3k、SkyAgent-Reason3k、SkyAgent-Nav3k、SkyAgent-Plan3k和SkyAgent-Act3k**：用于无人机具身下游任务的指令数据集。  
4. **SkyAgent-Eval**：基于GPT-4的评估方法，用于量化评估无人机具身任务的性能。  

***

**关键技术：**  

1. **模拟平台开发**：利用Unreal Engine 4和AirSim构建高保真无人机飞行模拟平台，支持多种城市场景和环境条件。  
2. **数据集构建**：通过专业标注人员在模拟环境中收集和标注大规模图像-文本数据，确保数据质量和多样性。  
3. **模型修改与适配**：对现有2D/3D视觉-语言模型进行输入和输出格式的修改，使其适用于无人机具身任务。  
4. **定制化评估方法**：设计基于GPT-4的评估方法，针对不同无人机具身任务提供灵活和客观的评估指标。  





### UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation

**中文题目**：UAV-VLA：用于大规模空中任务生成的视觉-语言-行动系统  

**论文作者**：Oleg Sautenkov, Yasheerah Yaqoot, Artem Lykov, Muhammad Ahsan Mustafa, Grik Tadevosyan, Aibek Akhmetkazy, Miguel Altamirano Cabrera, Mikhail Martynov, Sausar Karaf, Dzmitry Tsetserukou  

**科研单位**：Skolkovo Institute of Science and Technology  

**是否与无人机视觉语言导航有关？为什么**：是。该研究通过整合卫星图像处理、视觉语言模型（VLM）和GPT的强大功能，实现了基于自然语言指令的无人机任务生成和导航。  

**开源链接**：[GitHub链接](https://github.com/sautenich/uav-vla)  

**研究问题**：如何通过自然语言指令高效生成无人机的飞行路径和行动计划，以提高任务规划的效率和可访问性。  

**文章创新点**：  

1. 提出了一个大规模的视觉-语言-行动（VLA）系统，能够从单个文本任务请求中生成完整的路径-行动集，将文本输入与卫星图像相结合。  
2. 引入了名为UAV-VLPAnano-30的纳米基准，用于快速测量全球规模的视觉语言行动系统的任务解决方案。  

通过在UAV-VLPAnano-30上的实验验证了系统性能，展示了与人类水平相当的路径和行动生成能力。  

**数据集**：UAV-VLPA-nano-30基准数据集，包含30张高分辨率卫星图像，涵盖美国不同地点的多样环境，包括城市、郊区、农村和自然环境。  

**实验如何开展**：  

1. 使用命令“为四旋翼飞行器创建一个飞行计划，使其在100米高度绕过每个建筑物，返回起飞点并降落”。  
2. 在配备RTX 4090显卡和Intel Core i9-13900K处理器的PC上进行实验。  
3. 使用量化后的Molmo-7B-D BnB 4-bit模型进行对象搜索。  
4. 将UAV-VLA系统生成的飞行计划与人类生成的计划进行比较。  

**评价指标**：  

- 轨迹长度  
- 误差（使用RMSE计算，包括顺序方法、动态时间规整（DTW）和K-最近邻（KNN）方法）  

**核心模型**：UAV-VLA系统，包括目标提取GPT模块、对象搜索VLM模块和动作生成GPT模块。  

**关键技术**：  

- 目标提取GPT模块解析语言指令。  
- 对象搜索VLM模块在卫星图像中识别目标。  
- 动作生成GPT模块生成UAV动作。  

需要补充说明的内容：  该研究通过整合先进的语言模型和视觉模型，显著提高了无人机任务规划的效率和准确性。未来的工作将集中在创建专门的数据集以提高模型在各种UAV应用中的任务生成精度和效率，并开发一个端到端模型，将行动生成、路径规划和决策制定整合到一个统一的框架中。





### CITYNAV: LANGUAGE-GOAL AERIAL NAVIGATION DATASET WITH GEOGRAPHIC INFORMATION

**中文题目：**CityNav：含地理信息的语言目标空中导航数据集

**论文作者：**Jungdae Lee, Taiki Miyanishi, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Yutaka Matsuo, Nakamasa Inoue

**科研单位：**Science Tokyo, The University of Tokyo, NII, ATR, RIKEN AIP, Kyoto University, Sony Semiconductor Solutions

**是否与无人机视觉语言导航有关？为什么？**

是。本文介绍了一个名为CityNav的数据集，专门用于城市规模的无人机视觉语言导航（VLN）研究。该数据集包含32,637个自然语言描述及其对应的人类演示轨迹，利用真实城市的3D扫描和地理信息收集，旨在推动无人机在复杂城市环境中的自主导航研究。

**开源链接：**[CityNav Project](https://water-cookie.github.io/city-nav-proj/)

**研究问题（论文想解决什么？）** 

论文旨在解决现有无人机视觉语言导航数据集的局限性，特别是缺乏真实世界、城市规模的空中导航研究资源。通过引入CityNav数据集，作者希望填补这一空白，促进无人机在复杂城市环境中的自主导航能力的发展。

**文章创新点：**

- 开发了一个基于网络的3D飞行模拟器，用于收集大规模人类生成的飞行轨迹。
- 引入了CityNav数据集，包含32,637个语言目标描述和人类演示，利用真实城市的3D扫描和地理信息。
- 提供了一个基于地图的基线模型，使用内部2D空间地图表示地理信息，以应对城市规模的广泛搜索空间。
- 证明了结合人类驱动策略和地理信息可以显著提高城市规模的空中导航性能。

**数据集：** 

CityNav数据集包含32,637个自然语言描述及其对应的人类演示轨迹，覆盖了5,850个地理对象，如建筑物和汽车。数据集利用了SensatUrban数据集的3D扫描和CityRefer数据集的地理信息。

**实验如何开展：** 作者使用CityNav数据集对多种空中导航模型进行了基准测试，包括随机模型、序列到序列（Seq2Seq）、跨模态注意力（CMA）和基于地图的目标预测器（MGP）。实验在不同的评估集上进行，包括训练集、已见验证集、未见验证集和未见测试集。

**评价指标：**

- Navigation Error (NE)：代理停止点与目标之间的直线距离。
- Success Rate (SR)：成功在20米范围内停止的代理比例。
- Oracle Success Rate (OSR)：代理轨迹在xy平面上任何时候接近目标20米范围内的比例。
- Success weighted by Path Length (SPL)：考虑路径长度的成功率，奖励更短、更高效的路径。

**核心模型：** Map-based Goal Predictor (MGP) 是本文提出的基线模型，它利用最新的现成组件进行基于地图的目标预测。模型通过提取目标、地标和周围环境的名称，进行目标检测和分割，并可选地进行坐标细化，以动态生成导航地图。

**关键技术：**

- 基于网络的3D飞行模拟器，集成Amazon Mechanical Turk进行大规模数据收集。
- 利用真实城市的3D扫描和地理信息，收集人类生成的轨迹。
- 基于地图的导航模型，结合2D空间地图表示和人类演示轨迹，提高导航性能。
- 在挑战性条件下（如不可靠的GNSS信号和灾难情况）评估模型的鲁棒性。



### CloudTrack: Scalable UAV Tracking with Cloud Semantics

**中文题目：**CloudTrack：基于云语义的可扩展无人机跟踪

**论文作者：**Yannik Blei, Michael Krawez, Nisarga Nilavadi, Tanja Katharina Kaiser 和 Wolfram Burgard

**科研单位：**德国纽伦堡技术大学工程系

**是否与无人机视觉语言导航有关？为什么？**
是。本文提出了一个名为CloudTrack的新型方法，用于在无人机上进行基于语义条件的开放词汇对象检测和跟踪，这与无人机视觉语言导航密切相关，因为该方法利用视觉语言模型（VLMs）来理解和处理语义信息，从而实现对无人机的导航和控制。

**开源链接：**[CloudTrack GitHub Repository](https://github.com/utn-blei/CloudTrack)

**研究问题（论文想解决什么？）**
本文旨在解决在搜索和救援（SAR）任务中，如何利用无人机自动识别和跟踪目标对象的问题。传统的基于卷积神经网络（CNNs）的对象检测方法需要大量的数据收集和标注，且在未知目标对象的情况下不适用。本文提出的方法能够在没有进一步训练或微调的情况下，在无人机上实现实时的、基于语义条件的对象检测和跟踪。

**文章创新点：**
提出了一个在线开放词汇对象检测和跟踪的方法，能够在尊重无人机硬件限制的情况下提高语义准确性。
发布了包括ROS实现的代码。
发布了VOT22基准的指代表达式和SARD数据集的语义真值信息。
在常见的无人机伴随计算机上进行了广泛的评估，并展示了与两个最先进的检测基线相比的改进。

**数据集：**
Search and Rescue Image Dataset for Person Detection (SARD)
Referring VOT22

**实验如何开展：**
在两个公开可用的数据集SARD和VOT22上进行了评估。对SARD数据集进行了扩展，添加了每个人物的衬衫颜色标注，并手动创建了Referring VOT22数据集的指代表达式。

**评价指标：**
对于检测后端，使用了平均精度（Average Precision, AP）和每帧处理时间（tf）以及每个对象的处理时间（tobj）。
对于完整的跟踪管道，使用了平均交并比（mean Intersection over Union, mIoU）、后端响应时间（tb）、边缘平台上的跟踪帧率（FPSEdge）以及整体平均帧率。

**核心模型：**
CloudTrack框架，包括一个在服务器上运行的开放词汇对象检测器和视觉语言模型（VLM）的后端，以及一个在无人机伴随计算机上运行的轻量级对象跟踪器的前端。

**关键技术：**
开放词汇对象检测：使用Grounding DINO进行初始检测和边界框生成，然后使用VLM进行细粒度的语义分类。
视觉语言模型（VLM）：评估了GPT4-mini、LLaVA13b、LLaVA7b和PaliGemma等不同的VLM配置。
轻量级对象跟踪：在常见的无人机硬件上评估了六种兼容的轻量级跟踪器，以找到最佳的跟踪准确性和速度的组合。
重新初始化策略：当跟踪器在挑战性条件下丢失目标时，使用基于位置和尺寸的最近边界框进行重新初始化。



### Dynamic Texts From UAV Perspective Natural Images

**中文题目：**无人机视角自然图像中的动态文本

**论文作者：**Hidetomo Sakaino

**科研单位：**Weathernews Inc., 日本

**是否与无人机视觉语言导航有关？为什么？**
是。本文提出了一个基于相机的能见度和天气状况估计方法，使用多种深度学习（DL）和视觉语言模型（VLM）来处理对抗性条件，这对于无人机在复杂环境中的视觉语言导航具有重要意义。

**开源链接：**有论文链接 [IEEE Xplore](https://ieeexplore.ieee.org/document/10259232)

**研究问题（论文想解决什么？）**
本文旨在解决无人机图像处理在监视、检测和跟踪方面的挑战，特别是在处理尺度和视角变化、应对环境因素（如天空干扰和远小物体）以及在各种设置中确保高可见距离以保障飞行安全等问题。此外，论文还关注在飞行过程中由于局部天气条件快速变化导致的能见度问题，以及如何使用低成本方法进行飞行常规的能见度测量。

**文章创新点：**

1. 提出了一个集成系统，使用多种深度学习和视觉语言模型来估计固定和无人机相机的能见度、距离和天气状况。
2. 通过多个模块（如Dreject、Dcontext、Dvls、Dvld、Dweather、Dvis和Ddist）实现了更精细和丰富的图像描述生成。
3. 实验结果表明，该系统在各种光照和天气条件下具有稳定性、鲁棒性和准确性。
4. 提出了一种基于深度学习的分割方法，能够识别远处的物体并估计能见度距离，而无需依赖地标或地理标记场景。

**数据集：**

- Dreject实验使用了3050张包含4种不同对抗条件（晴朗、镜头反射、强光和浓雾）的图像。
- Dvls实验使用了白天和夜间机场相机图像进行泛视觉分割。
- Dvis实验使用了包含4424张训练图像、1109张验证图像和1175张测试图像的数据集。
- Dweather实验使用了包含1411张图像的测试数据集，涵盖晴天、雨天和雾天等场景。

**实验如何开展：**

1. **Dreject实验**：对输入图像进行分类，判断其质量是低还是高，以增强其他级联深度学习模型的稳定性和准确性。
2. **Dvls实验**：使用深度学习模型进行语义分割，识别图像中的物体和非物体（如天空），并通过泛视觉分割结果分离天空区域和地面物体。
3. **Dvis实验**：训练一个基于Transformer的回归模型，使用真实和合成场景的图像数据来估计能见度，而无需依赖物体识别。
4. **Dweather实验**：结合深度学习分类器和视觉语言模型，对图像进行天气状况分类，并生成包含天气条件的图像描述。

**评价指标：**

1. **Dreject**：分类准确率。
2. **Dvls**：像素级准确率和交并比（IoU）。
3. **Dvis**：Macro F1、Weighted F1、RMSE、MAE和准确率。
4. **Dweather**：分类准确率和损失值。

**核心模型：**

1. **Dreject**：分类模型，用于识别和排除低质量图像。
2. **Dvls**：基于Transformer的语义分割模型，用于识别图像中的物体和非物体。
3. **Dvis**：基于SwinTransformerV2的回归模型，用于估计能见度。
4. **Dweather**：结合深度学习分类器和视觉语言模型（BLIP-2）的天气状况分类模型。

**关键技术：**

1. **深度学习（DL）**：用于图像分类、分割和回归任务。
2. **视觉语言模型（VLM）**：用于理解图像和文本，生成图像描述。
3. **Transformer**：用于提高模型的特征提取能力和处理长距离依赖关系。
4. **数据增强**：通过生成对抗网络（GAN）和条件稳定扩散模型生成多样化的图像数据，增强模型的泛化能力。
5. **多模态融合**：结合图像和文本信息，提高模型在复杂环境下的鲁棒性和准确性。



### LEVIOSA: Natural Language-Based Uncrewed Aerial Vehicle Trajectory Generation

**中文题目**：LEVIOSA：基于自然语言的无人机轨迹生成方法  

**论文作者**：Godwyll Aikins, Mawaba Pascal Dao, Koboyo Josias Moukpe, Thomas C. Eskridge, Kim-Doang Nguyen  

**科研单位**：Florida Institute of Technology  

**是否与无人机视觉语言导航有关？为什么**：是。该研究通过自然语言处理技术实现了无人机根据自然语言指令的轨迹生成。  

**开源链接**：[GitHub链接](https://github.com/sesem738/Leviosa)  

**研究问题**：如何通过自然语言和语音指令生成无人机群的可执行飞行路径，以简化多无人机轨迹生成任务，并在搜索和救援、农业、基础设施检查和娱乐等领域具有重要应用。  

**文章创新点**：  

1. 提出了一种多批评家共识机制，利用多个批评家评估生成的轨迹，并通过多数投票方案确保高质量输出。  
2. 提出了层次化提示结构方法，将多个批评家的输出组织和总结为连贯的上下文，提高LLMs理解和执行复杂任务的能力。  

将多模态大型语言模型（LLMs）与强化学习（RL）相结合，用于无人机的高层规划和低层控制，实现了对复杂命令的处理和实时飞行控制的适应。  

**数据集**：未提及  

**实验如何开展**：  

1. 在受控的仿真环境中进行实验，评估框架根据用户定义的提示生成准确和高效无人机路径的能力。  
2. 使用了三种LLMs（Gemini 1.5 Pro, Gemini 1.5-Flash, GPT-4o）来解释用户提示并合成无人机路径的Python脚本。  
3. 对每种路径类型进行了十次试验，评估了不同LLMs在不同路径类型上的性能。  

**评价指标**：  

- 成功率（Success Rate）  
- 轨迹准确性（Accuracy）  
- 同步性（Synchronization）  
- 避碰能力（Collision Avoidance）  

**核心模型**：LEVIOSA框架  

**关键技术**：  

- 多模态大型语言模型（LLMs）  
- 强化学习（RL）  
- 多批评家共识机制  
- 层次化提示结构  

**需要补充说明的内容：**  LEVIOSA框架通过创新的多批评家共识机制和层次化提示结构，显著提高了无人机轨迹生成的准确性和鲁棒性。然而，该框架在处理复杂几何形状时仍存在一定的局限性，特别是在需要精确空间推理和多无人机协调的任务中。未来的工作将集中在提高框架对动态障碍物的检测和避碰能力，扩展其可扩展性，并探索异构无人机群的协调。



### NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation

**中文题目**：NavAgent：基于多尺度城市街景融合的无人机具身视觉与语言导航  

**论文作者**：Youzhi Liu, Fanglong Yao, Yuanchang Yue, Guangluan Xu, Xian Sun, Kun Fu  

**科研单位**：中国科学院航空航天信息研究所  

**是否与无人机视觉语言导航有关？为什么**：是。该研究提出了首个基于大型视觉语言模型的城市无人机具身导航模型NavAgent，通过融合多尺度环境信息实现无人机在复杂城市环境中的自主导航。  

**开源链接**：[GitHub链接](https://github.com/YouzhiLiu/NavAgent)  

**研究问题**：如何使无人机在复杂的城市环境中基于自然语言指令和环境观察进行导航，同时解决细粒度地标匹配和整体环境信息编码的挑战。  

**文章创新点**：  

1. 提出了NavAgent，首个基于大型视觉语言模型的城市无人机具身导航模型，通过融合多尺度环境信息实现自主导航。  
2. 设计并训练了视觉识别器，通过计算观察图像中的区域特征与地标描述的文本特征之间的相似度，提高了细粒度地标识别的准确性。  
3. 构建了动态增长的场景拓扑图，并使用拓扑图编码器对节点及其空间关系进行编码，增强了代理的长期规划能力。  
4. 开发了首个针对真实城市街景的细粒度地标数据集NavAgent-Landmark2K，包含2000对图像-文本对。  

**数据集**：NavAgent-Landmark2K 数据集，包含2000个细粒度地标图像-文本对，图像中的地标占据约5%的像素区域，文本包含多个修饰词的地标短语。

**实验如何开展**：  

1. 在Touchdown和Map2seq数据集上进行实验，评估NavAgent在未见场景中的性能。  
2. 使用GPT-4作为文本提取器，GLIP作为视觉识别器，LLaMa2-13b作为大型语言模型进行决策。  
3. 通过对比实验，验证NavAgent在任务完成率、最短路径距离和关键点准确率等指标上的性能提升。  

**评价指标**：  

- 任务完成率（TC）  
- 最短路径距离（SPD）  
- 关键点准确率（KPA）  

**核心模型**：NavAgent  

**关键技术**：  

- 多尺度环境信息融合  
- 细粒度地标识别  
- 动态场景拓扑图编码  
- 大型视觉语言模型驱动的导航决策  

需要补充说明的内容：  NavAgent通过融合全局拓扑图、局部全景图和细粒度地标信息，有效地解决了无人机在复杂城市环境中的视觉与语言导航问题。该模型在实验中表现出色，优于多个基线模型。然而，其在实际应用中可能受到计算资源和实时性限制，未来工作将致力于提高导航的稳定性和实时性，以适应更复杂的现实场景。



### NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions

**中文题目**：NEUSIS：一种用于复杂无人机搜索任务中的感知、推理和规划的组合神经符号框架  

**论文作者**：Zhixi Cai, Cristian Rojas Cardenas, Kevin Leo, Chenyuan Zhang, Kal Backman, Hanbing Li, Boying Li, Mahsa Ghorbanali, Stavya Datta, Lizhen Qu, Julian Gutierrez Santiago, Alexey Ignatiev, Yuan-Fang Li, Mor Vered, Peter J. Stuckey, Maria Garcia de la Banda, Hamid Rezatofighi  
**科研单位**：Monash University  

**是否与无人机视觉语言导航有关？为什么**：是。该研究提出了一种用于无人机在复杂搜索任务中的自主感知、推理和规划的组合神经符号框架，涉及视觉感知和路径规划等关键技术。  

**开源链接**：[GitHub链接](https://github.com/monash-robotics/NEUSIS)  

**研究问题**：如何使无人机在大型、危险环境中的指定时间内，根据简要描述自主搜索特定目标实体（EOIs），同时避免危险区域并高效导航。  

**文章创新点**：  

1. 提出了一种组合神经符号框架NEUSIS，集成了神经符号视觉感知、推理和定位（GRiD），以及概率世界模型和层次化规划组件（SNaC）。  
2. GRiD组件通过处理UAV视觉传感器数据，实现了对目标实体的3D定位和属性推理。  
3. 世界模型通过贝叶斯过滤和属性分布排名更新，提高了对目标实体位置和属性的置信度。  
4. SNaC组件采用层次化方法，通过约束优化问题求解和A*算法，实现了对搜索区域的高效路径规划。  

**数据集**：Neighborhood环境数据集，包含500x500米的搜索区域，有房屋、树木、围栏、道路和非EOI车辆等多种世界对象，以及不同的天气条件和夜间场景。  

**实验如何开展**：  

1. 在AirSim模拟平台上使用Neighborhood环境进行仿真测试。  
2. 设定不同的搜索任务场景，包括多个AOIs和EOIs，以及KOZs。  
3. 对比NEUSIS与基于YOLO-World和Fields2Cover的基线系统在成功率、搜索时间和目标定位性能上的表现。  

**评价指标**：  

- 成功率（SR）  
- F1分数  
- 精确度（Precision）  
- 召回率（Recall）  
- 搜索时间  

**核心模型**：NEUSIS框架，包括GRiD、世界模型和SNaC三个主要组件。  

**关键技术**：  

- 神经符号视觉感知（GRiD）  
- 概率世界模型  
- 层次化规划组件（SNaC）  
- 贝叶斯过滤  
- A*算法  

需要补充说明的内容：  NEUSIS框架在模拟环境中表现出色，但在实际应用中可能需要进一步优化以适应更复杂的现实环境。此外，该系统对准确的位置数据、体素网格和点云数据的依赖性较高，未来工作可以探索如何在数据不完全准确的情况下仍能保持高性能。



### Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methadology

**中文题目**：迈向现实的无人机视觉语言导航：平台、基准与方法 

**论文作者**：向宇，杨栋林，王志勤，关浩轩，陈金宇，吴文军，李洪盛，廖越，刘思 

**科研单位**：北京航空航天大学人工智能研究院，香港中文大学 MMLab，感知与交互智能研究中心 

**是否与无人机视觉语言导航有关？为什么**：是。因为论文聚焦于无人机视觉语言导航（UAV VLN）领域，通过构建具备真实飞行控制与复杂算法支持的开放平台 OpenUAV，为无人机 VLN 任务的现实应用奠定了基础。此外，还提出了以目标为导向的真实 UAV VLN 数据集，以及助力无人机完成视觉语言导航任务的 UAV-Need-Help 基准，并进一步构建了无人机导航 LLM，以增强无人机的导航效率。 

**开源链接**：[https://prince687028.github.io/OpenUAV](https://prince687028.github.io/OpenUAV) 

**研究问题**：传统研究在无人机视觉语言导航领域，多借用地面导航设置，采用固定离散动作空间，忽视了无人机飞行特性和复杂环境下的导航需求，导致任务规划缺乏现实映射。本文针对无人机关于高动态性飞行状态规划、任务场景多样化以及数据集缺乏等问题。 

**文章创新点**：

- 提出 OpenUAV 仿真平台，可实现无人机飞行的真实轨迹模拟，构建包含 22 种场景和 89 个物体的 6DoF 运动 UAV 数据集，首开现实无人机视觉语言导航先河。  
- 提出 UAV-Need-Help 基准，为无人机提供不同等级的辅助信息，增强其视觉语言导航任务完成效果。  
- 提出基于无人机导航语言大模型（LLM）的方法，借助多模态理解能力，将视觉和文本信息联合处理，完成层级式轨迹生成。 

**数据集**：OpenUAV——目标导向型 UAV VLN 数据集，包含约 12149 条带有多视图图像和详细指令的轨迹，涵盖各种复杂动态环境，为真实 UAV VLN 任务提供数据支撑。  

**实验如何开展**：

1. 在 OpenUAV 平台中，整合多种传感器，模拟不同环境下的数据收集工作。  
2. 设计 UAV-Need-Help 基准，通过设定不同的辅助信息等级，观察无人机在不同情境下的导航表现。  
3. 采用无人机导航 LLM 方法，结合多模态数据进行训练和测试，评估其在不同类型 UAV VLN 任务中的效果。  

**评价指标**：

- **Success Rate（SR）**：衡量无人机成功抵达目标地点的概率。  
- **Oracle Success Rate（OSR）**：判断无人机是否到达地面真实轨迹上的任意位置，即使没有到达最终目的地。  
- **Success weighted by Path Length（SPL）**：评估成功概率及路径长度，奖励更短和更优路径。  
- **Navigation Error（NE）**：计算无人机最终位置与目标之间的平均距离。  

**核心模型**：Vison-Language LLM（大型语言模型）  

**关键技术**：

- **多模态学习**：巧妙融合图像与文本，通过强化学习精准预测长期目标方位，进而借助前沿视角数据生成精细路径。  
- **轨迹规划算法**：采用基于深度强化学习的轨迹规划算法，使无人机能够根据环境变化和任务要求，动态调整飞行轨迹，提高导航效率和成功率。  
- **轨迹生成**：运用低秩适配器（LoRA）技术，在不对基础模型进行大规模调整的前提下，有效提升对环境的理解和规避障碍的能力，进而生成更高质量的飞行轨迹。  

需要补充说明的内容：  本文的研究成果为无人机视觉语言导航领域带来了新的突破，通过构建真实可靠的仿真平台和数据集，为后续研究提供了有力支持。所提出的 UAV-Need-Help 基准和无人机导航 LLM 方法，也为提高无人机在复杂环境下的导航性能提供了新的思路和方法。然而，该研究还存在一些局限性，如数据集规模有待进一步扩大，模型的泛化能力仍需提升等。随着技术的不断发展，未来的研究可以在此基础上进一步探索和优化，以实现无人机视觉语言导航在实际应用中的更大价值。



### TypeFly: Flying Drones with Large Language Model

**中文题目**：TypeFly：基于大型语言模型的无人机飞行控制  

**论文作者**：Guojun Chen, Xiaojing Yu, Neiwen Ling, Lin Zhong  

**科研单位**：Yale University  

**是否与无人机视觉语言导航有关？为什么**：是。该研究通过设计一种名为TypeFly的系统，利用大型语言模型（LLMs）和专门设计的编程语言MiniSpec，实现了无人机根据自然语言指令的高效飞行控制。  

**开源链接**：[TypeFly GitHub](https://github.com/guojun-chen/TypeFly)  

**研究问题**：如何降低大型语言模型在无人机控制中的响应延迟，提高任务执行效率和用户体验。  

**文章创新点**：  

1. 提出了一种名为MiniSpec的编程语言，专门用于LLMs生成无人机控制计划，提高了代码生成的效率和安全性。  
2. 引入了Stream Interpreting技术，允许无人机在LLMs生成计划的过程中就开始执行，显著降低了响应时间。  
3. 设计了异常处理机制replan，能够在遇到意外情况时重新规划任务，提高了任务的成功率。  
4. 通过probe技能，使LLMs在执行过程中能够动态查询信息，增强了系统的适应性和效率。  

**数据集**：未明确提及具体数据集，但提到了使用多种任务场景进行实验评估，包括基本规划、复杂规划、异常处理等。  

**实验如何开展**：  

1. 在真实办公室环境中进行无人机飞行实验，设置了多种任务场景。  
2. 使用Tello无人机和自建的边缘服务器进行硬件部署。  
3. 通过TypeFly系统控制无人机完成各种任务，并记录响应时间和任务完成时间。  

**评价指标**：  

- 任务成功率（Success Rate）  
- 响应时间（Response Time）  
- 任务完成时间（Completion Time）  
- 输出计划的总令牌数（Total Tokens in Output Plans）  

**核心模型**：TypeFly系统  

**关键技术**：  

- MiniSpec编程语言  
- Stream Interpreting技术  
- 异常处理机制replan  
- probe技能  

需要补充说明的内容：  TypeFly系统通过创新的编程语言MiniSpec和流式解释执行技术，显著降低了无人机控制的响应时间，提高了任务执行的效率和用户体验。然而，该系统在处理更复杂的场景理解和环境建模方面仍有提升空间。未来的工作可以探索更轻量级的环境建模方法，以及优化LLMs的推理速度，以进一步提升系统的性能。



## 视觉语言导航论文综述

### （综述型文章）UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility

**中文题目**：无人机与大型语言模型的融合：迈向智能低空移动的概述与展望  

**论文作者**：Yonglin Tian, Fei Lin, Yiduo Li, Tengchao Zhang, Qiyao Zhang, Xuan Fu, Jun Huang, Xingyuan Dai, Yutong Wang, Chunwei Tian, Bai Lie, Yisheng Lv, Levente Kovács, Fei-Yue Wang  

**科研单位**：中国科学院自动化研究所、澳门科技大学、北京理工大学、西北工业大学、湖南大学、奥布达大学  

**是否与无人机视觉语言导航有关？为什么**：是。该研究探讨了将大型语言模型（LLMs）与无人机（UAVs）相结合的方法，旨在提升无人机的自主性和智能性，特别是在复杂环境中的导航和任务执行能力。  

**开源链接**：[GitHub链接](https://github.com/Hub-Tian/UAVs Meet LLMs)  

**研究问题**：如何通过整合大型语言模型（LLMs）来提升无人机在复杂环境中的自主性和智能性，以实现更高效的低空移动解决方案。  

**文章创新点**：  

1. 提供了无人机系统与基础模型（FMs）整合的全面背景，包括无人机系统的基本组件和功能模块，以及典型FMs的总结。  
2. 系统地回顾了LLMs与UAVs整合的最新研究，识别了关键方法、多样化应用和关键挑战，特别是在导航、感知和规划任务中的应用。  
3. 提出了一个面向代理的UAV设计框架，概述了实现自主感知、记忆、推理和工具利用所需的架构和能力，为UAVs向更智能和适应性系统的演变铺平了道路。  

**数据集**：  

- **AirFisheye**：包含超过26,000张鱼眼图像，数据以每秒10帧的速度收集。  
- **SynDrone**：包含72,000个注释样本，提供28种像素级和对象级注释。  
- **WildUAV**：提供高分辨率RGB图像和深度真值数据，专注于复杂环境中的单目视觉深度估计。  
- **CapEAR**：包含2,864个视频，每个视频有5个描述，总计14,320个文本。  
- **ERA**：包含2,864个视频，涵盖25个事件类别，如地震、洪水、火灾等。  
- **VIRAT**：包含25小时的静态地面视频和4小时的动态空中视频，涉及23种事件类型。  
- **WebUAV-3M**：包含4,500个视频，总计超过330万帧，涵盖223个目标类别。  
- **UAVDark135**：包含135个视频序列，超过125,000个手动标注帧。  
- **DUT-VTUAV**：包含近170万对对齐的可见光-热图像，500个序列，涵盖13个子类别和15个场景。  
- **TNL2K**：包含2,000个视频序列，总计1,244,340帧和663个单词。  
- **PRAI-1581**：包含39,461张图像，涵盖1581个人身份。  
- **VOT2020**：包含多个专门任务的数据集，如短期跟踪、实时跟踪、长期跟踪、热跟踪和深度跟踪。  
- **GOT-10K**：包含420个视频剪辑，涵盖84个对象类别和31个运动类别。  
- **DTB70**：包含70个视频序列，每个序列包含多个视频帧，每个帧包含1280x720像素的RGB图像。  
- **Stanford Drone**：包含19,000多个目标轨迹，涵盖6种目标类型，约20,000个目标交互，40,000个目标与环境的交互，覆盖100多个校园场景。  
- **COWC**：包含32,716个唯一车辆和58,247个非车辆目标的标注，涵盖6个不同地理区域。  
- **Aeriform In-Action**：包含32个视频，13种动作类别，55,477帧，40,000个标注。  
- **MEVA**：包含9,300小时的视频，144小时的活动注释，37种活动类型，超过270万个GPS轨迹点。  
- **UAV-Human**：包含67,428个视频（155种动作类型，119个受试者），22,476帧的关键点标注（17个关键点），41,290帧的人重新识别（1,144个身份），22,263帧的属性识别（如性别、帽子、背包等）。  
- **MOD20**：包含20种动作，2,324个视频，503,086帧。  
- **NEC-DRONE**：包含5,250个视频，涉及19个演员和16种动作类别。  
- **Drone-Action**：包含240个高清视频，66,919帧，13种动作类型。  
- **UAVGESTURE**：包含119个视频，37,151帧，13种手势类型，10个演员。  
- **TrafficNight**：包含2,200对标注的热红外和sRGB图像数据，以及7个交通场景的视频数据，总时长约240分钟，每个场景提供高精度地图，提供详细的布局和拓扑信息。  
- **VisDrone**：包含263个视频，179,264帧，超过250万个对象实例标注，数据覆盖14个不同城市，涵盖多种天气和光照条件。  
- **ITCVD**：包含173张航拍图像，训练集135张，测试集38张，训练集和测试集之间有60%的区域重叠，训练集和测试集之间没有重叠。  
- **UAVid**：包含30个视频，300张图像，8个语义类别标注。  
- **AU-AIR**：包含32,823帧视频，1920x1080分辨率，30 FPS，分为30,000个训练验证样本和2,823个测试样本，总时长约2小时，共132,034个实例，分布在8个类别中。  
- **iSAID**：包含2,806张图像，总计655,451个实例，测试集935张图像（未公开标注，用于服务器端评估）。  
- **CARPK**：包含1,448张图像，约89,777辆汽车，提供边界框标注。  
- **highD**：包含16.5小时，110,000辆汽车，5,600次车道变更，45,000公里，总计约447小时的车辆行驶数据；4个预定义的驾驶行为标签。  
- **UAVDT**：包含100个视频，约80,000帧，30 FPS，包含841,500个目标框，涵盖2,700个目标。  
- **CADP**：包含5.24小时，1,416个交通事故剪辑，205个完整时空标注视频。  
- **VEDAI**：包含1,210张图像（1024x1024和512x512像素），9种车辆类型，总计约6,650个目标。  
- **xView**：包含超过100万的目标和60个类别，包括车辆、建筑物、设施、船只等，分为七个父类别和几个子类别。  
- **DOTA**：包含2806张图像，188,282个目标，15个类别。  
- **RSICD**：包含10,921张图像，54,605个描述性句子。  
- **HRSC2016**：包含3,433个实例，总计1,061张图像，包括70张纯海洋图像和991张包含混合陆地-海洋区域的图像，2,876个标记的船只目标，610张未标注图像。  
- **RSOD**：包含4种目标（坦克、飞机、高架桥、操场），12,000个正样本和48,000个负样本。  
- **NWPURESISC45**：包含31,500张图像，涵盖45个场景类别，每类别700张图像，分辨率256x256像素，空间分辨率从0.2m到30m。  
- **NWPU VHR10**：包含800张高分辨率图像，其中650张包含目标，150张为背景图像，涵盖10个类别（如飞机、船只、桥梁等），总计超过3,000个目标。  
- **CLIPSeg**：基于CLIP模型，支持语义分割、实例分割和零样本分割。  
- **SAM**：Segment Anything Model，通过在大规模和多样化的数据集上预训练，实现零样本分割能力。  
- **Embodied-SAM**：扩展了SAM的功能，支持3D场景分割。  
- **Point-SAM**：针对点云序列的分割任务，提供了详细的分割结果。  
- **Open-Vocabulary SAM**：结合了SAM的知识迁移策略，同时优化了分割和识别任务。  
- **TAP**：以视觉感知为中心的基础模型，通过引入视觉提示改进了SAM的架构，能够同时完成分割、识别和描述任务。  
- **EfficientSAM**：优化了SAM的表示，显著减少了模型复杂度，实现了轻量化设计，同时保持了出色的任务性能。  
- **MobileSAM**：针对移动设备进行了优化，实现了轻量化设计，同时保持了良好的任务性能。  
- **SAM 2**：引入了记忆模块，实现了对任意长度视频的实时分割，解决了复杂挑战，如遮挡和多目标跟踪。  
- **SAMURAI**：在SAM 2的基础上集成了卡尔曼滤波器，解决了SAM 2在记忆管理方面的局限性，实现了卓越的视频分割性能，无需重新训练或微调。  
- **SegGPT**：基于GPT的分割模型，能够处理复杂的分割任务和多模态场景。  
- **Osprey**：结合了SAM和GPT的技术，能够处理复杂的分割任务和多模态场景。  
- **SEEM**：结合了SAM和GPT的技术，能够处理复杂的分割任务和多模态场景。  
- **Seal**：结合了SAM和GPT的技术，能够处理复杂的分割任务和多模态场景。  
- **LISA**：结合了嵌入式推理的多模态大型模型，能够处理复杂的分割任务和多模态场景。  
- **LISA**：结合了嵌入式推理的多模态大型模型，能够处理复杂的分割任务和多模态场景。  
- **WAID**：包含14,375张无人机图像，涵盖6种野生动物物种和多种栖息地类型。  
- **MOCO**：包含7,449张图像和37,245个字幕，用于军事图像字幕任务。  
- **FloodNet**：包含2,343张图像，分为训练（60%）、验证（20%）和测试（20%）集，语义分割标签包括背景、被洪水淹没的建筑、未被洪水淹没的建筑、被洪水淹没的道路、未被洪水淹没的道路、水、树、车辆、池塘、草地等。  
- **AFID**：包含816张图像，分辨率分别为2720x1536和2560x1440，包含8个语义分割类别。  
- **Aerial SAR**：包含2,000张图像，30,000个行动实例，涵盖多种人类行为。  

**实验如何开展**：  

1. 在AirSim仿真平台构建三维城市环境，模拟无人机在复杂城市环境中的飞行任务，验证导航和避障算法的有效性。  
2. 使用真实的无人机硬件平台进行飞行实验，验证所提出方法在实际环境中的可行性和鲁棒性。  
3. 通过对比实验，将所提出的基于LLMs的导航方法与传统导航方法进行对比，评估其在任务成功率、路径长度、计算效率等方面的性能提升。  

**评价指标**：  

- **任务成功率（SR）**：衡量无人机在给定任务中成功完成任务的比例。  
- **路径长度（PL）**：衡量无人机完成任务所经过的路径长度，用于评估路径规划的效率。  
- **计算效率（CE）**：衡量算法在运行过程中的计算资源消耗，包括CPU和内存使用情况。  

**核心模型**：LLMs（如GPT-4）、VLMs（如GPT-4V）、VFMs（如CLIP）  

**关键技术**：  

- **跨模态对比学习**：通过对比学习方法，将视觉和语言模态进行对齐，提高模型的泛化能力。  
- **零样本学习**：利用LLMs和VLMs的零样本学习能力，实现对未见过任务的快速适应。  
- **多模态融合**：将视觉、语言等多种模态信息进行融合，提高模型的感知和决策能力。  

需要补充说明的内容：  该研究提出了一个面向代理的UAV设计框架，旨在实现无人机的自主感知、记忆、推理和工具利用。通过整合LLMs、VLMs和VFMs，无人机能够在复杂环境中进行高效的任务执行和决策。然而，该方法在实际应用中仍面临一些挑战，如计算资源的限制和模型的泛化能力。未来的研究可以进一步探索如何优化模型结构，提高计算效率，以及如何在更多实际场景中验证模型的有效性。





## 大模型+无人机相关文章

### REAL: Resilience and Adaptation using Large Language Models on Autonomous Aerial Robots

**中文题目**：基于大型语言模型的自主空中机器人弹性与适应性研究  

**论文作者**：Andrea Tagliabue, Kota Kondo, Tong Zhao, Mason Peterson, Claudius T. Tewari, Jonathan P. How  

**科研单位**：麻省理工学院（MIT）  

**是否与无人机视觉语言导航有关？为什么**：否。该研究主要关注如何利用大型语言模型（LLMs）提高无人机的适应性和弹性，而非视觉语言导航。  

**开源链接**：["未开源"]  

**研究问题**：如何利用大型语言模型（LLMs）提高自主移动机器人在面对意外情况时的适应性和弹性，特别是在控制和任务规划方面。  

**文章创新点**：  

1. 提出了一种新的方法（REAL），利用LLMs的先验知识进行在线适应和决策，覆盖了从低级控制器到任务规划的不同时间尺度和组件。  
2. 展示了在真实硬件实验中，LLMs能够改善无人机的位置控制性能或调节任务的安全性，这在空中机器人上尚属首次。  
3. 通过自然语言提示和反馈，使LLMs能够在不同层次（低级控制和任务级）上进行适应和决策。  

**数据集**：未明确提及具体数据集名称，但提到了在真实世界实验中使用了多种性能降低条件来测试无人机的适应能力。 

**实验如何开展**：  

1. 在真实世界的多旋翼无人机上部署REAL系统，通过OpenAI GPT-4 API查询LLMs。  
2. 设计多种实验场景，包括参数错误、未建模动力学和潜在危险情况，以测试无人机的适应和决策能力。  
3. 通过调整控制输入和任务规划，验证LLMs在不同情况下的表现。  

**评价指标**：  

- 位置跟踪误差  
- 任务完成度  
- 安全性评估  

**核心模型**：REAL方法  

**关键技术**：  

- 大型语言模型（LLMs）的自然语言理解和长序列推理能力  
- 自适应控制和任务规划的集成  
- 通过自然语言提示进行零样本学习（zero-shot prompting）  

**需要补充说明的内容：**  该研究展示了LLMs在无人机自主控制中的潜力，特别是在处理未预见情况和进行在线适应方面。然而，LLMs的决策速度受限于网络延迟和API调用频率，这可能影响其在高速或实时性要求高的场景中的应用。此外，LLMs的决策过程缺乏可解释性，这在安全关键的应用中可能是一个问题。未来的工作可以探索如何提高LLMs的决策速度和可解释性，以及将其应用于更复杂的多机器人系统。



### RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery

**中文题目**：遥感影像的大型视觉语言模型：联合字幕生成与问题回答  

**论文作者**：Yakoub Bazi, Laila Bashmal, Mohamad Mahmoud Al Rahhal, Riccardo Ricci, Farid Melgani  

**科研单位**：King Saud University, University of Trento  

**是否与无人机视觉语言导航有关？为什么**：否。该研究专注于遥感影像的分析，通过大型视觉语言模型实现图像字幕生成和视觉问题回答，不涉及无人机导航。  

**开源链接**：[GitHub链接](https://github.com/BigData-KSU/RS-LLaVA)  

**研究问题**：如何开发一个能够同时处理图像字幕生成和视觉问题回答的多任务模型，以提高遥感影像分析的效率和准确性。  

**文章创新点**：  

1. 提出RS-LLaVA模型，基于LLaVA模型，通过LoRA微调适应遥感数据。  
2. 开发RS-instructions数据集，整合多个图像字幕和VQA数据集，用于多任务指令跟随训练。  
3. 在单任务和多任务场景中，RS-LLaVA模型均优于现有最先进方法，展示了其在遥感数据分析中的潜力。  

**数据集**：RS-instructions数据集，包含UCM-caption、UAV、RSVQA-LR和RSIVQADOTA四个数据集，共7058个样本，涵盖图像字幕和VQA任务。  

**实验如何开展**：  

1. 使用Vicuna-v1.5语言模型（7B和13B两种规模）初始化RS-LLaVA模型。  
2. 采用CLIP-ViT（large）作为图像编码器，LoRA方法进行微调。  
3. RS-instructions数据集上进行训练和测试，评估模型在图像字幕生成和VQA任务上的性能。  

**评价指标**：  

- 图像字幕任务：BLEU score（n-gram值从1到4）、METEOR、ROUGE、CIDEr  
- VQA任务：准确率（RSVQA-LR数据集）、精确率、召回率、F1分数（RSIVQADOTA数据集）  

**核心模型**：RS-LLaVA  

**关键技术**：  

- 大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的应用  
- 低秩适应（LoRA）微调技术  
- 指令调优（instruction tuning）  

需要补充说明的内容：  RS-LLaVA模型展示了在遥感影像分析中的多任务处理能力，但在处理计数问题等复杂任务时仍面临挑战。未来研究可探索模型压缩技术，以降低计算资源需求，并扩展数据集和任务类型，如视觉定位和多时相图像变化检测，以增强模型的通用性和适用性。



### Say-REAPEx: An LLM-Modulo UAV Online Planning Framework for Search and Rescue

**中文题目**：Say-REAPEx：一种用于搜索与救援的LLM模块化无人机在线规划框架  

**论文作者**：Björn Döschl, Jane Jean Kiam  

**科研单位**：University of the Bundeswehr Munich  

**是否与无人机视觉语言导航有关？为什么**：否。该研究主要关注利用LLM（大型语言模型）来增强无人机在搜索与救援任务中的在线规划能力，而非视觉语言导航。  

**开源链接**：["未开源"]  

**研究问题**：如何在搜索与救援任务中，通过结合LLM和领域特定知识，提高无人机的在线规划效率和成功率。  

**文章创新点**：  

1. 提出了一种两步动作选择过程，包括非参数化动作的选择和动作参数的选择。  
2. 采用新的评分机制，将传感器能力纳入Can模型，并在Pay模型中引入在线规划的启发式方法。  
3. 通过减少规划时间和提高规划效率，适应搜索与救援任务中无人机电池容量有限的挑战。  

**数据集**：未明确提及具体数据集名称，但提到了使用Unreal Engine和AirSim进行模拟实验。  

**实验如何开展**：  

1. 使用REAP框架作为验证环境，结合ROS2作为中间件。  
2. 在Unreal Engine 5.2中使用AirSim作为四旋翼飞行器模拟器和物理引擎。  
3. 通过预训练的YOLOv8模型进行目标检测。  

**评价指标**：  

- 计算时间（Computation Time）  
- 处理的动作数量（Number of Processed Actions）  
- 精度（Precision）  
- 成功率（Success Rate）  

**核心模型**：Say-REAPEx框架  

**关键技术**：  

- LLM模块化规划  
- 在线启发式搜索（AEMS2）  
- 领域特定知识整合  

需要补充说明的内容：  该研究的主要挑战在于如何将LLM与领域特定知识有效结合，以提高无人机在复杂环境中的决策能力。尽管在计算时间和处理动作数量上取得了显著改进，但在精度和成功率方面仍有提升空间，主要受限于目标识别的准确性。未来工作将探索多无人机系统的应用，并增强框架对动态任务指令的适应性。



### SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments

**中文题目**：在线语义规划用于不完整自然语言规范的任务  

**论文作者**：Zachary Ravichandran, Varun Murali, Mariliza Tzes, George J. Pappas, Vijay Kumar  

**科研单位**：GRASP Laboratory, University of Pennsylvania  

**是否与无人机视觉语言导航有关？为什么**：否。该研究主要关注在不完整自然语言规范下，机器人如何在非结构化环境中进行在线语义规划，以完成高级任务。  

**开源链接**：[GitHub链接](https://zacravichandran.github.io/SPINE)  

**研究问题**：如何在不完整自然语言规范下，使机器人能够在部分已知、非结构化环境中进行在线语义规划，以完成高级任务。  

**文章创新点**：  

1. 提出了一种在线语义规划方法，用于部分已知环境中的语言规范任务。  
2. 设计了一种从不完整任务规范中推断子任务并在线更新的过程。  
3. 引入了一个验证模块，使LLM能够安全地提出导航和探索目标。  

**数据集**：未提及具体数据集名称，但实验在模拟和真实世界环境中进行，包括超过20,000平方米的复杂户外环境。  

**实验如何开展**：  

1. 在模拟环境中进行实验，包括超过40,000平方米的户外环境，任务要求机器人行驶高达400米。  
2. 在真实世界环境中进行实验，使用ClearPath Jackal机器人在半城市办公园区进行测试。  

**评价指标**：  

- 任务成功率  
- 完成任务所需时间  
- 完成任务所需距离  
- 用户交互次数  
- LLM API调用次数  

**核心模型**：SPINE（在线语义规划器）  

**关键技术**：  

- 使用LLM进行子任务推断  
- 基于场景图的语义映射  
- 在线验证和反馈机制  

需要补充说明的内容：  该研究展示了在不完整自然语言规范下，机器人如何通过在线语义规划完成复杂任务。实验结果表明，SPINE在模拟和真实世界环境中均表现出色，与显式任务分配方法相比，SPINE在用户交互次数和任务完成效率方面具有优势。此外，验证模块对于提高在线规划的可靠性至关重要，尤其是在环境不确定性较高的情况下。



### FlockGPT: Guiding UAV Flocking with Linguistic Orchestration

**中文题目**：FlockGPT：通过语言编排引导无人机群

**论文作者**：Artem Lykov, Sausar Karaf, Mikhail Martynov, Valerii Serpiva, Aleksey Fedoseev, Mikhail Konenkov, Dzmitry Tsetserukou

**科研单位**：未明确提及

**是否与无人机视觉语言导航有关？为什么**：否。该研究主要关注通过自然语言和生成式AI控制无人机群的编队和几何形状生成，而非视觉语言导航。

**开源链接**：无

**研究问题**：如何通过自然语言高效、直观地控制大规模无人机群的编队和几何形状。

**文章创新点**：

1. 提出了一种基于大型语言模型（LLM）的新型接口，用于与用户交互并生成目标几何形状描述。
2. 结合了无人机群技术和目标表面定义（通过符号距离函数），实现了无人机群在目标状态之间的平滑和自适应运动。
3. 开发了用户友好的对话系统，允许用户通过自然语言实时编辑和调整无人机群的几何形状。

**数据集**：未明确提及

**实验如何开展**：

1. 在Unity和Gazebo模拟环境中测试了大规模无人机群（64架无人机）的编队控制。
2. 使用Crazyflie 2.1迷你无人机进行了真实环境中的小型无人机群（8架无人机）测试。
3. 开展了用户研究，评估用户对生成的无人机群几何形状的识别率和系统易用性。

**评价指标**：

- 识别率（Recognition Rate）：用户对不同几何形状的识别准确度。
- NASA任务负荷指数（NASA-TLX）：评估系统的任务负荷，包括心理需求、物理需求、时间需求、整体表现、努力和挫败感。
- 用户体验问卷（UEQ）：评估系统的吸引力、刺激性、新颖性和愉悦质量。

**核心模型**：FlockGPT

**关键技术**：

- 大型语言模型（LLM）：用于自然语言处理和目标几何形状生成。
- 符号距离函数（SDF）：用于定义无人机群的目标表面。
- 人工势场法（AFP）：用于无人机群的避障和编队控制。
- 优化算法（L-BFGS-B和k-means）：用于点云优化和无人机群的均匀分布。

**需要补充说明的内容：**
该研究展示了通过自然语言控制无人机群的潜力，特别是在无人机表演、虚拟现实（VR）中的无人机群模拟和人-无人机群交互方面。未来的工作将集中在动态形状生成、情感驱动的无人机路径规划以及通过天空中的标志和文本实现人与人之间的新型通信方式。



### From Words to Flight: Integrating OpenAI ChatGPT with PX4/Gazebo for Natural Language-Based Drone Control

**中文题目**：从语言到飞行：将 OpenAI ChatGPT 与 PX4/Gazebo 集成为基于自然语言的无人机控制]

**论文作者**：Mohamed Lamine TAZIR, Matei MANCAS, Thierry DUTOIT 

**科研单位**：ISIALAB, Numediart Institute, University of Mons, Mons, Belgium

**是否与无人机视觉语言导航有关？为什么**：[否]。[该研究主要关注如何通过自然语言命令控制无人机，而非视觉语言导航]  

**开源链接**：[未开源]  

**研究问题**：如何使用户能够通过自然语言指令直观地控制无人机，而无需进行广泛的无人机驾驶培训  

**文章创新点**：  

1. 首次提出将 OpenAI ChatGPT 与 PX4/Gazebo 平台集成，实现基于自然语言的无人机控制
2. 开发并实施了用于验证和验证 ChatGPT 生成命令的系统，确保自然语言控制方法的可靠性和安全性
3. 确定并概述了设计有效提示的关键标准，以确保零样本响应生成，为未来涉及人机交互和大型语言模型的研究和应用提供了有价值的见解

**数据集**：[未提及具体数据集]  

**实验如何开展**：  

1. 通过 PX4/Gazebo 模拟器测试无人机控制算法和开发自主无人机应用程序
2. 设计了一系列测试场景，包括各种自然语言输入，代表不同的无人机动作，以评估生成命令的准确性和合规性

**评价指标**：  

- 命令生成准确性
- 任务成功率
- 执行时间

**核心模型**：OpenAI ChatGPT (GPT-3.5-Turbo)

**关键技术**：  

- [自然语言处理 (NLP)]  
- [命令验证和验证系统]  
- [MAVLink 消息映射]  

需要补充说明的内容：  该研究展示了如何利用先进的语言模型（如 ChatGPT）促进人类与机器人系统之间的直观和高效交互。未来的工作将致力于改进提示工程过程，以进一步提高响应质量，并将该方法扩展到其他模拟器或现实世界的机器人系统。此外，研究还将关注开发和利用自身的方法进行定位、轨迹规划和避障，而不仅仅依赖于 PX4 指挥官中预定义的功能。



### Game4Loc: A UAV Geo-Localization Benchmark from Game Data

**中文题目**：Game4Loc：来自游戏数据的无人机地理定位基准  

**论文作者**：Yuxiang Ji, Boyong He, Zhuoyue Tan, Liaoni Wu  

**科研单位**：厦门大学人工智能研究所，厦门大学航空航天学院  

**是否与无人机视觉语言导航有关？为什么**：否。该研究聚焦于利用现代计算机游戏数据构建无人机地理定位数据集，并提出了一种新的加权对比学习方法来处理部分匹配问题，以提高无人机在GPS拒止环境下的视觉定位性能。  

**开源链接**：[GitHub链接](https://yux1angji.github.io/game4loc)  

**研究问题**：现有无人机视觉地理定位数据集存在规模小、场景单一、飞行高度和角度受限等问题，且通常假设每个查询图像都有一个完美对齐的参考图像，这与实际应用场景不符。如何构建一个更符合实际的大规模无人机地理定位数据集，并提出有效的方法来处理部分匹配问题，提高无人机的地理定位性能。  

**文章创新点**：  

1. 提出了一个新的无人机地理定位基准数据集GTA-UAV，该数据集涵盖了多种飞行高度、姿态、场景和目标，使用现代计算机游戏模拟了大规模连续区域的无人机和卫星图像对。  
2. 引入了一种加权对比学习方法（weighted-InfoNCE），利用部分匹配数据区域的交并比作为权重标签，使模型能够学习部分匹配范式。  
3. 验证了所提出数据集和方法的有效性，并展示了其在真实世界任务中的潜力和泛化能力，仅使用少量可用的真实数据。  

**数据集**：GTA-UAV数据集，包含33,763张无人机视图图像，覆盖了城市、山脉、沙漠、森林、田野和海岸等多种场景，飞行高度范围为80m至650m，相机角度范围为roll ϕ ∈ [−10°, 10°], pitch θ ∈ [−100°, −80°] 和 yaw ψ ∈ [−180°, 180°]。  

**实验如何开展**：  

1. 使用ViT作为特征编码器，采用加权InfoNCE损失函数对正样本和半正样本进行训练。  
2. 通过计算无人机视图和卫星视图覆盖的地面区域的交并比（IOU）来构建部分匹配对，IOU大于0.39的为正样本对，IOU在0.14到0.39之间的为半正样本对。  
3. 在训练过程中采用互斥采样方法，确保同一批次中的样本对之间没有相关性。  

**评价指标**：  

- Recall@K (R@K)  
- Average Precision (AP)  
- Spatial Distance Metric (SDM@K)  
- Distance error between the retrieval results and the query location (Dis@1)  

**核心模型**：Vision Transformer (ViT)  

**关键技术**：  

- 加权对比学习（weighted-InfoNCE）  
- 互斥采样（Mutually Exclusive Sampling）  

**需要补充说明的内容：**  该研究通过利用游戏数据构建大规模无人机地理定位数据集，并提出了一种新的加权对比学习方法来处理部分匹配问题，有效提高了无人机在GPS拒止环境下的视觉定位性能。然而，该方法在真实世界数据上的泛化能力仍需进一步验证，且模型的计算复杂度较高，可能需要在实际应用中进行优化。



### Integrating Large Language Models for UAV Control in Simulated Environments: A Modular Interaction Approach

**中文题目**：无人机控制中大型语言模型的集成：一种模块化交互方法  

**论文作者**：Abhishek Phadke, Alihan Hadimlioglu, Tianxing Chu, Chandra N Sekharan  

**科研单位**：Christopher Newport University, Texas A&M University - Corpus Christi  

**是否与无人机视觉语言导航有关？为什么**：否。该研究主要探讨了将大型语言模型（LLMs）集成到无人机控制中，以实现自然语言指令的解析和响应，从而简化无人机的控制和使用。  

**开源链接**：["未开源"]  

**研究问题**：如何将大型语言模型集成到无人机控制中，以增强无人机的自主决策、动态任务规划、增强态势感知和改进安全协议等能力。  

**文章创新点**：  

1. 提出了一种模块化的交互方法，用于在模拟环境中集成大型语言模型（LLMs）进行无人机控制。  
2. 开发了一个模板开发框架，用于将LLMs集成到无人机控制中，并展示了与现有LLM模型和流行机器人模拟平台的概念验证结果。  
3. 探讨了LLMs在无人机技术中的多个关键领域的影响，包括自主决策、动态任务规划、增强态势感知和改进安全协议。  

**数据集**：未明确提及  

**实验如何开展**：  

1. 使用MATLAB和CoppeliaSim等平台构建模拟环境，并通过LLM/API用户界面（MatGPT）与LLM模型进行交互。  
2. 设计了多个任务场景，包括单个静态障碍物的避障任务，以验证LLM控制的无人机代理的性能。  

**评价指标**：  

- 任务成功率（例如，无人机成功避开障碍物并到达目标位置的比例）  
- API调用次数（例如，简单任务的API调用次数少于10次，复杂任务可能需要100次或更多）  

**核心模型**：ChatGPT模型（由OpenAI提供）  

**关键技术**：  

- 自然语言处理（NLP）技术，用于解析和响应自然语言指令。  
- 模块化交互框架，用于将LLMs集成到无人机控制中。  
- 模拟平台（如CoppeliaSim）用于渲染无人机代理和环境，并生成所需数据。  

**需要补充说明的内容：**  
该研究展示了将LLMs集成到无人机控制中的潜力，特别是在简化用户界面和增强人机交互方面。然而，研究也存在一些局限性，例如环境复杂度较低、仅限于模拟环境测试以及交互速度响应较慢等。未来的工作可以进一步探索在更复杂环境中的应用，以及提高交互速度和效率的方法。



### Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery

**中文题目**：利用YOLO-World和GPT-4V LMMs在无人机图像中进行零样本人员检测和动作识别  

**论文作者**：Christian Limberg, Artur Gonc¸alves, Bastien Rigault, Helmut Prendinger  

**科研单位**：National Institute of Informatics (NII), Tokyo, Japan  

**是否与无人机视觉语言导航有关？为什么**：否。该研究主要关注利用零样本大型多模态模型（LMMs）进行无人机图像中的人员检测和动作识别，而非直接涉及视觉语言导航。  

**开源链接**：未提供具体开源链接

**研究问题**：如何在资源有限或时间紧迫的情况下，利用零样本大型多模态模型（LMMs）在无人机图像中实现人员检测和动作识别。  

**文章创新点**：  

1. 探索了YOLO-World和GPT-4V LMMs在无人机图像中的零样本人员检测和动作识别应用。  
2. 使用YOLO-World进行人员检测，并结合GPT-4V对检测到的区域进行动作分类。  
3. 评估了这些模型在实际无人机应用场景中的可行性和性能。  

**数据集**：Okutama-Action数据集。包含从空中视角捕获的多个人员执行不同动作的视频数据，涵盖12种不同动作。  

**实验如何开展**：  

1. 使用Okutama-Action数据集进行评估，重点关注人员检测和动作分类。  
2. 利用YOLO-World进行人员检测，通过预训练权重和单字文本提示“Person”进行实验。  
3. 使用GPT-4V对检测到的区域进行动作分类，通过不同提示和实验设置进行评估。  

**评价指标**：  

- Precision（精确率）  
- Recall（召回率）  
- F1 Score（F1分数）  
- mIOU（平均交并比）  

**核心模型**：YOLO-World和GPT-4V LMMs  

**关键技术**：  

- YOLO-World的零样本对象检测能力  
- GPT-4V的视觉理解与动作分类能力  
- CLIP文本嵌入与视觉-语言网络融合  

**需要补充说明的内容：**  该研究展示了零样本LMMs在无人机图像处理中的潜力，尤其是在人员检测和动作识别方面。尽管GPT-4V在动作分类上的表现不尽如人意，但其在过滤不想要的区域提议和提供场景描述方面具有一定的应用价值。未来的工作可以进一步探索如何通过少量监督输入（如few-shot学习）来提高模型的准确性和适应性。



### UEVAVD: A Dataset for Developing UAV’s Eye View Active Object Detection

**中文题目**：UEVAVD：用于开发无人机视角主动目标检测的数据集  

**论文作者**：Xinhua Jiang, Tianpeng Liu, Li Liu, Zhen Liu, Yongxiang Liu  

**科研单位**：National University of Defense Technology  

**是否与无人机视觉语言导航有关？为什么**：否。该研究专注于通过改变无人机视角来提高目标检测性能，而非视觉语言导航。  

**开源链接**：[https://github.com/Leo000ooo/UEVAVD](https://github.com/Leo000ooo/UEVAVD)  

**研究问题**：解决无人机在空中对地面目标检测中因遮挡导致的检测性能下降问题。  

**文章创新点**：  

1. 提出了一个新的无人机眼视图主动视觉数据集（UEVAVD），以促进无人机主动目标检测（AOD）问题的研究。  
2. 改进了现有的基于深度强化学习（DRL）的AOD方法，通过在状态表示学习过程中引入归纳偏差，使代理的策略具有更强的泛化能力。  
3. 使用门控循环单元（GRU）从观测序列中提取状态表示，而不是依赖单视图观测。  
4. 通过Segment Anything Model（SAM）预分解场景，并使用生成的掩码过滤无关信息。  

**数据集**：UEVAVD数据集。包含五种车辆目标在不同环境设置下的多视角成像结果，涵盖城市和林地地形，以及不同程度的遮挡情况。  

**实验如何开展**：  

1. 在模拟环境中构建整个场景，并使用AirSim插件从无人机平台获取观测数据。  
2. 选择五种车辆目标，并在不同环境设置下进行观测，包括不同的遮挡和地形。  
3. 在每个场景中，无人机在特定采样点对目标进行观测，并记录观测结果。  
4. 使用改进的DRL方法对无人机的主动观测策略进行训练和测试。  

**评价指标**：  

- 回报值（Return）  
- 识别准确率（Recognition Accuracy）  
- 每集的总移动距离（Movement Distance）  

**核心模型**：Inductive Biases Enhanced Multistep Action Prediction (IBE-MAP)  
**关键技术**：  

- 场景预分解（Scene Pre-decomposition）  
- 基于记忆的状态估计（Memory-based State Estimation）  

需要补充说明的内容：  该研究通过引入归纳偏差和改进的状态表示学习方法，显著提高了无人机在复杂环境中的主动目标检测性能。未来工作将探索更有效的方法来进一步优化状态表示，以提高测试时的回报值并缩小泛化差距。

